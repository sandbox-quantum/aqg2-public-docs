{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Welcome","text":"<p>Welcome to the SandboxAQ AQtive Guard user guide. AQtive Guard (AQG) is designed to manage and secure Non-Human Identities (NHIs) and other cryptographic assets used by AI agents, whether benign or malicious, that are increasingly present in enterprise environments. By providing proactive, AI-driven cryptographic defenses, AQG empowers your organization to stay ahead of evolving threats that outpace traditional security measures, ensuring the integrity and security of your digital assets.</p> <p>As a unified, AI-driven platform for modern Non-Human Identity (NHI) and cryptography management, AQG features include:</p> <ul> <li>Vulnerability Detection and Inventory: Integrates data from multiple sources, including CrowdStrike, Palo Alto Networks, and ServiceNow, to build a complete and continuously updated inventory across leading cloud providers (AWS, GCP).</li> <li>AI-powered Insights and Risk Analysis: Leverages metadata for advanced filtering and clustering, and delivering prioritized, actionable insights with contextual guidance for remediation and risk reduction.</li> <li>Automated Remediation and Lifecycle Management: Streamlines and automates the lifecycle of identities and cryptographic keys, reducing manual overhead and minimizing risk.</li> <li>Compliance and NIST Standards: Provides targeted remediation recommendations, a powerful query engine, and robust reporting to demonstrate compliance and accelerate migration to emerging NIST standards.</li> </ul> <p>AQG enables you to efficiently and effectively manage and secure NHIs and cryptographic assets, reducing false positives and minimizing risk.</p>"},{"location":"#about-this-guide","title":"About this guide","text":"<p>To make the most of this guide, use the following resources:</p> <ul> <li>The user guide menu on the left to navigate through the guide.</li> <li>The contextual menu on the right for links to specific topics.</li> <li>The search box at the top of the guide to quickly find what you\u2019re looking for.</li> </ul>"},{"location":"accessibility-statement/","title":"Accessibility statement","text":"<p>AQtive Guard is committed to providing a website that is accessible to all. To help ensure all visitors to our website have a positive experience and find the information they need, we\u2019ve been using Web Accessibility Guidelines 2.1, which provides standards for making web content more accessible for all site visitors, including people who are blind or visually impaired.</p> <ul> <li>The Web Accessibility Guidelines 2.1 have three levels of accessibility: A, AA, and AAA. We chose Level AA as the target for our website.</li> </ul> <ul> <li>We\u2019ve worked hard to make our website accessible for all, and we believe our site is at Level AA accessibility.</li> </ul> <ul> <li>We monitor the website regularly to maintain this level, and are committed to fixing accessibility issues when they arise.</li> </ul> <p> </p> <p>If you have trouble accessing any part of the AQtive Guard website, or if you have suggestions for improving the site, we\u2019d like to hear from you.</p> <p>Email us at aqg-accessibility@sandboxquantum.com.</p>"},{"location":"changelog/","title":"Changelog","text":"<p>The AQtive Guard changelog provides a record of updates, improvements, and fixes made to the platform. Below is a list of AQtive Guard version numbers and their deployment dates. Each version includes a summary of the changes introduced in that release.</p>"},{"location":"changelog/#25011-2025-01-30","title":"25.01.1 - 2025-01-30","text":""},{"location":"changelog/#whats-new","title":"What\u2019s new","text":""},{"location":"changelog/#general","title":"General","text":"<p>- AQtive Guard alpha released. Please check AQG product user docs at \\&lt;yourdomain&gt;/docs for more information</p>"},{"location":"manage-your-data/","title":"How we manage your data","text":"<p>AQtive Guard is a cybersecurity platform that secures Non-Human Identities (NHIs) and cryptographic assets used by AI agents and other entities in enterprise environments. This document provides transparency into our data management practices, outlining how we handle user data to ensure privacy and security. It applies to all users of AQtive Guard products and services. Please note that this document is part of, and subject to, our overall Terms &amp; Conditions \u2014 it\u2019s meant to provide additional insight into our data practices, but doesn\u2019t replace or override any terms outlined in our main agreements.</p>"},{"location":"manage-your-data/#commitment-to-privacy","title":"Commitment to privacy","text":"<p>We recognize the critical importance of protecting your data and are dedicated to protecting the security and privacy of your data. Our goal is to safeguard your information while complying with applicable data privacy regulations, ensuring trust and confidence in our services.</p>"},{"location":"manage-your-data/#compliance-with-data-privacy-regulations","title":"Compliance with data privacy regulations","text":"<p>We prioritize your privacy and compliance with applicable laws. AQtive Guard applies industry-standard security measures to protect your data, ensuring transparent, secure, and trustworthy information handling.</p> <p>For more detailed information about our privacy practices, please refer to our Privacy Policy.</p>"},{"location":"manage-your-data/#certifications","title":"Certifications","text":"<p>AQtive Guard is actively pursuing and committed to achieving the following certifications:</p> <ul> <li>SOC 2 - SOC 2 is a widely recognized auditing standard developed by the American Institute of Certified Public Accountants (AICPA). This certification process involves a thorough assessment of our controls related to security, availability, processing integrity, confidentiality, and privacy, demonstrating our commitment to protecting customer data.</li> <li>ISO 27001 - ISO 27001 is an international standard that specifies the requirements for establishing, implementing, maintaining, and continually improving an information security management system (ISMS). This certification process demonstrates our ongoing commitment to systematically managing and protecting company and customer information.</li> <li>FedRAMP Moderate authorization: FedRAMP Moderate authorization is a U.S. government program that provides a standardized approach to security assessment, authorization, and continuous monitoring for cloud products and services. </li> </ul>"},{"location":"manage-your-data/#types-of-data-we-handle","title":"Types of data we handle","text":"<p>To provide our services, AQtive Guard handles data in the following general categories:</p> <ul> <li>Certificate and secret management data: We collect metadata about your digital certificates and secrets, such as details about their creation, usage, and lifecycle. We also store public keys and public certificates with their actual values. Private secrets, such as private keys, are collected only on an opt-in basis, requiring your explicit consent and configuration.</li> <li>Asset inventory data: We collect information about your IT environment, including things like servers, applications, and the cryptographic assets they use, to provide you with a comprehensive view of your security posture.</li> <li>Integration data: If you connect AQtive Guard with other security and IT tools, we handle data exchanged with those systems to provide centralized security management.</li> <li>Security analysis data: This includes data generated by our platform to help you understand your security risks, such as alerts, rule configurations, and compliance information.</li> <li>Account data: We collect the necessary information to manage your account, such as your email and contact details.</li> <li>User analytics - We collect anonymous and aggregated usage patterns and behaviors to help us improve AQtive Guard usability and features.</li> </ul> <p>For details about the information we collect, compliance with data privacy regulations, and your privacy rights ,please refer to our Privacy Policy.</p>"},{"location":"manage-your-data/#how-your-data-flows-across-aqtive-guard","title":"How your data flows across AQtive Guard","text":"<p>Your data is transmitted to AQtive Guard through secure, encrypted channels. We process it to enable platform functionalities like monitoring, alerting, and reporting on cryptographic assets. The data flow involves:</p> <ol> <li>Ingestion: Collecting and normalizing data.</li> <li>Analysis: Evaluating data for security risks and compliance.</li> <li>Visualization: Transforming data into actionable insights.</li> <li>Storage: Securing data with access controls and encryption.</li> </ol> <p>Refer to the Getting started diagram for a data flow diagram and details.</p>"},{"location":"manage-your-data/#network-connections","title":"Network connections","text":"<p>AQtive Guard maintains secure communication channels for all data exchanges:</p> <ul> <li>Web-Browser connections: AQtive Guard SaaS accepts incoming connections on TCP/443. These connections require TLS for authenticity and content fidelity. Server authenticity is verified via the signing key in every web browser\u2019s certificate-authority trust tree.</li> <li>AQtive Guard sensors: Data from native AQG sensors, such as the Filesystem Scanner or Network Analyzer, is pushed to AQtive Guard SaaS using HTTP TLS on TCP/443, which verifies server authenticity.</li> <li>Third-party data sources: Connections are initiated from the AQtive Guard SaaS backend to third-party APIs. These connections use user-defined endpoints and ports. TLS is optional for third-party integrations, depending on the specific configuration.</li> </ul>"},{"location":"manage-your-data/#data-handling","title":"Data handling","text":"<p>AQG sensors collect system observations and securely transmit summaries to AQtive Guard. The platform processes this data through a robust pipeline, ensuring sensitive information remains protected.</p> <ul> <li>Data transmission: Data is sent via TLS-wrapped HTTP. It\u2019s encapsulated in length-delimited protobuf messages, ensuring that no actual secrets are transmitted.</li> <li>Data processing: The data is processed for analysis through a pipeline of rules and filters.</li> <li>Secure storage: Processed data is stored in a secure database, accessible only by authorized API servers.</li> </ul>"},{"location":"manage-your-data/#data-retention-and-storage","title":"Data retention and storage","text":"<p>We handle your data with a focus on balancing the ability to recall historical information for purposes like audits and trend analysis with robust data privacy and security practices. Our approach ensures that we retain data only as long as necessary, optimizing for both utility and the protection of your information.</p>"},{"location":"manage-your-data/#data-retention-periods","title":"Data retention periods","text":"<p>Our data retention periods vary depending on the type of data, categorized into five tiers:</p> <ul> <li>Very short: Up to 14 days</li> <li>Short: Up to 2 months</li> <li>Medium: Up to 6 months</li> <li>Long: Up to 1 year</li> <li>Very long: Up to 5 years.</li> </ul> <p>For certain high-volume data streams, such as events, we limit retention to approximately 4 days. We carefully chose this short retention period to allow for system fault recovery. This ensures critical downstream effects are captured in other, more persistent data sets and minimizes the unnecessary long-term storage of transient data.</p>"},{"location":"manage-your-data/#data-storage-and-security","title":"Data storage and security","text":"<p>To maintain optimal security and efficient data management, we use tiered storage. This means that older data is automatically moved to storage tiers designed for long-term retention with appropriate security controls. This approach allows us to manage data effectively while upholding our commitment to data protection.</p>"},{"location":"manage-your-data/#data-compaction","title":"Data compaction","text":"<p>We also employ data compaction for certain ID-based topics. This process ensures that we only retain the most current and relevant version of a data element, removing older, redundant versions once they have been merged or processed. This practice contributes to data integrity and limits the unnecessary proliferation of data.</p>"},{"location":"manage-your-data/#data-security-measures","title":"Data security measures","text":"<p>AQtive Guard employs a comprehensive set of technical and organizational measures to protect your data, including:</p> <ul> <li>Encryption: We use industry-standard encryption protocols to protect your data both in transit (using protocols such as TLS) and at rest.</li> <li>Access controls: Access to customer data is strictly controlled based on the principle of least privilege. We implement role-based access control (RBAC) to ensure that only authorized personnel have access to the data necessary for their roles.</li> <li>Authentication and authorization: We use strong authentication methods to verify user identities. Authorization mechanisms ensure that users can only access resources they are permitted to use.</li> <li>Network security: Our network infrastructure is protected by firewalls, intrusion detection and prevention systems, and other security technologies to detect and prevent unauthorized access and malicious activity.</li> <li>Vulnerability management: We conduct regular vulnerability assessments and penetration testing to identify and address potential security weaknesses in our systems.</li> <li>Incident response plan: We have a well-defined incident response plan in place to effectively manage and mitigate any security incidents that may occur.</li> <li>Employee training: All AQtive Guard employees undergo regular training on data security and privacy best practices.</li> </ul>"},{"location":"manage-your-data/#data-sharing-and-third-party-integrations","title":"Data sharing and third-party integrations","text":"<p>AQtive Guard integrates with various third-party services to enhance the functionality of our platform. When you choose to integrate with these services, certain data may be shared between AQtive Guard and the third-party provider. This data sharing is governed by your agreement with the third-party provider and our internal security protocols. We ensure that any third-party providers we work with adhere to appropriate security and privacy standards.</p>"},{"location":"manage-your-data/#contact-us","title":"Contact us","text":"<p>If you have any questions or concerns about this Data Handling Policy or our data handling practices, please contact us at support@sandboxaq.com.</p>"},{"location":"use-conditions-statement/","title":"Use conditions statement","text":"<p>AQtive Guard screen text and Knowledge Base documents are based on guidance from the cited recognized standards and industry sources.</p> <p>However, cybersecurity risk management differs significantly across organizations due to the nature of the information on your systems, your commitments to customers or end users, local laws, unique threats, vulnerabilities, and risk tolerances.</p> <p>You are responsible for consulting your own security, compliance, and legal advisors to assess the applicability of any observations, insights, and/or results obtained through the use of AQtive Guard for your organization, as well as determining appropriate implementation and responses in accordance with internal policies, industry practices, and regulatory requirements.</p>"},{"location":"aqg-protect/","title":"AQtive Guard Protect","text":"<p>AQtive Guard (AQG) Protect extends the comprehensive visibility you get from AQtive Guard Discover by providing automated cryptographic management capabilities. It centralizes certificate management into a single framework, simplifying operations and ensuring full visibility and control over how certificates are created, stored, and used.</p> <p>Without proper lifecycle management, observability, and automation, certificates can become systemic vulnerabilities. AQG Protect addresses these challenges with features like automated short-lived certificate rotation and seamless integration, helping you mitigate risks and streamline operations.</p>"},{"location":"aqg-protect/#aqg-protect-navigation","title":"AQG Protect navigation","text":"<p>When you open AQG Protect, you\u2019ll find four main tabs designed to facilitate comprehensive cryptographic management. These tabs provide tools for monitoring, policy enforcement, and configuration:</p> <ul> <li>Dashboard - Offers an overview of your cryptographic management posture and key metrics.  </li> <li>Certificates - Provides tools for managing your certificates, including their lifecycle and rotation.  </li> <li>Policy templates - Allows you to define and apply cryptographic policies.  </li> <li>Configuration - Contains settings for AQG Protect.</li> </ul> <p>The following sections provide details on each tab.</p>"},{"location":"aqg-protect/#dashboard","title":"Dashboard","text":"<p>The Dashboard is your central hub for the AQG Protect module, providing immediate insights into your cryptographic operations and certificate health. Here you\u2019ll find a quick overview of key metrics, helping you monitor active workloads and proactively manage certificate lifecycles.</p> <p>Tip</p> <p>In AQtive Guard Protect, a workload refers to any application, service, or process that performs cryptographic signature operations.</p> <p>On the Protect dashboard, you\u2019ll find four main views:</p> <ul> <li>Workload usage - Displays cryptographic signature operations over the last 48 hours.  </li> <li>Most active workloads - Identifies the most active workloads over the last 48 hours.  </li> <li>Certificate expiration timeline - A graph visualizing certificate expiration trends, showing data for:  <ul> <li>Certificates expired in the last 7 days.  </li> <li>Certificates expiring within 7 days.  </li> <li>Certificates expiring in the next 8-30 days.  </li> </ul> </li> <li>Certificate expiration summary - Provides a breakdown of certificate statuses into four categories:  <ul> <li>Expired.  </li> <li>Expiring within 7 days.  </li> <li>Expiring in 8-30 days.  </li> <li>Expiring in 30+ days.</li> </ul> </li> </ul>"},{"location":"aqg-protect/#certificates","title":"Certificates","text":"<p>The Certificates tab provides a centralized view of all certificates currently managed by AQG Protect. Here, you\u2019ll find comprehensive details about each certificate, allowing you to monitor their status and apply management policies. </p> <p>The following details are presented for each certificate:</p> <ul> <li>Certificate CName - The certificate\u2019s Common Name (CNAME), which is typically the primary hostname or identity associated with the certificate.  </li> <li>Fully qualified name - The complete and unambiguous name of the host or resource associated with the certificate. This name must be unique.</li> <li>Policy template - The policy template currently applied to the certificate.  </li> <li>Time to live - The certificate\u2019s remaining validity period.  </li> <li>Protect setting - The specific protection settings applied by AQG Protect.  </li> <li>State - The current lifecycle status of a certificate/key pair within AQG Protect. This will either be:  <ul> <li>Active - the certificate has been enrolled and deployed.  </li> <li>Awaiting deployment - the certificate has been enrolled but not deployed.  </li> <li>Renewing - The certificate is currently in the process of being renewed.</li> </ul> </li> </ul> <p>At the end of each row, you can select Details for more information about the certificate, such as the:</p> <ul> <li>Certificate string - The complete, encoded cryptographic data of the certificate.  </li> <li>Deployment configuration - The specific settings for how a certificate is used on its deployed system or application.  </li> <li>Startup configuration - The parameters AQtive Guard Protect uses to manage a certificate when its client or agent initializes.</li> </ul>"},{"location":"aqg-protect/#policy-templates","title":"Policy templates","text":"<p>The Policy Templates tab allows you to view and manage the cryptographic policies available for comparison against your certificates. Here, you\u2019ll see which policies a certificate is using or following, ensuring compliance and alignment with security standards. The following details are presented for each policy:</p> <ul> <li>Policy name - The name of the policy, including its NIST security level and bits of security.  </li> <li>Signature algorithm - The cryptographic algorithm used for digital signatures within the policy.  </li> <li>Hash algorithm - The hashing algorithm specified by the policy.  </li> <li>Certificate count - The number of certificates currently associated with or following this policy.</li> </ul> <p>Note</p> <p>The NIST security level quantifies the strength of a cryptographic algorithm or system, typically expressed in bits, indicating the estimated computational effort required to break it.</p>"},{"location":"aqg-protect/#configuration","title":"Configuration","text":"<p>The Configuration tab displays the foundational settings for your AQG Protect infrastructure. Here, you\u2019ll find details about the parameters governing the Protect environment and the specific Root Certificate Authority (CA) certificate it\u2019s configured to use:</p> <ul> <li>Infrastructure Configuration - Displays the parameters that define how your AQG Protect infrastructure is set up.  </li> <li>CA Root Certificate - Shows the Root CA certificate that your Protect infrastructure is configured with.</li> </ul>"},{"location":"aqg-protect/getting-started/","title":"Getting started with AQG Protect","text":"<p>This guide will show you how to enroll and deploy certificates in AQG Protect. Enrolling certificates brings them under centralized control in AQtive Guard, enabling continuous monitoring, policy application, and automated lifecycle management.</p> <p>Important</p> <p>Activating a certificate in AQG protect is a 2-stage process: enrollment and deployment. After a certificate is enrolled, it appears as Awaiting deployment in AQG Protect. It becomes Active once it is successfully deployed.     </p>"},{"location":"aqg-protect/getting-started/#enroll-a-certificate","title":"Enroll a certificate","text":"<ol> <li>Log into AQtive Guard and select Protect in the main menu.  </li> <li>Select the Certificates tab, then select Enroll new certificate.</li> </ol> <p>You\u2019re now ready to choose your desired certificate management option.</p>"},{"location":"aqg-protect/getting-started/#choose-your-management-option","title":"Choose your management option","text":"<p>AQG Protect offers two options for certificate management, allowing you to choose the level of automation and control that best aligns with your operational requirements:</p> <ul> <li>Store &amp; Track - With this option, your certificate is securely stored within AQG Protect. An issue will be raised in AQtive Guard when your certificate nears expiration, so you can proactively plan manual rotation. </li> <li>Fully Managed - This option provides all the benefits of Store and Track \u2014 your certificate is securely stored with AQG Protect and its signature activity is monitored. Additionally, AQG can automatically rotate your certificates based on a defined policy and your chosen Certificate Authority (CA). You also have the flexibility to manually rotate certificates and update credentials directly within AQG.</li> </ul> <p>Note</p> <p>Currently supported CAs include Let\u2019s Encrypt and Smallstep.</p>"},{"location":"aqg-protect/getting-started/#enroll-in-store-track","title":"Enroll in Store &amp; Track","text":"<p>To enroll your certificate using the Store &amp; Track option, you\u2019ll need to obtain the following details from your certificate provider:</p> <ul> <li>FQN (Fully Qualified Name) - This is the complete, unambiguous name for your certificate. It helps AQG Protect identify and categorize the certificate for management. This name must be unique.</li> <li>Certificate (PEM) - The certificate itself, provided in Privacy-Enhanced Mail (PEM) format. This is the public part of your certificate.</li> <li>Private key (PEM) - The corresponding private key for the certificate, also in PEM format.</li> <li>Web server CNAME - The Canonical Name (CNAME) of the web server where this certificate will be deployed. This helps AQG associate the certificate with its operational environment.</li> </ul> <p>Once you\u2019ve entered these details, select Enroll Certificate.</p> <p>On the next screen, you can either deploy your certificate immediately by selecting Get deployment details, or select Do this later to defer deployment.</p>"},{"location":"aqg-protect/getting-started/#enroll-as-fully-managed","title":"Enroll as Fully Managed","text":"<p>To enroll in the Fully Managed option, you\u2019ll need to provide the details necessary for AQG Protect to interact directly with your Certificate Authority (CA) or other infrastructure components as required to automate certificate lifecycle management.</p> <p>The following information is required:</p> <ul> <li>FQN (Fully Qualified Name) - This is the complete, unambiguous name for your certificate within AQG Protect. It helps identify and categorize the certificate for management. This name must be unique.  </li> <li>ACME Challenge Type - Select the method for Automatic Certificate Management Environment (ACME) validation from the dropdown menu. This proves you control the domain for which the certificate is being issued:  <ul> <li>HTTP - (HTTP-01) The ACME CA challenges the client to host a random number at a random URL under <code>/.well-known/acme-challenge</code> on port 80. The CA verifies client control by issuing an HTTP GET request to that URL.  </li> <li>TLS - (TLS-ALPN-01) The ACME CA uses TLS to validate a challenge, leveraging application layer protocol negotiation (ALPN) in the TLS handshake. The client presents a self-signed TLS certificate containing the challenge response as a special X.509 certificate extension.  </li> </ul> </li> <li>Certificate Authority - Choose the Certificate Authority (CA) that will issue and manage your certificate. Options include:  <ul> <li>Step-CA by Smallstep </li> <li>Let\u2019s Encrypt </li> </ul> </li> <li>Security policy - Select the security policy that will govern this certificate\u2019s automated management from the dropdown menu.  </li> <li>CNAME - The Canonical Name (CNAME) for the web server where this certificate will be deployed. This helps AQG associate the certificate with its operational environment.  </li> <li>TTL (Time to Live) - Select the certificate\u2019s validity duration from the dropdown menu:  <ul> <li>4 hours  </li> <li>8 hours  </li> <li>12 hours  </li> <li>16 hours  </li> <li>20 hours  </li> <li>24 hours</li> </ul> </li> </ul> <p>Tip</p> <p>Once all information is entered correctly, the Certificate to enroll section will populate. Review the information to verify you\u2019re enrolling the correct certificate.</p> <p>When ready, select Enroll Certificate.</p> <p>On the next screen, you can either deploy your certificate immediately by selecting Get deployment details, or select Do this later to defer deployment. </p> <p>Important</p> <p>Until it is deployed, your certificate is not active and will have a status of Awaiting deployment in the Certificates tab of AQG Protect. </p>"},{"location":"aqg-protect/getting-started/#deploy-your-certificate","title":"Deploy your certificate","text":"<p>An enrolled certificate isn\u2019t active until deployed. You can deploy a certificate as a final step in the enrollment process, or at a later time. </p>"},{"location":"aqg-protect/getting-started/#deployment-during-enrollment","title":"Deployment during enrollment","text":"<p>After you click Enroll Certificate, you\u2019ll see a confirmation message that you\u2019ve enrolled your certificate. To deploy the certificate, select Get Deployment Details. This will take you directly to the newly enrolled certificate in Protect. The certificate details panel will open, displaying all the necessary deployment information you\u2019ll need to copy and paste into your environment.</p>"},{"location":"aqg-protect/getting-started/#deferred-deployment","title":"Deferred deployment","text":"<p>When you select Do this later at the time of enrollment, your certificate will be enrolled but inactive. It will appear as Awaiting deployment in AQG Protect.</p> <p>When you\u2019re ready to deploy your certificate:</p> <ol> <li>From the main menu select Protect.   </li> <li>In the Certificates tab, locate the certificate you want to deploy.  </li> <li>Select Details at the end of the row to display the deployment information you\u2019ll need to copy and paste into your environment.</li> </ol>"},{"location":"aqg-protect/getting-started/#deployed-certificates","title":"Deployed certificates","text":"<p>A deployed certificate has an Active status in the Certificates tab in AQG Protect. Protect setting indicates whether the certificate is enrolled in Store &amp; Track or Fully managed. </p> <p>Once the certificate has been deployed using either option, you\u2019ll see two related fields in the certificate details:</p> <ul> <li>Data sources - this will be either AQG Protect - Store &amp; Track or AQG Protect - Fully Managed.  </li> <li>AQG Protect status - the operational status of the certificate, with a link to view the certificate in the Protect area.</li> </ul>"},{"location":"aqtive-guard-sensors/","title":"AQtive Guard sensors","text":"<p>AQtive Guard (AQG) sensors power in-depth cryptographic analysis of filesystems, applications, and physical and virtual networks: </p> <ul> <li>AQG Filesystem Scanner - Scans filesystems and container images to create a trace file containing cryptographic data.</li> </ul> <ul> <li>AQG Java Tracer - Logs cryptographic calls made by a Java Virtual Machine (JVM) and its associated Java application, generating a trace file with cryptographic data.</li> </ul> <ul> <li>AQG Network Analyzer - Analyzes cryptographic activity within network traffic and powers AQtive Guard analysis for all sensors and data sources.</li> </ul> <p>For detailed information for using each sensor, refer to the corresponding section in this user guide.</p>"},{"location":"aqtive-guard-sensors/#download-an-aqg-sensor","title":"Download an AQG sensor","text":"<p>You can download AQG sensors from the AQtive Guard UI.</p> <p></p> <p>To download:</p> <ol> <li>Select Data sources from the main menu. </li> <li>In the tile for the desired sensor, select Download AQG Sensor. </li> </ol>"},{"location":"aqtive-guard-sensors/#upload-a-trace","title":"Upload a trace","text":"<p>You can upload trace files from the following AQG sensors using the AQtive Guard UI or the API.</p>"},{"location":"aqtive-guard-sensors/#upload-a-trace-using-the-ui","title":"Upload a trace using the UI","text":"<p>To upload a trace file from the UI, follow these steps:</p> <ol> <li>Navigate to Data Sources from the main menu, then select Upload in the tile corresponding to the AQG sensor that generated the trace: <ul> <li>AQG Filesystem Scanner</li> <li>AQG Java Tracer</li> </ul> </li> <li>To upload the <code>.cst</code> or <code>.gz</code> trace file, either:<ul> <li>Click in the target area and select the file from your local system.</li> <li>Drag and drop the file into the target upload area.</li> </ul> </li> </ol> <p>Note</p> <p>The maximum size for the trace file is 4GB. </p>"},{"location":"aqtive-guard-sensors/#upload-a-trace-using-the-api","title":"Upload a trace using the API","text":"<p>To upload a trace using the AQG API, you\u2019ll need: </p> <ul> <li>An AQG uploader token. Refer to Settings for instructions to retrieve the token. </li> <li>cURL or another configurable HTTP client.</li> </ul>"},{"location":"aqtive-guard-sensors/#upload-command","title":"Upload command","text":"<p>To upload your trace file, construct a shell command using the following as an example:</p> Bash<pre><code>export AQG_API_TOKEN=&lt;uploader_token&gt;\ncurl \\\n    -XPOST \\\n    -H \"Authorization: Bearer $AQG_API_TOKEN\" \\\n    -H 'Content-Type: application/jsonl' \\\n    -H \"Content-Encoding: gzip\" \\\n    --data-binary \"@&lt;path/to/example.cst.gz&gt;\" \\\n    \"https://&lt;aqg host&gt;/agent/trace/v0?slotid=slot$(hostname)&amp;sessionid=$(uuidgen)&amp;assetid=$(hostname)\"\n</code></pre> <p>Replace the following placeholders:</p> <ul> <li><code>&lt;uploader_token&gt;</code> - your retrieved uploader API token.</li> <li><code>&lt;path/to/example.cst.gz&gt;</code> - the path to your trace file.</li> <li><code>&lt;aqg_host&gt;</code> - the hostname of your AQtive Guard instance.</li> </ul> <p>Important</p> <p>The <code>@</code> preceding <code>&lt;path/to/example.cst.gz&gt;</code> must be included in the file path. The <code>@</code> prefix is necessary for cURL to properly interpret the contents of the data file. For example, a file located at <code>home/other_dir/my_file.cst.gz</code> becomes <code>@home/other_dir/my_file.cst.gz</code>.</p> <p>The following values are all generated automatically based on the hostname and other system information:</p> <ul> <li><code>slotid</code> - Combines slot prefix with hostname: <code>slot$(hostname)</code>. Acts as a container identifier for grouping related sessions. Example: <code>slotserver-myserver-01</code></li> <li><code>sessionid</code> - A UUID (Universally Unique Identifier). Must be unique per session to avoid data collisions. Example: <code>550e8400-e29b-41d4-a716-446655440000</code></li> <li><code>assetid</code> - Typically, the hostname of the machine. Uniquely identifies the device/system generating events. Example: <code>myserver-01</code>.</li> </ul> <p>Tips</p> <ul> <li>You can run the export command independently and then use multiple cURL invocations to upload multiple files as long as the same shell session remains open.</li> <li>Trace names aren\u2019t explicitly specified in the cURL command, but using unique and descriptive names will help you identify and organize trace files. </li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/","title":"AQG Filesystem Scanner","text":"<p>The AQtive Guard Filesystem Scanner is a command-line application that scans the filesystem or a container image to find cryptographic material. The logged information is formatted for analysis by AQtive Guard.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/#requirements","title":"Requirements","text":"<ul> <li>Windows Server x64</li> <li>Linux x64 and kernel &gt;= 4.0 </li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/#how-it-works","title":"How it works","text":"<p>Starting from the chosen root location, the AQG Filesystem Scanner goes through every file in the directory tree below, recursively descending into all subdirectories (without pursuing symbolic links). It tests the initial bytes of each file against a set of detectors for supported formats.</p> <p>Note</p> <p>The AQG Filesystem Scanner alters the access timestamp of files but won\u2019t alter the modification and change timestamps. Additionally, the scanner only performs read-only actions, so it won\u2019t lock files for other processes. A file with a mandatory or exclusive lock placed on it by another process will be skipped by the scanner, but the scanner will be able to access and open files with advisory locks on them.</p> <p>It parses supported files and logs the cryptographic material in a format suitable for analysis by AQtive Guard. No sensitive data, such as private keys, is stored. In the case of encrypted keystores, if a password is provided, the AQG Filesystem Scanner also attempts to decrypt the encrypted portions using that password.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/linux-getting-started-guide/","title":"Linux Filesystem Scanner getting started guide","text":"<p>This guide explains how to use the AQtive Guard Filesystem Scanner to obtain a cryptography scan from the filesystem in Linux.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/linux-getting-started-guide/#installation","title":"Installation","text":"<p>You can download the AQG Filesystem Scanner from the web UI. </p> <p>The sensor is distributed as a zip package named <code>cs-host-scanner-&lt;VERSION&gt;-x86_64-&lt;PLATFORM&gt;.zip</code>. When you extract this package, it creates a directory named <code>cs-host-scanner-&lt;VERSION&gt;-x86_64-&lt;PLATFORM&gt;</code> that contains an executable <code>cs-host-scanner</code> file and a <code>README</code> file. For instance:</p> <pre><code>cs-host-scanner-0.9.6-x86_64-linux/\n\u251c\u2500\u2500 cs-host-scanner\n\u2514\u2500\u2500 README.md\n</code></pre> <p>You can move the <code>cs-host-scanner-&lt;VERSION&gt;-x86_64-&lt;PLATFORM&gt;</code> directory anywhere on your system.</p> <p>Caution</p> <p>If you move the executable file, make sure to also move the DLLs. They must be in the same directory.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/linux-getting-started-guide/#scanning-a-filesystem","title":"Scanning a Filesystem","text":"<p>Navigate to a directory where you have write permissions to store scan results.</p> <p>Move the <code>cs-host-scanner</code> executable file to your chosen directory and run:</p> Bash<pre><code>./cs-host-scanner \\\n    --root /path/to/root/directory \\\n    --output scan.cst.gz\n</code></pre> <p>Note</p> <p>The AQG Filesystem Scanner only looks for regular files on Linux. It won\u2019t scan block devices such as <code>/dev/sda</code>, but it will scan regular files in directories like <code>/proc</code> or <code>/sys</code>.</p> <p>When the AQG Filesystem Scanner has finished executing, the directory you chose earlier will contain the generated trace file.</p> <p>You can change the directory where the AQG Filesystem Scanner generates traces with the <code>--output</code> option.</p> <p>Note</p> <p>Refer to the AQG Filesystem Scanner reference for details on using the scanner on Linux and for a list of available parameters.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/linux-getting-started-guide/#upload-a-trace","title":"Upload a trace","text":"<p>Refer to these instructions for uploading a trace using the web UI or API.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/","title":"AQG Filesystem Scanner reference","text":"<p>The AQtive Guard Filesystem Scanner is a command-line application designed to scan the filesystem or a container image, identifying cryptographic material and logging it in a format suitable for analysis with AQtive Guard.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#scanning","title":"Scanning","text":""},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#scanning-a-filesystem","title":"Scanning a filesystem","text":"<p>For information on scanning a filesystem, refer to the Windows or Linux getting started guides.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#scanning-a-container-image","title":"Scanning a container image","text":"<p>To run the AQG Filesystem Scanner on a container image, substitute the <code>--root</code> option with the <code>--image-name</code> option. For instance:</p> Bash<pre><code>./cs-host-scanner \\\n  --image-name python:latest \\\n  --output scan.cst.gz\n</code></pre> <p>Note</p> <p>This feature isn\u2019t available for Windows.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#configuration","title":"Configuration","text":"<p>The AQG Filesystem Scanner is configured through command-line options. For instance, to set the directory where the AQG Filesystem Scanner will write the resulting trace, use the <code>--output</code> option:</p> Bash<pre><code>./cs-host-scanner \\\n  --root /path/to/root/directory \\\n  --output path/to/scan.cst.gz\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#command-line-options","title":"Command-line options","text":"<p>Below is a list of available command-line options:</p> <ul> <li><code>-r</code>, <code>--root</code>: Specify the directory path to start searching from if the scan target is a   filesystem. The AQG Filesystem Scanner will traverse everything below this designated point. This can be specified   multiple times.</li> </ul> <ul> <li><code>--image</code>, <code>--image-name</code>: The image name if the scan target is a container.   This option is also mutually exclusive with the <code>--root</code> and <code>--scan-windows-stores</code> options. That is, you can use one or the other in a single command, but not both.</li> </ul> <ul> <li><code>-o</code>, <code>--output</code> (required): Specify the file to write the trace to. This is a trace in .cst format,   which can be uploaded to the web application and analyzed to generate a cryptography   usage report.</li> </ul> <ul> <li><code>--max-file-size</code>: The cutoff size in bytes for files to be scanned (not applicable to ZIP   files). The default is 1000000 (1MB). If set to <code>0</code>, the cutoff is disabled.</li> </ul> <ul> <li><code>-m</code>, <code>--module</code> (default: <code>default</code>): The choices are <code>pem</code>, <code>der</code>, <code>ssh</code>, <code>jks</code>, <code>jceks</code>,   <code>keys</code> (representing <code>pem</code>, <code>der</code> and <code>ssh</code>), <code>keystore</code> (representing <code>jks</code> and <code>jceks</code>),   <code>jar</code> and <code>default</code> (which covers all modules except <code>jar</code>). You can include this more than   once to specify multiple modules to use.</li> </ul> <ul> <li><code>--sc</code>, <code>--static-scanner-path</code>: The path to the Static Scanner binary, required   for running the AQG Filesystem Scanner with <code>--module jar</code>.</li> </ul> <ul> <li><code>-p</code>, <code>--password</code>: An optional password to be used while attempting decryption of encrypted   data. This can be specified multiple times.</li> </ul> <ul> <li><code>--allow-secrets-in-trace</code>: Allow secrets to be included in the generated trace. This only applies to the password    provided using the <code>--password</code> command-line option.</li> </ul> <ul> <li><code>-t</code>, <code>--tag</code>: Assign tags to categorize the generated trace. This can be specified   multiple times.</li> </ul> <ul> <li> <p><code>-x</code>, <code>--exclude</code>: Exclude a file or directory from a scan. Wildcards can be used to   specify a pattern: </p> <ul> <li><code>*</code> matches any sequence of characters within a filename. </li> </ul> <ul> <li><code>**</code> matches any sequence of characters within a file path. You can     specify this multiple times to exclude several files, directories, or patterns. An example is shown in      Multiple file exclusion.</li> </ul> </li> </ul> <ul> <li><code>-l</code>, <code>--max-files-per-second</code>: Set the limit on the number of files scanned per second. The default value is <code>0</code> which is no limit.</li> </ul> <ul> <li><code>-w</code>, <code>--work-load</code>: Define the limit as a percentage on the CPU load of the AQG Filesystem Scanner during   its execution. The default value is <code>100</code>, which is no limit.</li> </ul> <ul> <li><code>-q</code>, <code>--quiet</code>: Print less information. You can use this option multiple times. The opposite of <code>-v</code>.</li> </ul> <ul> <li><code>-v</code>, <code>--verbose</code>: Print more information. You can use this option multiple times. The opposite of <code>-q</code>.</li> </ul> <ul> <li><code>--help</code>: Display a list of all command-line options with an explanation of what they do.</li> </ul> <ul> <li><code>--scan-windows-stores</code>: Allow the AQG Filesystem Scanner to discover X.509 certificates managed using Windows Microsoft Management Console (MMC). The <code>CurrentUser</code> and <code>LocalMachine</code> locations are scanned and any stores with non-ASCII characters in their name are skipped.   Keep in mind that the name of the certificate shown may differ from what is in MMC. The names   shown in your session are the true names of the certificate store, while the names shown in MMC are the names of the certificate stores you may reference with the <code>CERT:drive</code>.</li> </ul> <ul> <li><code>--io-kbps-limit</code>: Set a limit on the IOPS and allow throttling of file I/0 operations. The default is no limit.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#examples","title":"Examples","text":"<p>Below are some configuration examples:</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#help-command","title":"Help command","text":"Bash<pre><code>./cs-host-scanner --help\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#pkcs12-keystore-decryption","title":"PKCS#12 Keystore Decryption","text":"Bash<pre><code>./cs-host-scanner \\\n  --root /path/to/root/directory \\\n  --password 'This is a command\\-line password' \\\n  --output path/to/scan.cst.gz\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#io-and-cpu-load-limitation","title":"I/O and CPU load limitation","text":"Bash<pre><code>./cs-host-scanner \\\n  --root /path/to/root/directory \\\n  -l 2500 -w 60. \\\n  --output path/to/scan.cst.gz\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#network-directory-exclusion","title":"Network directory exclusion","text":"<p>The scanner doesn\u2019t avoid any filesystem by default, but you can use the <code>--exclude</code> option to avoid scanning a network filesystem.</p> Bash<pre><code>./cs-host-scanner \\\n  --root / \\\n  --exclude /path/to/network/directory \\\n  --output path/to/scan.cst.gz\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#multiple-file-exclusion","title":"Multiple file exclusion","text":"<p>This example shows how to exclude several files from a scan by specifying multiple exclude patterns:</p> <ul> <li>Exclude zip files located in any subdirectories of <code>/foo</code></li> <li>Exclude every <code>PEM</code> file no matter which directory they\u2019re in</li> <li>Exclude all <code>Thumbs.db</code> files</li> </ul> Bash<pre><code>./cs-host-scanner \\\n  --root / \\\n  --exclude \"/foo/**/*.zip\" \\\n  --exclude \"**.pem\" \\\n  --exclude \"**/Thumbs.db\" \\\n  --output path/to/scan.cst.gz\n</code></pre> <p>Note</p> <p>The <code>*</code> characters need to be escaped with quotes when using a Linux shell.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#tags","title":"Tags","text":"Bash<pre><code>./cs-host-scanner \\\n  --root /path/to/root/directory \\\n  --tag \"Tag 1\" --tag \"Tag 2\" \\\n  --output path/to/scan.cst.gz\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#host-system-antivirus-exclusions","title":"Host system antivirus exclusions","text":"<p>Using the AQG Filesystem scanner with antivirus monitoring may increase the CPU load on the endpoints by more than 3x. The AQG Filesystem Scanner detects cryptographic assets such as keys and certificates in file systems or container images. This requires scanning a large volume of files, which antivirus software may flag unless exclusions are properly configured.</p> <p>To maximize scan performance and minimize the impact on system resources, we recommend excluding the scanner process. The following example provides detailed steps and considerations for configuring exclusions in Microsoft Defender Antivirus.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#configuring-microsoft-defender-antivirus","title":"Configuring Microsoft Defender Antivirus","text":"<p>To avoid false positives in Microsoft Defender Antivirus when using the AQG Filesystem Scanner on-premises, you must configure exclusions for file extensions, folder locations, and specific processes.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#configure-file-and-folder-exclusions","title":"Configure File and Folder Exclusions","text":"<p>Exclusions prevent Microsoft Defender Antivirus from scanning specified files, folders, or processes. This avoids unnecessary CPU load and potential conflicts.</p> <ul> <li> <p>Folder Exclusions - Avoid scanning entire directories that contain numerous files unrelated to security threats but necessary for AQG Filesystem Scanner operations. For instance:</p> <ul> <li>Windows: C:\\Program Files (x86)\\cryptosense\\</li> </ul> <ul> <li>Linux: /opt/cryptosense/</li> </ul> </li> </ul> <ul> <li> <p>File Exclusions - Target specific executable files related to the AQG Filesystem Scanner to prevent them from being scanned:</p> <ul> <li>Windows: C:\\Program Files (x86)\\cryptosense\\bin\\cs-host-scanner.exe</li> </ul> <ul> <li>Linux: /opt/cryptosense/cs-host-scanner</li> </ul> </li> </ul> <p>These exclusions can be set up through the Windows Security app or using PowerShell with commands like <code>Add-MpPreference</code> to add exclusions.</p> <p>More details on managing these settings through PowerShell and Windows Management Instrumentation (WMI) are available in the Microsoft document Configure and validate exclusions based on file extension and folder location.</p> <p>Note</p> <p>If you use a different antivirus software, refer to its documentation for specific details and best practices.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#configure-process-exclusions","title":"Configure Process Exclusions","text":"<p>Excluding specific processes is crucial to ensure that the antivirus doesn\u2019t interfere with background services like those of the AQG Filesystem Scanner.</p> <ul> <li> <p>Process Exclusions by Path - Use the full path to the executable to ensure accuracy and prevent other potentially malicious programs with the same name from being excluded. For example, using PowerShell:</p> <ul> <li><code>Add-MpPreference -ExclusionProcess \"C:\\Program Files (x86)\\cryptosense\\bin\\cs-host-scanner.exe\"</code></li> </ul> </li> </ul> <ul> <li>Using Wildcards - Wildcards can be used in path exclusions to generalize the exclusions where applicable. Be cautious when using wildcards to avoid overly broad exclusions that might compromise security\u200b\u200b\u200b\u200b.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#configure-exclusions-through-group-policy","title":"Configure Exclusions through Group Policy","text":"<p>Managing exclusions through Group Policy offers organizations centralized control over antivirus settings across multiple systems.</p> <ul> <li>Group Policy Management - This method involves setting antivirus configurations at the group level and provides a robust way to ensure consistent security settings across all endpoints. Detailed instructions for setting up exclusions via Group Policy are found in Microsoft\u2019s official guides\u200b\u200b. Refer to the links at the end of this document.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#regular-review-and-adjustment","title":"Regular Review and Adjustment","text":"<p>For optimal performance and security, it\u2019s essential to regularly review and update exclusion settings to adapt to any changes in the file system layout, AQG Filesystem Scanner updates, or antivirus capabilities.</p> <ul> <li>Review Settings - Review the exclusions list periodically to ensure it still aligns with current operational requirements and doesn\u2019t inadvertently exclude required security scans\u200b\u200b.</li> <li>Adjust as Necessary - Modify the exclusions as the system evolves, such as after software updates or changes in system architecture.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#documentation-and-compliance","title":"Documentation and Compliance","text":"<p>Ensure that all configurations are well-documented and comply with organizational IT security policies. Consider the implications of excluding certain files or processes on the overall security posture of the organization.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/reference/#additional-information","title":"Additional information","text":"<p>For more comprehensive guides on configuring and best practices for Microsoft Defender Antivirus, please refer to Microsoft\u2019s official documentation:</p> <ul> <li>Configure and validate exclusions based on extension, name, or location</li> <li>Review or edit your next-generation protection policies</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/","title":"AQG Filesystem Scanner supported formats","text":""},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#ssh-keys","title":"SSH keys","text":""},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#ssh-2-public-keys","title":"SSH-2 public keys","text":"<p>OpenSSH public key or known_hosts format using one of the following algorithms:</p> <ul> <li>ssh-rsa</li> <li>ssh-dss</li> <li>ecdsa-sha2-nistp256</li> <li>ecdsa-sha2-nistp384</li> <li>ecdsa-sha2-nistp521</li> <li>ssh-ed25519</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#putty-private-key-ppk-files","title":"PuTTY Private Key (PPK) files","text":"<p>The algorithms supported are the same as those supported for SSH-2 public keys.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#pem-or-der-encoded-asn1-files","title":"PEM- or DER-encoded ASN.1 files","text":"<ul> <li>X.509 certificates (containing RSA, EC or DSA public keys)</li> <li>X.509 format public keys</li> <li>PKCS#8 encrypted and unencrypted private keys</li> <li>RSA public and private keys</li> <li>DSA private keys</li> <li>Elliptic curve private keys</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#keystores","title":"Keystores","text":"<p>In the initial release, keystore objects are inventoried, but their contents (keys and certificates) are not included in the inventory. Supported keystores include:</p> <ul> <li>JavaKeystores (JKS, JCEKS, BKS, BKS_v1, UBER, BCFKS)</li> <li>PKCS#12</li> <li>Microsoft Serialized Certificate Stores (SST)</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#pkcs7-files","title":"PKCS#7 files","text":"<p>PKCS#7 files are not included in the inventory, but any extracted keys will be inventoried.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#pgp-files","title":"PGP files","text":"<ul> <li>RSA public keys</li> <li>DSA public keys</li> <li>ElGamal public keys</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/supported-formats/#zip-files","title":"ZIP files","text":"<p>When the AQG Filesystem Scanner encounters a <code>.zip</code> file, it scans the files inside it. It conducts the same checks as it would for typical files on a filesystem, with the following limitations:</p> <ul> <li>JAR files inside ZIP archives aren\u2019t supported.</li> <li>ZIP files inside ZIP archives aren\u2019t supported.</li> </ul> <p>It\u2019s also worth noting that the AQG Filesystem Scanner only supports files that follow the original PKZIP file format specification, without spanning across multiple files. In particular, it doesn\u2019t support ZIP64 archives and the following limitations apply:</p> <ul> <li>A maximum of 2<sup>16</sup> files can be inside the ZIP archive.</li> <li>The maximum compressed size is 4 GB for each entry.</li> <li>The maximum uncompressed size is 4 GB for each entry.</li> <li>The maximum overall size of the ZIP archive is 4 GB.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/windows-getting-started-guide/","title":"Windows Filesystem Scanner getting started guide","text":"<p>This guide explains how to use the AQG Filesystem Scanner to obtain a cryptography scan from the filesystem in Windows.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/windows-getting-started-guide/#installation","title":"Installation","text":"<p>You can download the AQG Filesystem Scanner from the web UI. </p> <p>The sensor is distributed as a zip package named <code>cs-host-scanner-&lt;VERSION&gt;-x86_64-&lt;PLATFORM&gt;.zip</code>. When you extract this package, it creates a directory named <code>cs-host-scanner-&lt;VERSION&gt;-x86_64-&lt;PLATFORM&gt;</code> that contains an executable <code>cs-host-scanner</code> file, the required libraries for Windows, and a <code>README</code> file. For instance:</p> <pre><code>cs-host-scanner-0.9.6-x86_64-windows\\\n\u251c\u2500\u2500 cs-host-scanner.exe\n\u251c\u2500\u2500 libffi-6.dll\n\u251c\u2500\u2500 libgmp-10.dll\n\u251c\u2500\u2500 zlib1.dll\n\u2514\u2500\u2500 README.md\n</code></pre> <p>You can move the <code>cs-host-scanner-&lt;VERSION&gt;-x86_64-&lt;PLATFORM&gt;</code> directory anywhere on your system.</p> <p>Caution</p> <p>If you move the executable file, make sure to also move the DLLs. They must be in the same directory.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/windows-getting-started-guide/#scanning-a-filesystem","title":"Scanning a Filesystem","text":"<p>Navigate to a directory where you have write permissions to store scan results.</p> <p>Move the following executable file and libraries to your chosen directory:  - <code>cs-host-scanner.exe</code>  - <code>libffi-6.dll</code>  - <code>libgmp-10.dll</code>  - <code>zlib1.dll</code></p> <p>then run:</p> PowerShell<pre><code>.\\cs-host-scanner.exe `\n    --root \\path\\to\\a\\root\\directory `\n    --root \\path\\to\\another\\root\\directory `\n    --output scan.cst.gz\n</code></pre> <p>Note</p> <p><code>--root</code> parameter can be provided multiple times, for instance once for each available drive.</p> <p>When the AQG Filesystem Scanner has finished executing, the directory you chose earlier will contain the generated trace file.</p> <p>You can change the directory where the AQG Filesystem Scanner generates traces with the <code>--output</code> option.</p> <p>Note</p> <p>Refer to the AQG Filesystem Scanner reference for details on scanning .NET Framework applications and for a list of available parameters.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/windows-getting-started-guide/#scanning-windows-certificate-stores","title":"Scanning windows certificate stores","text":"<p>The AQG Filesystem Scanner can scan Windows Certificate Stores for certificates. When configured, it scans all available stores under the <code>CurrentUser</code> and <code>LocalMachine</code> locations.</p> <p>In order to achieve this, navigate to a directory where you have write permissions to store scan results.</p> <p>Move the following executable file and libraries to your chosen directory:  - <code>cs-host-scanner.exe</code>  - <code>libffi-6.dll</code>  - <code>libgmp-10.dll</code>  - <code>zlib1.dll</code></p> <p>then run:</p> PowerShell<pre><code>.\\cs-host-scanner.exe `\n    --scan-windows-stores\n    --output scan.cst.gz\n</code></pre> <p>Note</p> <p><code>--root</code> and <code>--scan-windows-stores</code> can be used together, for instance to scan multiple drives and Windows certificate stores in a single command.</p>"},{"location":"aqtive-guard-sensors/aqg-filesystem-scanner/windows-getting-started-guide/#upload-a-trace","title":"Upload a trace","text":"<p>Refer to these instructions for uploading a trace using the web UI or API.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/","title":"AQG Java Tracer","text":"<p>The AQtive Guard Java Tracer is a tool that logs cryptographic calls made by a Java Virtual Machine (JVM) and its associated Java application. The logged information is formatted for analysis by AQtive Guard.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/#requirements","title":"Requirements","text":"<ul> <li>Java 8 or higher</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/#download","title":"Download","text":"<p>You can download the AQG Java Tracer from the web UI. </p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/#installation","title":"Installation","text":"<p>When you extract the <code>cs-java-tracer-.zip</code> package, it creates a directory named <code>cs-java-tracer-&lt;VERSION&gt;</code> that contains a JAR file <code>cs-java-tracer.jar</code>. For instance:</p> <pre><code>cs-java-tracer-1.8.2/\n\u2514\u2500\u2500 cs-java-tracer.jar\n</code></pre> <p>Tip</p> <p>You can move the <code>cs-java-tracer.jar</code> file to anywhere on your system. It doesn\u2019t have to remain in the <code>cs-java-tracer-&lt;VERSION&gt;</code> directory.</p> <p>Caution</p> <p>Renaming the tracer JAR file can cause the trace to fail. If you must rename the JAR file, refer to Renaming the Tracer Jar File in the AQG Java Tracer reference for instructions.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/getting-started/","title":"AQG Java Tracer getting started guide","text":"<p>This guide explains how to use the AQG Java Tracer to obtain a cryptography trace from an application.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/getting-started/#prerequisites","title":"Prerequisites","text":"<p>Before using the AQG Java Tracer, make sure you\u2019ve followed the installation instructions.</p> <p>You\u2019ll also need a Java application that you can run in a terminal with the <code>java</code> command. For other Java applications, refer to the AQG Java Tracer reference.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/getting-started/#tracing-the-application","title":"Tracing the application","text":"<p>Navigate to a directory where you have write permissions and execute your app with the <code>java</code> command as you typically would, but this time, include the <code>-javaagent:/path/to/cs-java-tracer.jar</code> command-line parameter.</p> <p>For instance, you can run the following command if your application doesn\u2019t require any other command-line parameters:</p> <pre><code>java -javaagent:/path/to/cs-java-tracer.jar -jar /path/to/application.jar\n</code></pre> <p>When your application has finished executing, the <code>cs-tracer</code> directory will contain the generated trace file(s):</p> <pre><code>cs-tracer\n\u2514\u2500\u2500 cs-trace_2022-06-02-13-30-53-113_79629.cst.gz\n</code></pre> <p>Note</p> <p>When an application shuts down properly, the trace will stop and finalize before the shutdown process begins, meaning the shutdown itself will not be included in the trace. If the application is terminated abruptly, the resulting <code>gzip</code> file may be missing a trailer and appear corrupted. However, AQtive Guard should still be able to analyze the trace contents.</p> <p>Tip</p> <p>Additional configuration is required for Java applications launched from within application servers. See Using the tracer in application frameworks for details.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/getting-started/#upload-a-trace","title":"Upload a trace","text":"<p>Refer to these instructions for uploading a trace using the web UI or API.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/","title":"AQG Java Tracer reference","text":""},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#configuration","title":"Configuration","text":"<p>You can configure the AQG Java Tracer with JVM properties using the <code>-D</code> parameter in the command line.</p> <p>For instance, to specify the directory for trace writing, use the <code>cryptosense.agent.out</code> parameter as follows:</p> <pre><code>java \\\n    -Dcryptosense.agent.out=/path/to/trace-directory \\\n    -javaagent:/path/to/cs-java-tracer.jar \\\n    -jar /path/to/application.jar\n</code></pre> <p>With Java 9 and later, you can also pass these properties via the <code>JDK_JAVA_OPTIONS</code> environment variable. For example:</p> <pre><code>JDK_JAVA_OPTIONS='-javaagent:/path/to/cs-java-tracer.jar \u2026' \\\n    java -jar /path/to/application.jar\n</code></pre> <p>This is more flexible as it allows you to set the properties without modifying your application startup script. However, it is not supported by all JREs.</p> <p>If you\u2019re not using the <code>java</code> command to launch your application directly, you can usually set those properties where you added the <code>javaagent:/path/to/cs-java-tracer.jar</code> parameter. Refer to the examples below.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#parameters","title":"Parameters","text":"<p>The supported properties are:</p> <ul> <li><code>cryptosense.agent.out</code>: path to the directory where traces are written. (default is <code>cs-tracer</code>). Make sure the application under test has write permissions in this directory.</li> <li><code>cryptosense.agent.compress</code>: on-the-fly gzip-compression of outputted JSON files (default is <code>true</code>). Note that you can upload either compressed or uncompressed traces to the analyzer.</li> <li><code>cryptosense.agent.unlimitedTraceSize</code>: Deprecated. Use <code>cryptosense.agent.maxTraceSize</code>.</li> <li><code>cryptosense.agent.maxTraceSize</code>: set a maximum uncompressed trace size in MB. The default is 4096, i.e. 4GB. Setting a value of 0 means there\u2019s no limit.</li> <li><code>cryptosense.agent.prefix</code>: optional custom file name prefix for traces (default is <code>cs-trace</code>). The file name will consist of this prefix followed by an underscore and timestamp, and have a <code>.cst</code> or <code>.cst.gz</code> extension, depending on the compression. For example, if the prefix is <code>testprefix</code>, the file name would be <code>testprefix_2018-12-13-08-42-33-428_11435.cst</code> (depending on the trace time).</li> <li><code>cryptosense.agent.trace</code>: determines whether to include stack traces for each call in the report (default is <code>true</code> ). Setting this to <code>false</code> significantly reduces the trace file size, but the report will lack important information.</li> <li><code>cryptosense.agent.ignoreUpdate</code>: determines whether to discard the calls to various <code>update()</code> functions (such as <code>MessageDigest.update</code>). The default is <code>false</code>. Setting this to <code>true</code> significantly reduces the trace file size, but the report will lack important information.</li> <li><code>cryptosense.agent.excludeBuiltins</code>: determines whether to include the calls to certain internal crypto functions in the JRE in the trace (default is <code>false</code> ). The excluded calls consist of hash function calls to verify JAR files upon startup, and internal hash calls for certain internal PBKDFs with extensive iterations that might quickly fill up a trace. Note that this doesn\u2019t affect the results since the calls are accounted for by the Analyzer.</li> <li><code>cryptosense.agent.tags</code>: comma-separated tags to include in the trace header. To include a space in the tag, uses single quotes like <code>cryptosense.agent.tags='spaced tag 1','spaced tag 2'</code>. The tags are written in the trace header as a list of strings (default is an empty list).</li> </ul> <p>Here are some examples with different parameters:</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#limiting-the-trace-size","title":"Limiting the trace size:","text":"<pre><code>java \\\n    -Dcryptosense.agent.out=/path/to/trace-directory \\\n    -Dcryptosense.agent.maxTraceSize=200 \\\n    -javaagent:/path/to/cs-java-tracer.jar \\\n    -jar /path/to/application.jar\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#adding-a-prefix-to-the-trace-name","title":"Adding a prefix to the trace name:","text":"<pre><code>java \\\n    -Dcryptosense.agent.out=/path/to/trace-directory \\\n    -Dcryptosense.agent.prefix=prefix \\\n    -javaagent:/path/to/cs-java-tracer.jar \\\n    -jar /path/to/application.jar\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#excluding-stack-traces-from-calls","title":"Excluding stack traces from calls:","text":"<pre><code>java \\\n    -Dcryptosense.agent.out=/path/to/trace-directory \\\n    -Dcryptosense.agent.trace=false \\\n    -javaagent:/path/to/cs-java-tracer.jar \\\n    -jar /path/to/application.jar\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#using-the-tracer-in-application-frameworks","title":"Using the Tracer in Application Frameworks","text":"<p>Java applications are often launched from within application servers. In this case, you\u2019ll need to add the necessary parameters to a config file:</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#tomcat","title":"Tomcat","text":"<p>The <code>bin/setenv.sh</code> file should be created or edited to contain:</p> <pre><code>CATALINA_OPTS=\"$CATALINA_OPTS -javaagent:/path/to/cs-java-tracer.jar -Dcryptosense.agent.&lt;PARAM&gt;=&lt;VALUE&gt;\"\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#jboss","title":"JBoss","text":"<p>For JBoss, you need to whitelist the cryptosense package in <code>standalone.conf</code>:</p> <pre><code>JBOSS_MODULES_SYSTEM_PKGS=\"${JBOSS_MODULES_SYSTEM_PKGS:+$JBOSS_MODULES_SYSTEM_PKGS,}cryptosense\"\n</code></pre> <p>You can then add:</p> <pre><code>JAVA_OPTS=\"$JAVA_OPTS -javaagent:/path/to/cs-java-tracer.jar -Dcryptosense.agent.&lt;PARAM&gt;=&lt;VALUE&gt;\"\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#weblogic","title":"WebLogic","text":"<p>Before calling Java, edit the <code>startWebLogic.sh</code> file to include:</p> <pre><code>export JAVA_OPTIONS=\"$JAVA_OPTIONS -javaagent:/path/to/cs-java-tracer.jar -Dcryptosense.agent.&lt;PARAM&gt;=&lt;VALUE&gt;\"\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#other-frameworks","title":"Other frameworks","text":"<p>Our AQG Java Tracer works with several other frameworks, including WebSphere and Firefly. Visit our support portal if you have questions or need help.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#renaming-the-tracer-jar-file","title":"Renaming the tracer JAR file","text":"<p>Renaming the JAR file is not recommended because the boot class path in the JAR manifest must match the file name exactly.</p> <p>If you do rename the JAR file, you need to modify the manifest accordingly. Otherwise, you\u2019ll encounter <code>java.lang.NoClassDefFoundError</code> during runtime. While your application may not crash, the tracer won\u2019t function correctly.</p> <p>You can rename the JAR file in one of two ways: adding a parameter to the JVM invocation or extracting and editing the JAR file manifest.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#add-a-parameter","title":"Add a parameter","text":"<p>You can rename the JAR file by adding the following parameter to the JVM invocation:</p> <pre><code>-Xbootclasspath/a:path/to/cs-java-tracer-2.2.4.jar\n</code></pre> <p>Important</p> <p>This needs to be added in addition to <code>-javaagent:path/to/cs-java-tracer-2.2.4.jar</code> in <code>JDK_JAVA_OPTIONS</code>.</p> <p>An example command when run directly on an app is shown below:</p> <pre><code>user@cs-java-tracer % java -javaagent:cs-java-tracer-2.2.4.jar -Xbootclasspath/a:cs-java-tracer-2.2.4.jar Demo\n</code></pre> <p>With this option, you don\u2019t need to edit the manifest file.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#extract-and-edit-the-manifest-file","title":"Extract and edit the manifest file","text":"<p>You can rename the JAR file by extracting the <code>META-INF/MANIFEST.MF</code> file from the JAR and changing the boot class path line to:</p> <pre><code>Boot-Class-Path: new-name.jar\n</code></pre> <p>The command above renames the JAR file to <code>new-name.jar</code>.</p> <p>You\u2019ll then need to replace the old <code>META-INF/MANIFEST.MF</code> file with the modified version.</p>"},{"location":"aqtive-guard-sensors/aqg-java-tracer/reference/#supported-operations","title":"Supported operations","text":"<p>The AQG Java Tracer covers methods from the following classes for any cryptographic providers conforming to the Java Cryptography Architecture (JCA), for example, Bouncy Castle Java, conscrypt and other provider implementations. This coverage is at the JCA/JCE level, but there are some algorithm implementation specific parameters, from for example Bouncy Castle, that we also capture.</p> <ul> <li><code>java.security.KeyPairGenerator</code></li> <li><code>java.security.KeyStore</code></li> <li><code>java.security.MessageDigest</code></li> <li><code>java.security.Signature</code></li> <li><code>javax.crypto.Cipher</code></li> <li><code>javax.crypto.KeyAgreement</code></li> <li><code>javax.crypto.KeyGenerator</code></li> <li><code>javax.crypto.Mac</code></li> <li><code>javax.crypto.SecretKeyFactory</code></li> </ul> <p>From the Java Generic Security Services (Java GSS-API) we cover the following class:</p> <ul> <li><code>javax.net.ssl.SSLServerSocket</code></li> </ul> <p>The AQG Java Tracer also covers methods from the following classes from vendor-specific APIs:</p> <ul> <li>com.amazonaws.services.kms.AWSKMSClient</li> <li>com.azure.security.keyvault.keys.cryptography.CryptographyClient</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/","title":"AQG Yanadump","text":"<p>The AQG <code>yanadump</code> tool complements network analysis by enabling live traffic monitoring, providing continuous visibility into network activity.  It analyzes encrypted traffic within monitored connections, assessing the use of supported protocols and providing actionable insights without accessing exchanged data or relying on pre-captured PCAP files.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/#requirements","title":"Requirements","text":"<ul> <li>x64 architecture</li> <li>Linux kernel version:<ul> <li><code>&gt;= 3.2</code> for live capture</li> <li><code>&gt;= 2.6.32</code> for PCAP analysis</li> </ul> </li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/#download","title":"Download","text":"<p>You can download the AQG Network Analyzer from the web UI.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/#when-to-use-yanadump","title":"When to use Yanadump","text":"<p>Analyzing networks at the scale of gigabit Ethernet links presents challenges due to the high data volume.  Since the data is too large to capture and save on disk, it needs to be analyzed as close to the network traffic as possible.</p> <p><code>Yanadump</code> is a high-efficiency network analyzer developed by SandboxAQ to capture and process cryptographic data on high-speed networks.  It operates as a standalone, portable Linux binary optimized for environments where high network speeds and large data volumes make storing full packet captures (PCAPs) impractical.</p> <p>Instead of saving complete PCAP files, <code>yanadump</code> processes traffic in real time and generates compact Protobuf files containing only the relevant cryptographic details, as shown in the following diagram:</p> <p></p> <p>Key <code>yanadump</code> features include:</p> <ul> <li>Efficient Data Reduction: Reduces a 40GB PCAP to a 150MB Protobuf file (0.4% of original size), further compressible to 35MB (0.08%).</li> <li>Targeted Information: Extracts only handshake-related information, supporting efficient cryptographic analysis.</li> <li>Protocol Support: Supports Ethernet, 802.1Q (VLAN), GRE, IPv4, IPv6, and VXLAN, making it adaptable to various network configurations, including cloud environments.</li> <li>Scalability: Achieves traffic parsing speeds of approximately 1Gbps per CPU GHz, suitable for gigabit-scale networks.</li> <li>Resource Management: Controls maximum memory usage, preventing excessive resource utilization.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/","title":"GCP packet mirroring tutorial","text":"<p>Network Packet Mirroring captures real-time activity without impacting network performance. This tutorial provides step-by-step instructions for implementing packet mirroring in Google Cloud Platform (GCP) to capture live network traffic data for AQtive Guard analysis. </p> <p>This tutorial is based on the official Google Cloud documentation for Packet mirroring. It provides steps for mirroring traffic within a single Virtual Private Cloud (VPC) and zone using the GCP Console. </p> <p>Important</p> <p>This tutorial is provided as a starting point. Refer to the official GCP documentation and enhance as necessary to safely implement network packet mirroring in your specific production environment. </p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#prerequisites","title":"Prerequisites","text":"<p>To enable GCP packet mirroring, you\u2019ll need the following:</p> <ul> <li>GCP project where resources are managed. You\u2019ll select this GCP project during the configuration process.</li> <li>Appropriate permissions within the selected project. A role such as <code>roles/editor</code> contains the necessary permissions to perform the required tasks. </li> </ul> <p>Tip</p> <p>Assume that any settings in the console not explicitly mentioned in these instructions are sane defaults that only require modification to meet the requirements of your specific environment.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#configuration","title":"Configuration","text":"<p>To enable GCP packet mirroring, you\u2019ll perform the following four main steps:</p> <ol> <li>Create Collector Instance(s) to receive mirrored traffic. </li> <li>Create an Internal Passthrough Network Load Balancer and configure it to route mirrored traffic from the source to the collector instances.</li> <li>Configure Firewall Rules to secure the mirrored traffic. </li> <li>Create a Packet Mirroring Policy to specify which traffic will be mirrored and routed to the collector instance(s). </li> </ol> <p>The following sections provide detailed instructions for each of these steps. </p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-collector-instances","title":"Create Collector Instance(s)","text":"<p>To create a collector instance, you\u2019ll first need to create an instance template and then set up a Managed Instance Group (MIG).</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-an-instance-template","title":"Create an Instance Template","text":"<ol> <li>In the Google Cloud console, navigate to Instance templates.</li> <li>Select Create instance template and enter a template name.</li> <li>In the Location section, select the location for the connection:<ul> <li>Global - allows you to use the instance template across multiple regions.</li> <li>Regional - select the location where you want to create the instance template.</li> </ul> </li> <li> <p>Under Machine Configuration, select a Machine type suitable for the amount of traffic to be mirrored.</p> <p>Tip</p> <p>For testing purposes, <code>n2-standard-2</code> is typically sufficient. However, if you experience dropped packets during capture, it\u2019s a signal that the machine type is under-provisioned. </p> </li> <li> <p>Under Boot disk, select Change.</p> <ul> <li>Select an appropriate OS image for your instance.</li> <li>Adjust the disk size to meet your requirements. </li> </ul> </li> <li>Expand the Advanced options and select Networking.<ul> <li>In Network tags, add a suitable tag such as <code>packet-collector</code> to target this instance later.</li> <li>Update the network interface or IP address settings if required.</li> </ul> </li> <li>Select Create to create the instance template.</li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-a-managed-instance-group","title":"Create a Managed Instance Group","text":"<ol> <li>In the Google Cloud console, navigate to Instance groups, then select Create instance group. </li> <li>Select New managed instance group (stateless) and enter an instance group name.</li> <li>In the Instance template dropdown, select the template you created previously.</li> <li>Under Location: <ul> <li>Select Single zone.</li> <li>From the Region and Zone dropdowns, select the location of the network resources where you want to mirror traffic.</li> </ul> </li> <li> <p>The Autoscaling section is enabled by default for stateless groups:</p> <ul> <li>Set Minimum number of instances to <code>1</code>.</li> <li>Set Maximum number of instances to <code>2</code> for high availability, or <code>1</code> for testing or low-throughput scenarios. </li> </ul> <p>This configuration will scale the group based on resource usage and instance status.</p> </li> <li> <p>Select Create to create the MIG.</p> </li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-an-internal-passthrough-network-load-balancer","title":"Create an Internal Passthrough Network Load Balancer","text":"<p>Follow these steps to create a Network Load Balancer (NLB) to route mirrored traffic from the source to the collector instances.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#nlb-considerations","title":"NLB considerations","text":"<p>Review these important considerations before configuring the NLB for Packet Mirroring:</p> <ul> <li> <p>Enable Packet Mirroring. Packet Mirroring must be enabled when creating the NLB. </p> <p>Caution</p> <p>This setting cannot be changed after the NLB is created.</p> </li> </ul> <ul> <li>Region Alignment. The NLB must be in the same region as the resources whose traffic will be mirrored.</li> <li>Backend Configuration. <ul> <li>The NLB backend service must use a session affinity of <code>NONE</code>.</li> <li>Backend subsetting must be disabled.</li> </ul> </li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-an-nlb","title":"Create an NLB","text":"<ol> <li>In the Google Cloud console, navigate to Load balancing.</li> <li>Select Create load balancer.</li> <li>Configure the following, selecting Next after each: <ul> <li>Type of load balancer - Network Load Balancer (TCP/UDP/SSL)</li> <li>Proxy or passthrough - Passthrough load balancer</li> <li>Public facing or internal - Internal</li> </ul> </li> <li>Under Create load balancer, select Configure to proceed with the setup.</li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#complete-nlb-configuration","title":"Complete NLB configuration","text":"<ol> <li>Enter a Load Balancer name.</li> <li>Select the Region that matches the region of the resources where you want to mirror traffic.</li> <li>Select the Network for the resources where you want to mirror traffic.</li> <li>Select Backend configuration. </li> <li>Under Backend service configure the following:<ul> <li>Backend type - Instance group </li> <li>Protocol - TCP</li> </ul> </li> <li>In the New backend section, select the Instance group you created previously to forward packets to.</li> <li>From the Health check list, select Create a health check, then configure and save the following:<ul> <li>Name - enter a name for the health check</li> <li>Protocol - TCP</li> <li>Port - 22</li> </ul> </li> <li>Click Frontend configuration.</li> <li> <p>Under New Frontend IP and port, configure the following:</p> <ul> <li>Name - enter a name for the frontend.</li> <li>Subnetwork - select a subnetwork in the same region as the instances to mirror.</li> <li>Ports - All</li> <li>Advanced configurations - expand and select the checkbox for Enable this load balancer for Packet Mirroring </li> </ul> <p>When complete, select Done. </p> </li> <li> <p>Select Create.</p> </li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#configure-firewall-rules","title":"Configure Firewall Rules","text":"<p>Follow these steps to configure firewall rules to secure the VPC for Packet Mirroring traffic.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#firewall-rule-considerations","title":"Firewall rule considerations","text":"<p>Review these important considerations before creating firewall rules: </p> <ul> <li>Egress Rules - Make sure that no firewall rules override the implied egress rule. This allows mirrored traffic to flow from the source resources to the destination instances behind the NLB.</li> <li> <p>Allow Traffic to Collector Instances - Ensure the packet collector instances in the MIG behind the NLB can retrieve traffic from the IP ranges of the mirrored resources.</p> <ul> <li>To allow collector instances to receive IPv4 traffic from any resource, create a firewall rule with a source IPv4 address range of <code>0.0.0.0/0</code>.</li> <li>To allow collector instances to receive IPv6 traffic from any resource, create a firewall rule with a source IPv6 address range of <code>::/0</code>.</li> </ul> <p>Important</p> <p>To prevent internet traffic from reaching the collector instances, make sure they have only internal IPv4 and IPv6 addresses.</p> </li> </ul> <ul> <li>Cloud NAT Traffic - For internet traffic passing through Cloud NAT to be mirrored properly to the collector instance(s), make sure a rule exists that:<ul> <li>allows all traffic </li> <li>selects collector instances by network tag</li> </ul> </li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#firewall-rules-for-packet-mirroring","title":"Firewall rules for packet mirroring","text":"<p>You\u2019ll need to create the following firewall rules to enable packet mirroring:</p> <ul> <li>Allow Traffic to the Packet Collector</li> <li>Allow Health Checks to the Packet Collector</li> <li>Allow Traffic to the Packet Collector via Network Tag</li> </ul> <p>The following sections provide detailed configuration instructions for each rule. </p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-a-rule-to-allow-traffic-to-the-packet-collector","title":"Create a Rule to Allow Traffic to the Packet Collector","text":"<ol> <li>In the Google Cloud console, navigate to Firewall policies and select Create firewall rule.</li> <li>Enter a Name for the rule.</li> <li>From the Network dropdown, select the network you\u2019re mirroring traffic to and configure the following:<ul> <li>Direction of traffic - Ingress</li> <li>Action on match - Allow</li> <li>Targets - All instances in the network</li> <li>Source filter - IPv4 ranges</li> <li>Source IPv4 ranges - Enter <code>0.0.0.0/0</code> to allow all traffic on the network to be mirrored</li> <li>Destination filter - Select IPv4 ranges and enter the IP address ranges for the:<ul> <li>Internal passthrough NLB</li> <li>Packet collector instances in Destination IPv4 ranges</li> </ul> </li> <li>Protocols and ports - Allow all</li> </ul> </li> <li>Select Create to create the rule. </li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-a-rule-to-allow-health-checks-to-the-packet-collector","title":"Create a Rule to Allow Health Checks to the Packet Collector","text":"<ol> <li>In the Google Cloud console, navigate to Firewall policies and select Create firewall rule.</li> <li>Enter a Name for the rule.</li> <li>From the Network dropdown, select the network you\u2019re mirroring traffic to and configure the following:<ul> <li>Direction of traffic - Ingress</li> <li>Action on match - Allow</li> <li>Targets - All instances in the network</li> <li>Source filter - IPv4 ranges</li> <li>Source IPv4 ranges - <code>130.211.0.0/22</code> and <code>35.191.0.0/16</code> for the Google Cloud health check address ranges</li> <li>Destination filter - select IPv4 ranges and enter the IP address ranges for the:      - Internal passthrough NLB     - Packet collector instances in Destination IPv4 ranges</li> <li>Protocols and ports - Allow all</li> </ul> </li> <li>Select Create to create the rule. </li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-a-rule-to-allow-traffic-to-the-packet-collector-via-network-tag","title":"Create a Rule to Allow Traffic to the Packet Collector via Network Tag","text":"<ol> <li>In the Google Cloud console, navigate to Firewall policies and select Create firewall rule.</li> <li>Enter a Name for the rule.<ul> <li>Direction of traffic - Ingress</li> <li>Action on match - Allow</li> <li>Targets - Specified target tags</li> <li>Target tags - enter the network tag specified during instance template creation</li> <li>Source filter - select a filter that matches your network ranges. In most cases this will be IPv4 ranges<ul> <li>Enter an appropriate filter. If IPv4 ranges was selected, enter <code>0.0.0.0/0</code> to allow all IPv4 traffic</li> </ul> </li> <li>Destination filter - IPv4 ranges</li> <li>Destination IPv4 ranges - <code>0.0.0.0/0</code></li> <li>Protocols and ports - Allow all</li> </ul> </li> <li>Select Create to create the rule. </li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/gcp-packet-mirroring-tutorial/#create-a-packet-mirroring-policy","title":"Create a Packet Mirroring Policy","text":"<ol> <li>In the Google Cloud console, navigate to Packet Mirroring and select Create policy.</li> <li> <p>Configure the following settings, then select Continue:</p> <ul> <li>Name: Enter a name for the packet mirroring policy.</li> <li> <p>Region: Select the region that includes the mirrored sources and collector destination. </p> <p>Note</p> <p>The policy must be in the same region as the source and destination.</p> </li> </ul> <ul> <li>Enabled: Select this to activate the policy upon creation.</li> <li>VPC Networks: Select the VPC networks where the mirrored source and collector destination are located, then select Continue.</li> <li>Mirrored sources and destination: <ul> <li>Select Mirrored sources and destination are in the same VPC network</li> <li>Select the network. </li> </ul> </li> </ul> </li> <li> <p>Select Mirrored sources:</p> <ul> <li>You can select one or more sources. Google Cloud mirrors any instance that matches at least one of your selected sources.</li> <li>Subnets: Select one or more subnets. Google Cloud mirrors existing and future instances in the selected subnets.</li> <li>Select Continue.</li> </ul> </li> <li>Select the internal passthrough Network Load Balancer (NLB) configured for Packet Mirroring, then select Continue.</li> <li>Configure the traffic to mirror:<ul> <li>Mirror all IPv4 traffic - select this option to mirror all IPv4 traffic.</li> <li>Mirror filtered traffic - select this option to mirror both IPv4 and IPv6 traffic, and configure the following:<ul> <li>Allow all protocols</li> <li>Allow all IPv4 ranges (0.0.0.0/0)</li> <li>Allow all IPv6 ranges (::/0)</li> <li>Allow both ingress and egress traffic</li> </ul> </li> </ul> </li> <li>Configure the Traffic direction of the traffic that you want to mirror.</li> <li>Select Submit to create the packet mirroring policy.</li> </ol>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/","title":"Getting started capturing live network traffic","text":"<p>The <code>yanadump</code> tool provides an alternative to PCAP analysis. <code>yanadump</code> listens on a network interface and captures handshake information in a trace file (.bin) formatted for analysis in AQtive Guard. </p> <p><code>yanadump</code> functions as a network probe, enabling live network traffic monitoring through two deployment methods. It must be deployed on a machine that either sends and receives traffic or receives forwarded traffic for analysis:</p> <ul> <li>Direct traffic monitoring: Listen to traffic from the network interface of an endpoint, such as a Linux server.</li> <li>Traffic mirroring: Use cloud-native traffic mirroring to monitor forwarded traffic from cloud-based assets or in a hybrid environment.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#direct-traffic-monitoring-linux","title":"Direct traffic monitoring (Linux)","text":"<p>To dump handshake information from a live interface, run:</p> <pre><code>$ yanadump -i interface0 --output-format protobuf -o /path/to/out.bin\n</code></pre> <p>The <code>yanadump</code> binary requires both the <code>CAP_NET_ADMIN</code> and <code>CAP_NET_RAW</code> Linux capabilities to capture packets. This can be achieved by doing one of the following:</p> <ul> <li>Run the <code>yanadump</code> tool as root (not recommended)</li> <li>Add the following capabilities to the <code>yanadump</code> binary:</li> </ul> <pre><code>$ sudo setcap 'cap_net_raw+eip cap_net_admin+eip' /path/to/yanadump\n</code></pre> <p>Info</p> <p>The specified interface is put in promiscuous mode by default. However, some network interfaces don\u2019t support promiscuous mode and an error such as <code>Failed to set promiscuous mode</code> can be encountered. The promiscuous mode can be disabled with the <code>--disable-promiscuous</code> argument.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#stopping-a-live-capture","title":"Stopping a live capture","text":"<p>To run the <code>yanadump</code> tool for a predetermined amount of time, use the unix <code>timeout</code> command:</p> <pre><code>timeout 1h yanadump -i interface0 --output-format protobuf -o mytrace.bin\n</code></pre> <p>In this example, the <code>yanadump</code> command will be terminated after running for one hour.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#continuous-monitoring","title":"Continuous monitoring","text":"<p>Deploy <code>yanadump</code> to regularly push <code>protobuf</code> files to AQtive Guard on a recurring schedule (such as daily) for ongoing monitoring of your network cryptography. </p> <p>To continuously analyze machine traffic, first ensure the machine uploading the trace has access to the AQtive Guard API.</p> <p>The following command line streams handshake information to AQtive Guard continuously and create a new AQtive Guard session every day:</p> Bash<pre><code>$ export AQG_API_TOKEN='xxxxx'\n$ ./yanadump -i interface0 --api-session-renew 1d --api-url https://API.AQG.DOMAIN/\n</code></pre> <p>In this example:</p> <ul> <li><code>xxxxx</code> is the API token generated from the settings page of AQtive Guard web UI</li> <li><code>https://API.AQG.DOMAIN/</code> is the base URL of the AQtive Guard instance.</li> </ul> <p>The API token can also be passed through the <code>--api-token</code> argument, however it should be considered insecure as the token ends up in <code>ps</code> output.</p> <p>Info</p> <p>In the case of high data volume, handshake information may be dropped if the upload to the AQtive Guard API takes too long. The channel capacity can be increased with the <code>--channel-capacity</code> argument,  but this will increase memory usage.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#how-to-use-a-custom-ca","title":"How to use a custom CA","text":"<p><code>yanadump</code> verifies the TLS certificate of the server specified with <code>--api-url</code>. This verification can fail if the API server makes use of a custom CA certificate. In that case, the location of a custom certificate can be specified through the <code>--ca-file</code> argument, as in the following example:</p> Bash<pre><code>$ ./yanadump -i interface0 --api-url https://API.AQG.DOMAIN/ --ca-file ./public.pem\n</code></pre> <p>The file may contain multiple CA certificates and all certificate(s) must be in PEM format. If this argument is not set, the platform-specific certificate source is used:</p> <ul> <li>On Windows, certificates are loaded from the system certificate store.</li> <li>On macOS, certificates are loaded from the keychain. The user, admin and system trust settings are merged together as documented by Apple.</li> <li>On Linux and other UNIX-like operating systems, CA files are searched within the default directories unless otherwise specified through the environment variables <code>SSL_CERT_FILE</code> and <code>SSL_CERT_DIR</code>.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#session-renewal","title":"Session renewal","text":"<p>By default, <code>yanadump</code> uploads handshake information within a single session for a given live capture.</p> <p>The <code>--api-session-renew</code> argument can be used to create a new AQG session at a regular interval. The format is <code>&lt;number&gt;&lt;unit&gt;</code>, where <code>&lt;unit&gt;</code> can be <code>d</code> (days), <code>h</code> (hours), <code>m</code> (months) or <code>y</code> (years). For example, to renew the session every 2 days:</p> Bash<pre><code>$ ./yanadump -i interface0 --api-url https://API.AQG.DOMAIN/ --api-session-renew 2d\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#network-interface-deletion","title":"Network interface deletion","text":"<p>Some networking services (eg. VPNs) delete a network interface when they are stopped and create a new network interface when they are launched. <code>yanadump</code> transparently handles this situation and resume trafic capture on a network interface that has been deleted and created again within a given amount of time. It allows such network services to be restarted without having to restart <code>yanadump</code>.</p> <p>By default, this duration is set to 2 minutes and can be modified with the <code>--if-check-duration</code> argument. For instance, to wait for the interface <code>interface0</code> to be created within 5 seconds after having been deleted:</p> Bash<pre><code>$ ./yanadump -i interface0 --if-check-duration 5\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#inactive-stream-collection","title":"Inactive stream collection","text":"<p>In live capture mode, the memory consumption might grow constantly in some situations, for instance if TCP connections aren\u2019t closed properly. To prevent these situations, an inactive stream collector runs every 2 minutes by default and drop streams inactive (that is with no packet exchanged) for longer that 1 minute. These values can be changed with <code>--stream-collector-timeout</code> and <code>--stream-collector-interval</code>.</p> <p>To disable the inactive stream collector:</p> Bash<pre><code>$ ./yanadump -i interface0 --stream-collector-interval 0\n</code></pre> <p>To run the inactive stream collector every 4 minutes and drop streams inactive for longer that 30 seconds:</p> Bash<pre><code>$ ./yanadump -i interface0 --stream-collector-interval 240 --stream-collector-timeout 30\n</code></pre>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#yanadump-traffic-mirroring-strategy","title":"Yanadump traffic mirroring strategy","text":"<p>Using cloud-native traffic mirroring, <code>yanadump</code> extends monitoring to cloud assets, enabling seamless coverage in hybrid environments. </p> <p>By analyzing traffic from virtual instances, containers, and other cloud resources, it centralizes monitoring for both on-premises and cloud networks, delivering consistent security insights and comprehensive visibility across the entire enterprise infrastructure.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-live-network-traffic-monitoring/#yanadump-implementation-plan","title":"Yanadump implementation plan","text":"<p>We recommend strategically deploying <code>yanadump</code> at key aggregation points, such as core routers or switches, rather than on individual endpoints. This placement enables it to efficiently monitor traffic from multiple sources, providing broad visibility with minimal deployment effort.</p> <ol> <li>Assess traffic mirroring and forwarding options. Identify current traffic mirroring, forwarding, and monitoring configurations (such as SPAN ports, TAPs, AWS VPC Traffic Mirroring, or Azure Network Watcher) across your on-premises and cloud environments. Aligning with current configurations enables centralized data collection and streamlines analysis across the network.  </li> <li>Identify key network aggregation points. Locate core routers, switches, or other network convergence points where traffic from multiple endpoints converges. These will be prioritized for <code>yanadump</code> deployment, providing maximum visibility from minimal infrastructure.  </li> <li>Select critical cloud assets. Identify essential cloud resources, such as virtual machines and containers, for monitoring. Configure traffic mirroring to forward traffic from these cloud assets to <code>yanadump</code>, ensuring a unified view across hybrid environments.  </li> <li>Deploy yanadump at aggregation points. Install <code>yanadump</code> at selected on-premises convergence points to capture and analyze aggregated network traffic efficiently. This placement enables <code>yanadump</code> to monitor multiple sources with minimal deployment footprint.  </li> <li>Enable cloud traffic mirroring. For comprehensive hybrid network coverage, configure traffic mirroring for the selected cloud assets to direct relevant traffic to <code>yanadump</code> for analysis. This setup allows AQtive Guard to receive both on-premises and cloud data in one central location.  </li> <li>Automatic analysis. Set <code>yanadump</code> to upload its data to AQtive Guard. This streaming design allows for time-based cryptographic insights without large data storage requirements, keeping analysis efficient.</li> </ol> <p>Refer to the GCP packet mirroring tutorial for an example of configuring traffic mirroring on Google Cloud Platform. </p> <p>Important</p> <p>To ensure proper analysis in AQtive Guard, PCAP network captures must include TCP handshake packets (SYN/SYNACK) and, if applicable, RST packets. If your network device pre-filters traffic or limits captured data (for example, truncating after a certain number of bytes), verify that these critical packets are retained. Omitting them prevents tools like <code>yanadump</code> from identifying and analyzing TCP streams, resulting in incomplete or unusable analysis.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-pcap-upload/","title":"Getting started with PCAP upload","text":"<p>The AQG Network Analyzer can process packet capture (PCAP) files to detect cryptographic objects within network captures.</p> <p>Tip</p> <p>The AQG Network Analyzer also includes the <code>yanadump</code> tool that can be deployed as a standalone and portable Linux binary to prepare live network traffic for analysis. Refer to Getting started with live network traffic monitoring for details. </p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-pcap-upload/#create-a-pcap-file","title":"Create a PCAP file","text":"<p>You can use your preferred network sniffer to produce a PCAP file. For instance, to create a PCAP file using tcpdump, run:</p> <pre><code>tcpdump -w file.cap\n</code></pre> <p>This tutorial provides more information on using tcpdump.</p> <p>The AQG Network Analyzer can also process packet capture data through integrations with popular network security and monitoring platforms.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/getting-started-with-pcap-upload/#upload-a-pcap-file","title":"Upload a PCAP file","text":"<p>To upload a PCAP file to AQtive Guard:</p> <ol> <li>Select Settings from the main menu on the left.</li> <li>In the Create Uploader Token tile, select Create.</li> <li>Copy the provided token to a secure place.</li> <li>Use the code below to push a PCAP file to AQtive Guard.</li> </ol> <pre><code>export AQG_API_TOKEN='xxxxx'\n\n# Parse and upload a trace to AQG\n./yanadump --file capture.pcap --api-url https://API.AQG.DOMAIN/\n\n# Optionally parameterize upload\n# --slot-id \u201cslt_&lt;32-hex-nibbles&gt;\u201d\n</code></pre> <p>In the above query:</p> <ul> <li><code>xxxxx</code> is the is the token provided earlier in step 3.</li> <li><code>https://API.AQG.DOMAIN/</code> is the base URL of the AQtive Guard instance, eg. <code>https://your-domain.aqtiveguard.sandboxaq.com</code>.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/","title":"AQG Network Analyzer reference","text":""},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#network-analyzer-data","title":"Network analyzer data","text":"<p>The Handshakes and Ciphersuites tables below provide details about the negotiation of cryptographic parameters during data transmission.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#handshake-data","title":"Handshake data","text":"<p>The AQG Network Analyzer can identify both complete and incomplete handshakes and extract the following data:</p> Data Complete Handshakes Incomplete Handshakes Source IP \u2705 \u2705 Target IP \u2705 \u2705 Source Port \u2705 \u2705 Target Port \u2705 \u2705 Selected Ciphersuite \u2705 \u2014 Client-supported Ciphersuites \u2705 \u2705 Selected EC Group \u2705 \u2014 Client supported Groups \u2705 \u2705 Certificate and Key information \u2705 \u2014 Client timestamp \u2705 \u2705 Server timestamp \u2705 \u2014 Server name \u2705 \u2705 <p>Handshake data can be accessed through the AQtive Guard UI.</p> <p>You can also use the <code>yanadump</code> tool to dump handshake information from a PCAP file. Run:</p> <pre><code>$ yanadump -f /path/to/file.pcap --output-format protobuf -o /path/to/out.bin\n</code></pre> <p>This generates a trace file that includes all TLS handshake information in Protobuf format. This compact format saves considerable time when uploading to AQtive Guard for analysis. </p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#ciphersuite-data","title":"Ciphersuite data","text":"<p>The AQG Network Analyzer provides an in-depth analysis of TLS ciphersuites and extracts the following:</p> <ul> <li>TLS version  </li> <li>Key exchange algorithm  </li> <li>Signature algorithm  </li> <li>Symmetric cipher algorithm  </li> <li>MAC algorithm  </li> <li>Hash algorithm</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#pcap-formats-and-packet-types","title":"PCAP formats and packet types","text":"<p>The AQG Network Analyzer supports several PCAP formats, PCAP link layers, and protocols. Refer to Getting started with PCAP upload for details on using PCAPs for analysis.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#pcap-formats","title":"PCAP formats","text":"<p>The AQG Network Analyzer supports any format that the pcap-parser supports. These formats are:</p> <ul> <li>The original PCAP</li> <li>PcapNG</li> <li>\u201cModified\u201d PCAP</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#pcap-link-layers","title":"PCAP link layers","text":"<p>The AQG Network Analyzer supports the following PCAP link layers:</p> <ul> <li><code>LINKTYPE_NULL</code> - Null (assuming the capturing host was little-endian)</li> <li><code>LINKTYPE_LOOP</code> - Loop (assuming the capturing host was little-endian)</li> <li><code>LINKTYPE_ETHERNET</code> - Ethernet</li> <li><code>LINKTYPE_IPV4</code> - IPv4</li> <li><code>LINKTYPE_IPV6</code> - IPv6</li> <li><code>LINKTYPE_RAW</code> - Raw</li> <li><code>LINKTYPE_LINUX_SLL</code> - Linux cooked capture encapsulation</li> <li><code>LINKTYPE_LINUX_SLL2</code> - Linux cooked capture encapsulation v2</li> </ul> <p>Refer to the <code>LINKTYPE</code> definitions for details.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#supported-network-protocols","title":"Supported network protocols","text":"<p>The AQG Network Analyzer supports the following protocols for both PCAP analysis and <code>yanadump</code> live streaming.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#supported-layer-4-l4-packet-types","title":"Supported &lt;= Layer 4 (L4) packet types","text":"<p>The AQG Network Analyzer supports the following packet types for L4 or lower:</p> <ul> <li>Ethernet</li> <li>Dot1q</li> <li>IPV4 / IPv6</li> <li>Generic Routing Encapsulation (GRE)</li> <li>VXLAN</li> <li>TCP / UDP</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#supported-layer-7-l7-handshake-extraction-protocols","title":"Supported Layer 7 (L7) handshake extraction protocols","text":"<p>The AQG Network Analyzer supports the following handshake extraction protocols for L7:</p> <ul> <li><code>TLS 1.3</code> - Extracts client-supported ciphersuites, elliptic curves, and signature     algorithms (classic, hybrid, or PQC), along with the server\u2019s selected ciphersuites.</li> <li><code>SSL 3.0</code>, <code>TLS 1.0</code>, <code>TLS 1.1</code>, and <code>TLS 1.2</code> - Extracts classic cryptographic objects as in <code>TLS 1.3</code>, as well as any available X.509 certificates.</li> </ul>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/reference/#yanadump-live-streaming-formats-and-protocols","title":"Yanadump live streaming formats and protocols","text":"<p><code>yanadump</code> directly captures packets from Linux network interfaces for analyzing live network traffic, and also supports VXLAN, which AWS uses for its port mirroring capability. It can parse generic traffic at a speed of ~1Gbps/CPU GHz.</p> <p>Refer to Getting started with live network traffic monitoring for details on using <code>yanadump</code>.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/troubleshooting/","title":"Troubleshooting","text":""},{"location":"aqtive-guard-sensors/aqg-network-analyzer/troubleshooting/#logging","title":"Logging","text":"<p>The <code>AQG_LOG</code> environment variable allows you to configure logging settings. By default, the logging level is set to <code>info</code>.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/troubleshooting/#set-the-logging-level","title":"Set the logging level","text":"<p>Use the following command to log only error messages:</p> Bash<pre><code>$ AQG_LOG=error yanadump\n</code></pre> <p>This command sets the log level for <code>yanadump</code> to <code>debug</code> while other modules retain the default <code>info</code>:</p> Bash<pre><code>$ AQG_LOG=info,yanadump=debug yanadump\n</code></pre> <p>For more detailed information about using the environment variable, refer to the env_logger documentation.</p>"},{"location":"aqtive-guard-sensors/aqg-network-analyzer/troubleshooting/#problem-self-signed-certificate-verification-failure","title":"Problem: Self-signed certificate verification failure","text":"<p>Self-signed certificates must have a valid Subject Alternative Name and a Basic Constraint extension with the CA value set to <code>FALSE</code> in order to be accepted. </p> <p>The following OpenSSL command line example generates a self-signed certificate for localhost:</p> Bash<pre><code>$ openssl req -x509 -newkey rsa:4096 -keyout key.pem -out cert.pem -days 3650 -nodes -subj '/CN=localhost' -addext 'basicConstraints=CA:FALSE' -addext 'subjectAltName=DNS:localhost'\n</code></pre>"},{"location":"dashboard/","title":"Dashboard","text":"<p>The AQtive Guard Dashboard is the initial view when you log in. It provides a comprehensive overview of your cryptographic environment, enabling you to monitor the status of scanned IT assets and cryptographic health. It also highlights potential security and compliance issues so you can take corrective action.</p> <p>Tip</p> <p>To return to the dashboard from other views, select Dashboard from the main menu.</p>"},{"location":"dashboard/#dashboard-tour","title":"Dashboard tour","text":"<p>The AQtive Guard Dashboard provides a centralized view of your cryptographic inventory and health, divided into four key sections.</p> <p></p> <ol> <li> <p>IT Assets - Monitor the health of IT assets tracked over the past 3 months. </p> <ul> <li>Select IT Assets in the main menu to access detailed information on scanned hosts, applications, and container images. </li> </ul> </li> <li> <p>Crypto inventory - Visualize your cryptographic landscape, including discovered keys and certificates.</p> <ul> <li>Select Inventory from the main menu to explore details. </li> </ul> </li> <li> <p>Scan status in the last 7 days  - View a quick summary of scanned infrastructure over the past week, including:</p> <ul> <li>Hosts</li> <li>Applications</li> <li>Container images</li> </ul> </li> <li> <p>Highest impact issues - Identify out-of-policy rules and prioritize issues based on their severity (critical, high, medium, or low). </p> <ul> <li>Select View to explore the associated issue details, or select Issues from the main menu to view all discovered issues. </li> </ul> </li> </ol>"},{"location":"data-sources/","title":"Data sources","text":"<p>AQtive Guard supports analysis of data from 3rd party tools and standardized file formats to leverage your existing systems and tools. Integrated external data sources include:</p> <ul> <li>Qualys: Import Qualys certificate and server scan data and analyze potential cryptographic vulnerabilities.</li> <li>CrowdStrike: Import scans from CrowdStrike Falcon and generate a cryptographic analysis of available data.</li> <li>ServiceNow: Ingest certificate data from ServiceNow for centralized certificate management and enhanced security posture.</li> <li>AWS Key Management Service (KMS): Ingest data from AWS KMS for enhanced key management and security monitoring.</li> <li>Palo Alto Networks: Ingest and analyze TLS handshake data from Next-Generation Firewall log files.</li> <li>CBOM: Upload and analyze your Cryptography Bill of Materials.</li> <li>SentinelOne: Ingest SentinelOne data for installed applications and IT inventory details.</li> </ul> <p>For detailed information for each data source, refer to the corresponding section in the user guide menu.</p>"},{"location":"data-sources/aws-kms/","title":"AWS Key Management Service","text":"<p>AQtive Guard can ingest AWS Key Management Service (KMS) data for enhanced key management and security monitoring.</p>"},{"location":"data-sources/aws-kms/#aws-kms-requirements","title":"AWS KMS requirements","text":"<p>You\u2019ll need the following from AWS before setting up the integration:</p> <ul> <li>AWS Credentials:  <ul> <li>Access Key ID</li> <li>Secret Access Key</li> </ul> </li> <li>The following <code>Read</code> permissions for the AWS KMS credentials:  <ul> <li><code>ListKeys</code></li> <li><code>DescribeKey</code></li> <li><code>GetPublicKey</code></li> </ul> </li> </ul> <p>There are two ways to obtain the requirements:</p> <ol> <li>Create an IAM user in your AWS account, generate credentials, then complete one of the following:<ul> <li>Attach a policy or individual permissions to the user.</li> <li>Assign the user to a group and attach the policy to the group.</li> </ul> </li> <li>Use a federated user (for example, managed by Microsoft or Google) and assume a role to obtain temporary credentials.</li> </ol> <p>Tip</p> <p>Since temporary credentials need to be re-entered regularly, we recommend avoiding this method if your organization allows it.</p> <p>You\u2019ll also need to generate an AWS access key (both methods).</p> <p>The following sections provide step-by-step instructions for configuring the integration by creating an IAM user.</p>"},{"location":"data-sources/aws-kms/#configure-the-aws-kms-integration","title":"Configure the AWS KMS integration","text":"<p>Perform the following steps to configure the AWS KMS integration in AWS and AQtive Guard.</p>"},{"location":"data-sources/aws-kms/#aws-configuration","title":"AWS configuration","text":"<p>Complete the following steps in AWS to create a policy, create an IAM user, and generate credentials. </p>"},{"location":"data-sources/aws-kms/#create-a-policy","title":"Create a policy","text":"<p>To create a policy that will be attached to the IAM user:</p> <ol> <li>Log into the AWS console.</li> <li>From the AWS dashboard, search for IAM and select Identity and Access Management (IAM).</li> <li>Under the Access management menu on the left, select Policies.</li> <li> <p>Select Create policy.</p> <p>Depending on your organization\u2019s policies, you may alternately use a predefined power user role that grants all <code>read</code> and <code>write</code> KMS-related permissions.</p> <p>To do this, enter AWSKeyManagementServicePower in the search bar and select the role, then skip to Create a user.</p> </li> <li> <p>In the Service dropdown, enter KMS.</p> </li> <li>Select the following permissions required for the integration:<ul> <li><code>ListKeys</code></li> <li><code>DescribeKey</code></li> <li><code>GetPublicKey</code></li> </ul> </li> <li>Select Next.</li> <li> <p>Enter a Policy name, an optional Policy description and then select Create policy. </p> <p>You\u2019ll see a notification confirming the policy was created. Select it to confirm the permissions are configured correctly.</p> </li> </ol>"},{"location":"data-sources/aws-kms/#create-a-user","title":"Create a user","text":"<p>To create an IAM user:</p> <ol> <li>Log into the AWS console.</li> <li>Under the Access management menu on the left, select Users.</li> <li>Select Create user.</li> <li>Enter a User name and select Next.</li> <li>Select the Attach policies directly tile.</li> <li>Search for the policy you created earlier and select Next.</li> <li> <p>Select Create user.</p> <p>You\u2019ll see a notification confirming the user was created successfully.</p> </li> <li> <p>Select the user to open the user details page and select Create access key.</p> </li> <li>Select Third-party service, then Next.</li> <li>Enter a description, then select Create access key.</li> </ol> <p>Tip</p> <p>Make sure to copy the credentials and save them in a secure place. Once you close the Retrieve access keys screen, they can\u2019t be shown again. If you do lose access to the credentials, you can create a new access key.</p>"},{"location":"data-sources/aws-kms/#aqtive-guard-configuration","title":"AQtive Guard configuration","text":"<p>Complete the following steps to configure the AWS KMS integration in AQtive Guard.</p> <ol> <li>Log in to AQtive Guard and select Data sources from the main menu.</li> <li>Select Configure in the AWS KMS panel.</li> <li>Paste the required credentials you copied from AWS into the designated fields:<ul> <li>Your Access key.</li> <li>Your Secret access key.</li> </ul> </li> <li>Select your Region from the dropdown.</li> <li>If you\u2019re using temporary credentials, enter your session token.</li> <li>(Optional) Select Test Connection to check the connection to the AWS KMS API.</li> <li> <p>Select Submit to update the settings.</p> <p>You\u2019ll see a notification confirming that the configuration has succeeded.</p> </li> </ol>"},{"location":"data-sources/aws-kms/#use","title":"Use","text":"<p>Once the integration is configured, you can trigger an AWS KMS data ingestion.</p> <p>Note</p> <p>If the AWS KMS settings aren\u2019t configured, the ingestion option will be disabled.</p> <p>To ingest key data from AWS KMS:</p> <ol> <li>Select Data sources from the main menu, then select Details in the AWS KMS panel.</li> <li> <p>Select: Start keys ingestion to trigger ingestion of data into AQtive Guard.</p> <p>You\u2019ll see a notification confirming that the data ingestion has started.</p> </li> </ol>"},{"location":"data-sources/aws-kms/#unlink-the-aws-kms-integration","title":"Unlink the AWS KMS integration","text":"<p>Unlink the AWS KMS integration only if your organization needs to reconfigure or stop data sharing with AWS.</p> <p>To unlink the AWS KMS configuration:</p> <ol> <li>Select Data sources from the main menu, then select Details in the AWS KMS panel.  </li> <li>Select Unlink.</li> <li>Select Confirm and unlink AWS KMS.</li> </ol>"},{"location":"data-sources/cbom/","title":"CBOM","text":"<p>AQtive Guard supports uploading Cryptographic Bill of Materials (CBOM) files in JSON format for analysis and inventory.  CBOM provides a comprehensive inventory of an application\u2019s cryptographic objects and dependencies to expand your cryptographic and IT asset inventory in AQtive Guard.  </p>"},{"location":"data-sources/cbom/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid Cryptographic Bill of Materials (CBOM) JSON file. AQtive Guard supports the following formats:<ul> <li>CycloneDX version 1.4 - AQtive Guard validated this version with outputs from CryptoBOM Forge by Santander.</li> <li>CycloneDX version 1.6 - AQtive Guard validated this version using outputs from Sonarqube by IBM.</li> </ul> </li> </ul> <p>Tip</p> <p>The OWASP CycloneDX Tool Center lists numerous tools capable of generating CycloneDX 1.4 or 1.6 files. While some of these tools may produce compatible outputs, they aren\u2019t formally tested or supported by AQtive Guard, and functionality cannot be guaranteed.</p>"},{"location":"data-sources/cbom/#upload-a-cbom-file","title":"Upload a CBOM file","text":"<p>To upload a CBOM file, follow these steps:</p> <ol> <li>Navigate to Data Sources from the main menu, then select Upload in the CBOM panel.</li> <li> <p>Enter the following metadata to provide attributes for the CBOM data in AQtive Guard:</p> <ul> <li>Application Name - Enter the name of the application or codebase that this CBOM data belongs to, typically referencing the root name of the codebase.</li> <li>Language - The programming language used for the application, as provided by the CBOM, or the tool used to scan the application.</li> </ul> <p>Note</p> <p>Make sure the metadata is entered correctly before you upload the JSON file. It\u2019s used to identify the CBOM data in the associated AQtive Guard tables.     </p> </li> <li> <p>To upload the CBOM JSON file, either: </p> <ul> <li>Click in the target area and select the file from your local system.</li> <li>Drag and drop the file into the target upload area.</li> </ul> <p>The data will begin uploading automatically. </p> </li> </ol>"},{"location":"data-sources/crowdstrike/","title":"CrowdStrike","text":"<p>Integration with CrowdStrike Falcon enables seamless scanning of remote hosts, analysis of filesystem data, and presentation of actionable insights within the AQtive Guard UI. This guide outlines the orchestration process, leveraging the CrowdStrike API and the AQtive Guard Network Analyzer.</p> <p></p> <ol> <li>When you select the targets to launch a scan in the AQtive Guard UI, the AQtive Guard API (1) connects to the CrowdStrike API (2).</li> <li>The CrowdStrike API runs the AQG Filesystem Scanner on the targeted remote hosts (3).</li> <li>The data is sent from CrowdStrike to AQtive Guard storage where it is analyzed by AQtive Guard (4).</li> <li>The analyzed data is presented in the AQtive Guard UI dashboard (5) and data tables.</li> </ol>"},{"location":"data-sources/crowdstrike/#crowdstrike-requirements","title":"CrowdStrike requirements","text":"<p>The minimum requirements for application or product support are:</p> <ul> <li>CrowdStrike API client with the following Falcon API scopes:<ul> <li>Hosts (read)</li> <li>Real time response (admin) (write)</li> <li>Real time response (read, write)</li> </ul> </li> </ul>"},{"location":"data-sources/crowdstrike/#falcon-module-dependencies","title":"Falcon module dependencies","text":"<p>Falcon customers require the following subscriptions to use the CrowdStrike integration:</p> <ul> <li>Falcon Prevent</li> <li>Falcon Insight XDR</li> </ul>"},{"location":"data-sources/crowdstrike/#supported-platforms","title":"Supported platforms","text":"<ul> <li>Windows</li> <li>Linux</li> </ul>"},{"location":"data-sources/crowdstrike/#configure-the-crowdstrike-integration","title":"Configure the CrowdStrike integration","text":"<p>To configure the CrowdStrike integration in AQtive Guard, you\u2019ll need your CrowdStrike Base URL, Client ID, and Client secret. To obtain these:</p> <ol> <li>Log into your Falcon Console as a Falcon Administrator.</li> <li>Select Support, then API Clients and Keys. You can also search for API keys and select API Clients and Keys from the search results.</li> <li>Select Create API Client and specify a name and description.</li> <li>In the Scopes section, you\u2019ll select the necessary scopes:<ul> <li>Select Write next to Real time response (admin) and Real time response.</li> <li>Select Read next to Real time response.</li> </ul> </li> <li>Select Create and copy the Base URL, Client ID, and Secret values.</li> </ol> <p>Tip</p> <p>Make sure to copy the credentials and save them in a secure place. Once you close the window, they can\u2019t be shown again.  If you do lose access to the credentials, you can reset an API client\u2019s Secret, edit the scopes that are associated with it, or revoke all access.</p> <p>Log in to AQtive Guard to complete the following steps.</p> <ol> <li>Select Data sources from the main menu, then select Configure in the CrowdStrike panel.  </li> <li>Paste the information you copied from CrowdStrike into the designated fields:<ul> <li>Your Base URL (into the Instance URL field).</li> <li>Your API Client ID.</li> <li>Your API Client secret.</li> </ul> </li> <li>(Optional) Select Test Connection to check the connection to the CrowdStrike API.</li> <li>Select Submit to update the settings.</li> </ol> <p>Note</p> <p>Selecting Submit performs the same check as the Test connection button, in addition to verifying the client ID and secret are valid.</p>"},{"location":"data-sources/crowdstrike/#use","title":"Use","text":"<p>Once the integration is configured, you can trigger a CrowdStrike scan.</p> <p>Note</p> <p>If the CrowdStrike settings aren\u2019t configured, the scan option will be disabled.</p>"},{"location":"data-sources/crowdstrike/#trigger-crowdstrike-scan","title":"Trigger CrowdStrike scan","text":"<p>Follow these instructions to run a scan and ingest CrowdStrike data for analysis by AQtive Guard.</p> <ol> <li>Select Data sources from the main menu, then select Details in the CrowdStrike panel.</li> <li>In the Scans section, select Start Scan and configure the following:<ul> <li>Scan name - a default date and time stamp is provided. You can change this to any unique name.</li> <li>Scan Settings Profile - the impact that the scan will have on your infrastructure. There are 3 options:<ul> <li>High impact - this will use a workload of 100%. There\u2019s also no limit to the number of files that can be scanned per second, and the max file size is 1 GB.</li> <li>Standard impact (default) - this will use a workload of 50%. The number of files that can be scanned per second is 10,000 and the max file size is 1 MB.</li> <li>Low impact - this will use a workload of 5%. The number of files that can be scanned per second is 1,000 and the max file size is 512 KB.</li> </ul> </li> <li>Platform - select Linux or Windows.</li> <li>Hostname (optional) - only scan hosts that contain the entered text in their name.</li> <li>Host Activity (optional) - only scan hosts that were pinged within the Last X hours.    The Estimated number of affected hosts section updates as you fill in the required fields.</li> </ul> </li> <li>Select Start Scan to scan the selected hosts and ingest the data into AQtive Guard for analysis.</li> </ol>"},{"location":"data-sources/crowdstrike/#view-crowdstrike-scan-data","title":"View CrowdStrike scan data","text":"<p>Once you start a scan, you can see the progress in the Scans table. The table displays the following information:</p> <ul> <li>Name - the scan name provided when configuring the scan.</li> <li>Status - the status of a scan. The possible values are:<ul> <li>Pending - scan is in the scan queue.</li> <li>In Progress - actively scanning.</li> <li>Completed - scan has been successfully ingested into AQtive Guard. </li> <li>Canceled - the scan has been terminated by user action. </li> </ul> </li> <li>Progress - the number of hosts scanned / total number of hosts selected.</li> <li>Scan Start - the timestamp for when the scan started (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Scan End - the timestamp for when the scan is completed or stopped (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Duration - the length of time the scan was active.</li> </ul> <p>You can view more information for a specific scan by selecting Details in the row of the scan you\u2019re interested in. The Scan Results table appears, which displays the following:</p> <ul> <li>IT Asset - the name of the asset. </li> <li>Status - the status of the scan on the asset. The possible values are:<ul> <li>Pending</li> <li>Scanning</li> <li>Retrieval</li> <li>Downloading</li> <li>Success</li> <li>Fail</li> </ul> </li> <li>Platform - the asset\u2019s platform. This is either Linux or Windows.</li> </ul> <p>Any successful asset scans will be also be shown in IT assets.</p>"},{"location":"data-sources/crowdstrike/#unlink-the-crowdstrike-integration","title":"Unlink the CrowdStrike integration","text":"<p>Unlink the CrowdStrike integration only if your organization needs to reconfigure or stop data sharing with CrowdStrike.</p> <p>To unlink the CrowdStrike configuration:</p> <ol> <li>Select Data sources from the main menu, then select Details in the CrowdStrike panel.  </li> <li>Select Unlink.</li> <li>Select Confirm and unlink CrowdStrike.</li> </ol>"},{"location":"data-sources/palo-alto-networks/","title":"Palo Alto Networks","text":"<p>AQtive Guard seamlessly integrates with Palo Alto Networks Next-Generation Firewalls (NGFW) by ingesting and analyzing TLS handshake data directly from firewall log files. By integrating these logs, AQtive Guard provides a centralized platform for analyzing potential cryptographic risks and anomalies within your network traffic.</p>"},{"location":"data-sources/palo-alto-networks/#prerequisites","title":"Prerequisites","text":"<ul> <li>A valid Comma-Separated Values (CSV) log file exported from a Palo Alto NGFW.</li> </ul>"},{"location":"data-sources/palo-alto-networks/#export-a-log-file-from-a-palo-alto-networks-ngfw","title":"Export a log file from a Palo Alto Networks NGFW","text":"<p>You can export the contents of a log file to a CSV-formatted report. By default, the report contains up to 2,000 rows of log entries.</p>"},{"location":"data-sources/palo-alto-networks/#set-the-number-of-rows-to-export","title":"Set the number of rows to export","text":"<ol> <li>Under Device and Setup, select Management.</li> <li>Select the Log Export and Reporting tab. </li> <li>Edit the number of Max Rows in CSV Export (up to 1,048,576 rows).</li> <li>Select OK to save your changes.</li> </ol>"},{"location":"data-sources/palo-alto-networks/#download-the-log","title":"Download the log","text":"<ol> <li>Select Export to CSV and wait for the progress bar to complete.</li> <li>Select Download file to save the log to your local folder.</li> </ol>"},{"location":"data-sources/palo-alto-networks/#upload-a-palo-alto-networks-log-file","title":"Upload a Palo Alto Networks log file","text":"<p>To upload the log file in AQtive guard, follow these steps:</p> <ol> <li>Navigate to Data Sources from the main menu, then select Palo Alto Networks.</li> <li>To upload the CSV file, either:<ul> <li>Click in the target area and select the file from your local system.</li> <li>Drag and drop the file into the target upload area.</li> </ul> </li> <li>The data will begin uploading automatically.</li> </ol>"},{"location":"data-sources/qualys/","title":"Getting started with Qualys data source","text":"<p>Integration with Qualys enables you to import scans from Qualys and generate a centralized cryptographic analysis of the available data:</p> <ul> <li>Secrets - In Qualys, container secrets are digital credentials providing identity authentication and authorizing access to privileged accounts, applications, and services.</li> <li>Certificates - Digitally signed documents used to verify the identity of a server or website.</li> <li>TLS Configs - Capture the cryptography used in a handshake between a client and server to negotiate the encryption protocols and algorithms used for secure communication.</li> </ul>"},{"location":"data-sources/qualys/#qualys-requirements","title":"Qualys requirements","text":"<ul> <li>The Qualys Gateway URL you will connect to. Refer to Identify your Qualys platform.</li> <li>A username and password for the Qualys admin account to be used for authentication.</li> </ul>"},{"location":"data-sources/qualys/#configure-the-qualys-integration","title":"Configure the Qualys integration","text":"<p>There are three main steps to configure the Qualys integration:</p> <ol> <li>In Qualys: Create an AQtive Guard role.</li> <li>In Qualys: Create an AQtive Guard user and assign the AQtive Guard role. </li> <li>In AQtive Guard: Configure the Qualys data source.</li> </ol>"},{"location":"data-sources/qualys/#create-an-aqtive-guard-role","title":"Create an AQtive Guard role","text":"<p>In Qualys, follow these steps to create an AQtive Guard role. For more information, refer to Creating a role in the Qualys user documentation.</p> <ol> <li>Log in to the Qualys Administration portal as an administrator.</li> <li>Under the Modules dropdown, select the Administration utility. </li> <li>Select Roles to open the Role Management tab. </li> <li>Select New Role to open the Role Creation window. </li> <li>Under Role Details, enter a role Name (<code>AQTIVE GUARD</code>) and Description (optional).</li> <li>Select Continue to open the Permissions tab. </li> </ol>"},{"location":"data-sources/qualys/#edit-permissions-for-this-role","title":"Edit permissions for this role","text":"<p>Following the principle of least privilege, enable only the modules and permissions required for API access. All other modules and permissions should be manually disabled (unchecked).</p> <ol> <li>Under Edit permissions for this role, enable API Access by selecting the checkbox.</li> <li> <p>Search for the following required modules in the Modules dropdown and select them: </p> <ul> <li>Certificate View</li> <li>Container Security  3. To update permissions for each module, select Change and expand its list of permission groups. Configure permissions as follows:</li> <li>Certificate View module <p>* Under CERTVIEW Permissions, enable <code>CERTVIEW API Access</code>. </p> </li> </ul> <ul> <li> <p>Container Security module </p> <p>* Under CS Image Permissions, select <code>List Image/s</code>.</p> <p>* Under CS Permissions, select <code>CS API Access</code>.  4. Once you\u2019ve selected the required modules and permissions, select Continue. 5. Under Review and confirm role permissions, select Finish.</p> <p></p> </li> </ul> </li> </ol>"},{"location":"data-sources/qualys/#create-an-aqtive-guard-user","title":"Create an AQtive Guard user","text":"<p>In Qualys, follow these steps to create an AQtive Guard user with the AQTIVE GUARD role.</p> <ol> <li>Log in to the Qualys Administration portal as an administrator.</li> <li>Under the Modules dropdown, select the Administration utility. </li> <li>Select Users to open the User Management tab. </li> <li>Select Create User, then select Create Manager User from the dropdown. </li> <li>In the General Information tab, complete the required fields for the Qualys admin responsible for the AQtive Guard integration. </li> <li>In the User Role tab, next to Allow access to: select <code>API</code>.</li> <li>Select Save to finish creating the user.</li> </ol>"},{"location":"data-sources/qualys/#assign-the-role-to-the-new-user","title":"Assign the role to the new user","text":"<ol> <li>In the Qualys Administration utility, select Users to open the User Management tab.</li> <li>Select the Username for the <code>manageruser</code> you created for the AQtive Guard integration, then select Edit to open the User Edit window. </li> <li>In the Roles and Scopes tab, assign the <code>AQTIVE GUARD</code> role you created earlier.</li> <li>Under Edit Scope, select Allow user view access to all objects, or adjust as necessary for your BU (required).   </li> <li>Select Save to assign the role. </li> </ol> <p>Important</p> <p>View access permission is required to allow AQtive Guard to ingest Qualys data.</p> <p></p>"},{"location":"data-sources/qualys/#configure-the-qualys-data-source","title":"Configure the Qualys data source","text":"<p>Log in to AQtive Guard to complete the following steps.</p> <ol> <li>Select Data sources from the main menu, then select Configure in the Qualys panel.</li> <li>Enter the following information into the designated fields:  <ul> <li>Instance URL - the location of the Qualys API.  </li> <li>Username - your username required to authenticate with the Qualys API.  </li> <li>Password - password associated with the username for authentication.  </li> </ul> </li> <li>(Optional) Select Test Connection to check the connection to the Qualys API.</li> <li>Select Submit to update the settings.</li> </ol> <p>Note</p> <p>Selecting Submit performs the same check as the Test connection button, in addition to verifying the username and password are valid.</p>"},{"location":"data-sources/qualys/#use","title":"Use","text":"<p>Once the integration is configured, there are three types of Qualys ingestions available: Secrets, Certificates, and Handshakes. </p> <p>Note</p> <p>Only one type of ingestion can be active at a time.</p> <p>All of these processes:</p> <ol> <li>Ping the Qualys API.  </li> <li>Extract the relevant data.  </li> <li>Map the data into the AQtive Guard format.  </li> <li>Ingest the data into AQtive Guard for analysis.</li> </ol> <p>Note</p> <p>If the Qualys settings aren\u2019t configured, the ingestion options will be disabled.</p>"},{"location":"data-sources/qualys/#ingest-qualys-secrets","title":"Ingest Qualys secrets","text":"<p>To ingest secrets data from Qualys:</p> <ol> <li>Select Data sources from the main menu, then select Details in the Qualys panel.</li> <li>Select Start secrets ingestion to trigger the ingestion of secrets data into AQtive Guard.</li> </ol>"},{"location":"data-sources/qualys/#ingest-qualys-certificates","title":"Ingest Qualys certificates","text":"<p>To ingest certificate data from Qualys:</p> <ol> <li>Select Data sources from the main menu, then select Details in the Qualys panel.</li> <li>Select Start certificates ingestion to trigger the ingestion of certificate data into AQtive Guard.</li> </ol>"},{"location":"data-sources/qualys/#ingest-qualys-handshakes","title":"Ingest Qualys handshakes","text":"<p>To ingest handshake data from Qualys:</p> <ol> <li>Select Data sources from the main menu, then select Details in the Qualys panel.</li> <li>Select Start TLS configs ingestion to trigger the ingestion of handshake data into AQtive Guard.</li> </ol>"},{"location":"data-sources/qualys/#view-qualys-data","title":"View Qualys data","text":"<p>Once the ingestion is complete, any relevant data will begin populating. To find it:</p> <ul> <li>From IT Assets, select Hosts or Container images.  </li> <li>From Inventory, select either the Secrets, Certificates, or Handshakes tab, depending on the ingestion type. </li> </ul> <p>Look for the Qualys tag in the Data sources column.</p>"},{"location":"data-sources/qualys/#unlink-the-qualys-integration","title":"Unlink the Qualys integration","text":"<p>Unlink the Qualys integration only if your organization needs to reconfigure or stop data sharing with Qualys.</p> <p>To unlink the Qualys configuration:</p> <ol> <li>Select Data sources from the main menu, then select Details in the Qualys panel. </li> <li>Select Unlink.</li> </ol>"},{"location":"data-sources/sentinelone/","title":"Getting started with SentinelOne","text":"<p>AQtive Guard can ingest SentinelOne endpoint data, including installed applications and IT inventory details, to enable centralized IT management and enhance security posture.</p>"},{"location":"data-sources/sentinelone/#sentinelone-requirements","title":"SentinelOne requirements","text":"<ul> <li>The SentinelOne URL you will connect to.</li> <li>A SentinelOne API token (requires user creation).</li> </ul>"},{"location":"data-sources/sentinelone/#configure-the-sentinelone-integration","title":"Configure the SentinelOne integration","text":"<p>There are two main steps to configure the SentinelOne integration:</p> <ol> <li>In SentinelOne: Create a user account and generate an API token.</li> <li>In AQtive Guard: Configure the SentinelOne data source.</li> </ol>"},{"location":"data-sources/sentinelone/#create-a-user","title":"Create a user","text":"<p>In SentinelOne, follow these steps to create a user and generate an API token. For more information, refer to Generating an API token in the SentinelOne user documentation.</p> <ol> <li>Sign in to the SentinelOne Management Console with Admin user credentials.</li> <li>In the Management Console, select Settings.</li> <li>In the Settings view, select Users, then Service Users.</li> <li>Select the Actions dropdown, then Create New Service User.</li> <li>Enter the information for the new service user.</li> <li>In Role, select Admin.</li> <li>Select Save.</li> </ol> <p>Log in to the SentinelOne Management Console with the credentials of the new user you just created to complete the following steps.</p> <ol> <li>Navigate to Settings, then Users.</li> <li>Select the newly added service user.</li> <li>Select Options, then Generate API token.</li> <li>Copy or download this API Token.</li> </ol> <p>Note</p> <p>The API token will not be displayed again for security reasons. </p>"},{"location":"data-sources/sentinelone/#configure-the-sentinelone-data-source","title":"Configure the SentinelOne data source","text":"<p>Log in to AQtive Guard to complete the following steps.</p> <ol> <li>Select Data sources from the main menu, then select Configure in the SentinelOne panel.</li> <li>Enter the following information into the designated fields:  <ul> <li>Instance URL - the location of the SentinelOne API.  </li> <li>API Token - the API token you generated in SentinelOne.   </li> </ul> </li> <li>(Optional) Select Test Connection to check the connection to the SentinelOne API.</li> <li>Select Submit to update the settings.</li> </ol> <p>Note</p> <p>Selecting Submit performs the same check as the Test connection button, in addition to verifying the API token is valid.</p>"},{"location":"data-sources/sentinelone/#use","title":"Use","text":"<p>Once the integration is configured, you can trigger a SentinelOne inventory ingestion.</p> <p>Note</p> <p>If the SentinelOne settings aren\u2019t configured, the ingestion option will be disabled.</p> <p>To ingest inventory data from SentinelOne:</p> <ol> <li>Select Data sources from the main menu, then select Details in the SentinelOne panel.</li> <li> <p>Select: Start IT Inventory ingestion to trigger the ingestion of data into AQtive Guard.</p> <p>You\u2019ll see a notification confirming that the data ingestion has started.</p> </li> </ol>"},{"location":"data-sources/sentinelone/#view-sentinelone-data","title":"View SentinelOne data","text":"<p>Once the ingestion is complete, any relevant data will begin populating. To find it:</p> <ul> <li>From IT Assets, select Hosts or Apps.  </li> </ul> <p>Look for the S1 tag in the Data sources column.</p>"},{"location":"data-sources/sentinelone/#unlink-the-sentinelone-integration","title":"Unlink the SentinelOne integration","text":"<p>Unlink the SentinelOne integration only if your organization needs to reconfigure or stop data sharing with SentinelOne.</p> <p>To unlink the SentinelOne configuration:</p> <ol> <li>Select Data sources from the main menu, then select Details in the SentinelOne panel. </li> <li>Select Unlink.</li> <li>Select Confirm and unlink SentinelOne.</li> </ol>"},{"location":"data-sources/servicenow/","title":"ServiceNow","text":"<p>AQtive Guard can ingest certificate data from ServiceNow for centralized certificate management and enhanced security posture.</p>"},{"location":"data-sources/servicenow/#servicenow-requirements","title":"ServiceNow requirements","text":"<p>To enable the ServiceNow integration, you\u2019ll need:</p> <ul> <li>Admin access to AQtive Guard.</li> <li>Coordination with a ServiceNow Administrator to create a ServiceNow user account.</li> </ul>"},{"location":"data-sources/servicenow/#configure-the-servicenow-integration","title":"Configure the ServiceNow integration","text":"<p>In your ServiceNow instance, your ServiceNow administrator will need to create a user account to be linked with AQtive Guard.</p> <p>Tip</p> <p>Make sure your ServiceNow administrator adds the <code>sn_disco_certmgmt.pki_user</code> role to the integration user account.</p> <p>Log in to AQtive Guard to complete the following steps.</p> <ol> <li>Select Data sources from the main menu, then select Configure in the ServiceNow panel.  </li> <li>Enter the information from ServiceNow into the designated fields:<ul> <li>Your ServiceNow Instance URL.</li> <li>The User ID of the integration user account your ServiceNow administrator created.</li> <li>The Password for the user account.</li> </ul> </li> <li>(Optional) Select Test Connection to check the connection to the ServiceNow API.</li> <li>Select Submit to update the settings.</li> </ol> <p>Note</p> <p>Selecting Submit performs the same check as the Test connection button, in addition to verifying the user ID and password are valid.</p>"},{"location":"data-sources/servicenow/#use","title":"Use","text":"<p>Once the integration is configured, you can trigger a ServiceNow certificate ingestion. </p> <p>This process:</p> <ol> <li>Pings the ServiceNow API.</li> <li>Extracts the relevant data.</li> <li>Maps the data into the AQtive Guard format.</li> <li>Ingests the data into AQtive Guard for analysis.</li> </ol> <p>Note</p> <p>If the ServiceNow settings aren\u2019t configured, the ingestion options will be disabled.</p>"},{"location":"data-sources/servicenow/#trigger-servicenow-ingestion","title":"Trigger ServiceNow ingestion","text":"<p>To ingest certificate data from ServiceNow:</p> <ol> <li>Select Data sources from the main menu, then select Details in the ServiceNow panel.</li> <li>Select Start certificates ingestion to trigger ingestion of certificate data into AQtive Guard.</li> </ol>"},{"location":"data-sources/servicenow/#view-servicenow-ingestion-data","title":"View ServiceNow ingestion data","text":"<p>Once the ingestion is complete, any relevant data will begin populating. To find it, select Certificates from Inventory and look for the ServiceNow tag in the Data sources column.</p>"},{"location":"data-sources/servicenow/#unlink-the-servicenow-integration","title":"Unlink the ServiceNow integration","text":"<p>Unlink the ServiceNow integration only if your organization needs to reconfigure or stop data sharing with ServiceNow.</p> <p>To unlink the ServiceNow configuration:</p> <ol> <li>Select Data sources from the main menu, then select Details in the ServiceNow panel.  </li> <li>Select Unlink. </li> <li>Select Confirm and unlink ServiceNow.</li> </ol>"},{"location":"exports/","title":"Exports","text":"<p>AQtive Guard Exports enables you to save and export custom Inventory or IT Asset views, allowing you to analyze and report on specific data sets.</p> <p>Exports allow you to:</p> <ul> <li>Save custom data views: Preserve the exact configuration of your data view, including applied filters and sorting.</li> <li>Export data in standard formats: Export the displayed data as a CSV for further analysis and reporting.</li> <li>Re-run past queries: Quickly refresh a previously saved data view with up-to-date results.</li> </ul>"},{"location":"exports/#use","title":"Use","text":"<p>The following sections explain how to use exports.</p>"},{"location":"exports/#create-an-export","title":"Create an export","text":"<p>To create an export:</p> <ol> <li>Select Inventory or IT Assets from the main menu, then select the appropriate tab for the data type you\u2019d like to create an export of. </li> <li>Apply local or global filters until you\u2019ve narrowed down to the specific information you want to export.</li> <li>Select Export and enter:<ul> <li>A name for the export.</li> <li>An optional description.</li> </ul> </li> <li>Select Create.</li> </ol> <p>You\u2019ll be taken to the Exports page in a new tab where you can view the details.</p> <p>Note</p> <p>You can also access this page by selecting Exports in the main menu.</p>"},{"location":"exports/#view-export-details","title":"View export details","text":"<p>Use the export details view to manage and re-run previous exports.</p> <ol> <li>Select Exports from the main menu.</li> <li>Select Details in the row of the export you want to see.</li> </ol> <p>The Exports screen shows the following information about a previously created export:</p> <ul> <li>Description - The description you entered when you created the export.</li> <li>Content to export - The category of data you exported (keys, certificates, etc.)</li> <li>Last exported - the most recent date and time the export was run (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Global filters - Any global filters that were applied during creation of the export.</li> <li>Local filters - Any local filters that were applied during creation of the export.</li> </ul> <p>Tip</p> <p>You can also re-open an export to review and modify its saved filters by selecting Restore filters and open. </p> <p>This screen also contains the Export history table, which shows:</p> <ul> <li>Created at - the date and time the export was created (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Status - the status of the export.</li> </ul> <p>You can also download or delete an export on the Exports screen.</p>"},{"location":"exports/#download-an-export","title":"Download an export","text":"<p>To download an export:</p> <ol> <li>Select Exports from the main menu.</li> <li>Select Details in the row of the export you\u2019re interested in.</li> <li>In the Export history table, select Download</li> </ol>"},{"location":"exports/#delete-an-export","title":"Delete an export","text":"<p>You can delete an individual export record, or delete the entire export including all its settings and history.</p>"},{"location":"exports/#delete-a-single-export-record","title":"Delete a single export record","text":"<p>To delete the export record for an individual export:</p> <ol> <li>Select Exports from the main menu.</li> <li>Select Details in the row of the export you\u2019re interested in.</li> <li>In the Export history table, select Delete in the row of the export you want to remove.</li> </ol>"},{"location":"exports/#delete-an-entire-export","title":"Delete an entire export","text":"<p>You may delete an entire export, including all its settings and history. </p> <p>Caution</p> <p>Deleting an entire export is a permanent action that removes all associated filter settings and export history. To re-run the same export in the future, you\u2019ll need to re-create a new export using the same settings as the original. </p> <p>If you\u2019re sure you want to delete the export settings and history:</p> <ol> <li>Select Exports from the main menu.</li> <li>In the row of the export you want to delete, select Details.</li> <li>Select Delete export, then select Yes, delete export to confirm.</li> </ol>"},{"location":"filters-and-data-enrichment/","title":"Filters and data enrichment","text":"<p>Filters and data enrichment in AQtive Guard (AQG) enhance your inventory analysis by providing deeper insights and focusing your attention on relevant risks. These features are comprised of three related tools that work in harmony.</p> <p></p> <ol> <li>Local filters - allow you to temporarily narrow views in data tables and exports.</li> <li>Global filters - enable you to narrow the scope of data across dashboards, priority issues, and statistics for the current user. </li> <li>Enrichments {coming soon} - provide a specialized set of intelligent filters that leverage the proprietary AQG intelligence database.</li> </ol>"},{"location":"filters-and-data-enrichment/#using-filters","title":"Using filters","text":"<p>As you explore the following sections, experiment with different filter types to learn about their impact on your data view.</p> <p>Your filter settings offer several benefits to facilitate exploration:</p> <ul> <li>Non-destructive: Won\u2019t modify the underlying data.</li> <li>Personalized: Unique to your view, without affecting others in your organization.</li> <li>Flexible: Can be easily changed or reset at any time.</li> </ul> <p>Use local and global filters to focus on a specific subset of related items. Then, apply enrichments to bring in additional context for deeper insight and a more targeted understanding of the data.</p>"},{"location":"filters-and-data-enrichment/#local-filters","title":"Local filters","text":"<p>Local filters are found at the top of the Inventory and IT Assets tables, providing contextual filtering options to customize the data view within each table and tab. Use them to refine analysis results and facilitate more effective investigation and management.</p> <p>The scope of local filters is limited to your active table view, and they solely impact the data display and export from that table. Local filters don\u2019t influence other data, such as dashboards, priority issues, or statistics. </p> <p>Tip</p> <p>Local filter settings are temporary. They reset when you navigate away from the active tab, reverting to reflect current global filter settings.</p> <p>Refer to the Inventory and IT Assets for details on the specific data and filters found on each tab. </p>"},{"location":"filters-and-data-enrichment/#global-filters","title":"Global filters","text":"<p>Global filters, found at the top of all AQG data views, have a broader impact than local filters. They:</p> <ul> <li>Affect the data display across views.</li> <li>Influence dashboard data and statistics.</li> <li>Impact priority issues.</li> </ul> <p>Unlike local filters, global filters persist across all your views and sessions until you modify them, remaining in effect even after logging off and back in. This provides a consistent filtering experience throughout the AQtive Guard platform.</p> <p>Tip</p> <p>Any time your data isn\u2019t displaying as expected, check your global filter settings.</p>"},{"location":"filters-and-data-enrichment/#global-filter-types","title":"Global filter types","text":"<p>The following global filters are available to help you narrow down your data view across AQG:</p> <ul> <li>Profile - View data for one or more selected profiles. This enables you to focus on the data relevant to a particular profile, to assist you in auditing for compliance or other specialized needs. Refer to Profiles for details. </li> <li>Severity - Filter your data by issue severity. Refer to Issues to learn more. </li> <li>Source - Select one or more data sources to filter your view to display data from the select Data source(s) or AQG scanner(s). Refer to AQtive Guard Sensors and Data sources for details on available data sources. </li> <li>Current scans - Narrow the date and time range for your data view. This is especially useful when you want to: <ul> <li>Retrieve data from a recent scan.</li> <li>Analyze changes or trends within a particular time frame.</li> <li>Isolate issues or events that occurred during a specific scan.</li> </ul> </li> <li>Enrichments {coming soon} - A specialized intelligent filter, covered in detail in Enrichments.</li> </ul>"},{"location":"filters-and-data-enrichment/#enrichments","title":"Enrichments","text":"<p>Data enrichment is a powerful AQtive Guard feature that provides deeper, clearer insights into your inventory and cryptographic assets. It works by sifting through the noise and filtering out irrelevant or misleading data. This ensures you get a more accurate and comprehensive understanding of your assets, giving you the clarity you need to make informed decisions and proactively manage potential compliance and security risks.</p> <p>With data enrichment, you can:</p> <ul> <li>Reduce the time you spend on root-cause analysis, allowing you to quickly identify and address potential issues.</li> <li>Zero in on actionable priorities, ensuring that you\u2019re focusing on the most critical aspects of your inventory.</li> <li>Gain deeper insights into your Non-Human Identity (NHI) and cryptographic assets and issues, empowering you to make more informed decisions.</li> <li>Create custom filters and views that meet your specific needs, making it easier to manage and prioritize compliance and remediation efforts.</li> </ul>"},{"location":"filters-and-data-enrichment/#how-it-works","title":"How it works","text":"<p>AQG data enrichment is powered by our proprietary intelligence database, continuously updated with information compiled by collecting and analyzing vast amounts of Non-Human Identity (NHI) and cryptographic data from numerous public online sources. This database is compiled via specialized collection and analysis processes, verified by our world-class cybersecurity and cryptography research team, ensuring that the data is accurate, relevant, and up-to-date.</p> <p>Enrichment can be accessed on the far right side of the Global filter bar from the following pages:</p> <ul> <li>The Keys and Certificates tabs on the Inventory page.</li> <li>The Issues page.</li> </ul> <p>From these pages, you can apply the enrichment filters to your inventory. This applies relevant metadata that adds useful context to better understand the nature, usage, and potential risks associated with discovered cryptographic objects. </p> <p>Tip</p> <p>The enrichment feature functions similarly to other Global filters, impacting data across dashboards, priority issues, and statistics, and persisting across views and login sessions. </p>"},{"location":"filters-and-data-enrichment/#using-data-enrichment","title":"Using data enrichment","text":"<p>To apply intelligent filters using enrichment metadata, follow these steps:</p> <ol> <li>Select Enrichments in the Global filter bar on the Issues page or the Keys or Certificates tabs of the Inventory page. </li> <li>For the desired item(s) under each data enrichment category, select one or more checkboxes:<ul> <li>Include - adds or retains objects that meet the selected criteria.</li> <li>Exclude - removes objects that meet the selected criteria.</li> </ul> </li> </ol> <p>Tip</p> <p>The top five filters with the most associated objects are displayed under each category by default. Use the Search at the top of the enrichment categories to filter the view based on your search criteria. </p>"},{"location":"filters-and-data-enrichment/#enrichment-categories","title":"Enrichment categories","text":"<p>Available enrichment categories include:</p> <ul> <li>Publicly known: This inventory object is recognized from publicly available software or online sources.</li> <li>Unknown: This inventory object is not recognized from publicly available software or online sources.</li> <li>Distribution: This inventory object is associated with a specific Docker image.</li> <li>Package: This inventory object is associated with a specific software package or application bundle.</li> <li>Application: This inventory object is linked to a particular application or application category.</li> <li>Path: This inventory object was discovered in your filesystem at a specific location.</li> </ul>"},{"location":"getting-started/","title":"Getting started","text":"<p>As organizations embrace digital transformation, cloud adoption, and artificial intelligence (AI), they face a significant increase in Non-Human Identities (NHIs) such as service accounts, API keys, device identities, and AI agents. Securing these NHIs is paramount.</p> <p>The foundation for securing many NHIs is cryptography\u2014the keys and certificates used for authentication and secure communication. However, managing this cryptographic landscape is complex due to its often siloed, poorly inventoried, and inconsistently managed nature.</p> <p>This complexity leads to significant risks:</p> <ul> <li>Outages caused by expired certificates.</li> <li>Breaches resulting from weak or compromised keys.</li> <li>Compliance failures due to unmanaged cryptographic assets.</li> <li>Inability to prepare for future threats, such as those posed by quantum computing.</li> </ul> <p>AQtive Guard (AQG) addresses these foundational challenges. It\u2019s an AI-powered cybersecurity platform designed to manage and secure NHIs and other cryptographic assets. AQG provides an end-to-end cryptographic management platform that offers a full inventory of existing cryptography usage, including vulnerability and compliance analysis. It also offers a path to centrally managed, robust, and agile cryptography.</p> <p>This guide introduces the key components of AQG and how they work together. The following diagram illustrates the principal elements and data flow.</p> <p></p> <ol> <li>Host Machines and Cloud Network - The AQG <code>yanadump</code> tool processes live network traffic and extends monitoring to cloud assets via traffic mirroring, ensuring seamless coverage in hybrid environments.</li> <li>Data sources - AQG offers powerful AQG sensors, for in-depth cryptographic analysis of filesystems, applications, and physical and virtual networks. It also integrates with a variety of external data sources to centralize your IT assets and crypto inventory.</li> <li>Analysis - When AQG analyzes data, it evaluates detected cryptographic objects and NHI against the active rules.</li> <li>API - The AQG API enables integration and automation, and powers the AQG web UI.</li> <li>UI - Provides a user-friendly interface for monitoring your cryptography and NHI data.</li> </ol>"},{"location":"getting-started/#host-machines-and-cloud-network","title":"Host Machines and cloud network","text":"<p>To gain comprehensive insight into cryptographic activity across physical and cloud infrastructures, AQG leverages the yanadump tool. This tool enables real-time processing of live network traffic and extends monitoring to cloud assets through cloud-native traffic mirroring, ensuring seamless coverage in hybrid environments.</p>"},{"location":"getting-started/#data-sources","title":"Data sources","text":"<p>The first step in cryptographic management is understanding the existing cryptographic landscape. AQG achieves comprehensive discovery through a powerful combination of data sources.</p> <p>AQG provides two types of data sources: Native AQG sensors and integrated third-party data sources.</p>"},{"location":"getting-started/#native-aqg-sensors","title":"Native AQG sensors","text":"<p>Native AQG sensors include the Filesystem Scanner, Java Tracer, and Network Analyzer. Developed by SandboxAQ, these sensors perform deep, comprehensive discovery, often identifying cryptographic assets that other tools might miss.</p> <ul> <li>The Filesystem Scanner scans filesystems and container images to create a trace file containing cryptographic data.</li> <li>The Java Tracer logs cryptographic calls made by a Java Virtual Machine (JVM) and its associated Java application, generating a trace file with cryptographic data.</li> <li>The Network Analyzer processes static or streaming network traffic to detect Transport Layer Security (TLS) configurations and handshakes. It can be used with appliances like NetScout or cloud provider tools such as Virtual Private Cloud (VPC) Traffic Mirror.</li> </ul>"},{"location":"getting-started/#third-party-data-sources","title":"Third-party data sources","text":"<p>AQG integrates with existing security and information technology tools, allowing ingestion of data from sources you may already use. Current integrations include:</p> <ul> <li>Qualys - Import Qualys certificate and server scan data and analyze potential cryptographic vulnerabilities.</li> <li>CrowdStrike: Import scans from CrowdStrike Falcon and generate a cryptographic analysis of available data.</li> <li>ServiceNow: Ingest certificate data from ServiceNow for centralized certificate management and enhanced security posture.</li> <li>AWS Key Management Service (KMS): Ingest data from AWS KMS for enhanced key management and security monitoring.</li> <li>Palo Alto Networks: Ingest and analyze TLS handshake data from Next-Generation Firewall log files.</li> <li>SentinelOne: Ingest SentinelOne data for installed applications and IT inventory details.</li> </ul> <p>AQG can also ingest and analyze Cryptography Bill of Materials (CBOM) JSON files and Packet Capture (PCAP) files. This allows for immediate value while planning broader sensor deployment for deeper visibility.</p>"},{"location":"getting-started/#rules-and-profiles","title":"Rules and profiles","text":"<p>In AQG, Rules are grouped together as Profiles. These profiles define the criteria for cryptographic analysis. When data is ingested, the system checks the detected cryptography against these rules. If a rule violation is detected, it becomes an Issue.</p> <p>AQG offers built-in Profiles for NIST compliance and Post-Quantum Cryptography (PQC) readiness. For instance, the NIST Profile includes rules such as Certificate validity too long to help meet NIST recommendations. You can also create custom profiles and rules for specific organizational needs.</p>"},{"location":"getting-started/#inventory","title":"Inventory","text":"<p>Once data is ingested and rules are applied, this combined data flows into a unified Inventory. This inventory provides a comprehensive view of cryptographic assets, which can be browsed, filtered, and exported.</p> <p>The inventory is organized into several categories:</p> <ul> <li>Keys: Provides a comprehensive view of the keys within the inventory, including their type (e.g., Rivest-Shamir-Adleman (RSA) or Digital Signature Algorithm (DSA)), length in bits, and whether they have an associated private key. This helps identify potential weaknesses like short RSA keys.</li> <li>Certificates: Offers a detailed overview of an organization\u2019s digital certificates, including validity periods, signing algorithms, and issuer information. This is crucial for identifying certificates with long validity periods or those using quantum-vulnerable signature algorithms (e.g., RSA, Elliptic Curve Digital Signature Algorithm (ECDSA)).</li> <li>Operations: Shows cryptographic calls made by applications and their various functions.</li> <li>TLS Configs: Displays configurations related to TLS.</li> <li>Handshakes: Represents TLS handshakes detected in network traffic between source and target addresses, detailing TLS versions and selected ciphersuites.</li> <li>Secrets: Represents the discovery of sensitive assets, such as AWS access keys hardcoded in a Docker image.</li> </ul> <p>Global filters, such as Profile, Severity, Source, and Current Scans can be applied to narrow down the visible data based on triggered rules.</p>"},{"location":"getting-started/#it-assets","title":"IT assets","text":"<p>The IT Assets page provides an inventory of:</p> <ul> <li>Hosts: Hosts provides an overview of scanned laptops and servers, showing their hostname, operating system, and last scan date. It also lists the data sources, like Qualys or the AQG Filesystem scanner, that contributed to the host information. Drilling into a host\u2019s Details shows an in-depth view of the cryptography discovered on that host, including associated keys and certificates.</li> <li>Apps: Lists applications that have been scanned.</li> <li>Container Images: Provides similar information for cryptography discovered within container images, allowing tracking of cryptography within these dynamic systems.</li> </ul>"},{"location":"getting-started/#api","title":"API","text":"<p>AQG provides a comprehensive, robust API with endpoints for custom reporting or data exchanges with external systems. Data displayed in the UI can be easily accessed from these API endpoints using scripting languages like Python. For example, the API can automate cryptography status reports for specific IT assets, significantly reducing manual effort.</p>"},{"location":"getting-started/#ui","title":"UI","text":""},{"location":"getting-started/#dashboard","title":"Dashboard","text":"<p>When you first log into AQG, you\u2019re presented with a comprehensive Dashboard. This serves as a central hub, providing an overview of the entire cryptographic environment and immediate insights into the status of scanned IT assets and overall cryptographic health.</p>"},{"location":"getting-started/#issues","title":"Issues","text":"<p>The Issues page presents a consolidated and prioritized risk picture. AQG evaluates every key, certificate, and cryptographic operation against rules based on standards like NIST, Federal Information Processing Standards (FIPS), and other industry best practices.</p> <p>Issues are ranked by Severity and the number of Occurrences, enabling a focus on the most critical concerns immediately. By selecting the Details of an issue, you can explore:</p> <ul> <li>A brief description of the issue.</li> <li>Guidance on how to resolve the finding.</li> <li>Risk factors associated with not resolving it.</li> <li>References for further information.</li> <li>Specifications: The standard configuration of the rule that triggered the finding.</li> </ul> <p>The Dashboard\u2019s most critical section is Highest impact issues. This area immediately highlights out-of-policy findings, prioritized by severity and occurrences, enabling quick focus on the most pressing security and compliance concerns. To explore issue details, select View or navigate to the Issues tab in the main menu.</p>"},{"location":"glossary/","title":"Cryptography glossary","text":""},{"location":"glossary/#a","title":"A","text":""},{"location":"glossary/#cryptographic-algorithm","title":"(Cryptographic) Algorithm","text":"<p>A sequence of computational instructions that performs specific (cryptographic) tasks.</p>"},{"location":"glossary/#api-application-programming-interface","title":"API (Application Programming Interface)","text":"<p>A set of functions that enable different software components to communicate and transfer data.</p>"},{"location":"glossary/#aqtive-guard-api-client","title":"(AQtive Guard) API client","text":"<p>The AQtive Guard API Client (cs-api) is a CLI program you can use to interface with AQtive Guard using its API. Its two main purposes are uploading traces created using the AQtive Guard sensors and launching an analysis in AQtive Guard.</p>"},{"location":"glossary/#aql-aq-language","title":"AQL (AQ Language)","text":"<p>The SandboxAQ domain-specific language (DSL) designed for querying AQtive Guard data. It\u2019s interpreted as a KSQL table. </p>"},{"location":"glossary/#asymmetric-cryptography","title":"Asymmetric cryptography","text":"<p>Synonym: public key cryptography</p>"},{"location":"glossary/#augmented-trace","title":"Augmented trace","text":"<p>A trace file enhanced with additional details, such as cryptographic call sites.</p>"},{"location":"glossary/#authenticity","title":"Authenticity","text":"<p>A security property achieved through cryptographic methods that guarantees data is genuine, verifiable, and trustworthy. It provides confidence in the validity of a transmission, information, message, or its sender.</p>"},{"location":"glossary/#b","title":"B","text":""},{"location":"glossary/#block-cipher","title":"Block cipher","text":"<p>A cryptographic algorithm that maps blocks of fixed lengths to blocks of the same length such that for every output there is exactly one input.</p>"},{"location":"glossary/#brute-force-attack","title":"Brute-force attack","text":"<p>An attack method of trying every possible value for a secret input, like a secret key or password, until the correct one is found.</p>"},{"location":"glossary/#c","title":"C","text":""},{"location":"glossary/#ccpa-california-consumer-privacy-act","title":"CCPA (California Consumer Privacy Act)","text":"<p>A California law that gives residents rights over their personal data, including the right to know what information is collected, to have it deleted, and to opt out of its sale. It requires businesses to be transparent about their data practices and protect consumer privacy.</p>"},{"location":"glossary/#certificate","title":"Certificate","text":"<p>A signed document, linking a public key to an identity. Common examples of an identity are a domain name, an email address, or a X.500 common name. Certificates usually also contain additional management data, like a validity period, a serial number, issuer identity, and potential extensions providing more context data.  </p>"},{"location":"glossary/#certificate-authority-ca","title":"Certificate Authority (CA)","text":"<p>A trusted organization that issues certificates to verified entities, such as websites or individuals, linking their identity to public keys. This allows the user to authenticate a communication partner using public key cryptography.</p>"},{"location":"glossary/#csr-certificate-signing-request","title":"CSR (Certificate Signing Request)","text":"<p>A message sent from an applicant to a certificate authority (CA) in order to apply for a certificate.</p>"},{"location":"glossary/#collision","title":"Collision","text":"<p>A scenario where two distinct inputs to a cryptographic hash function produce the same output.</p>"},{"location":"glossary/#collision-attack","title":"Collision attack","text":"<p>An attack that attempts to find two inputs to a cryptographic hash function that produce the same output.</p>"},{"location":"glossary/#computational-infeasibility-computationally-infeasible","title":"Computational infeasibility (Computationally infeasible)","text":"<p>A task or calculation is considered computationally infeasible if it would require an unrealistically long time or computing resources to complete, making it practically impossible to achieve within a practical timeframe.</p>"},{"location":"glossary/#confidentiality","title":"Confidentiality","text":"<p>A security property that guarantees data is not disclosed to users, processes, or devices unless they are authorized to access the information.</p>"},{"location":"glossary/#counter-with-cbc-mac-ccm","title":"Counter with CBC-MAC (CCM)","text":"<p>A block cipher mode of operation that provides both confidentiality and authentication. </p>"},{"location":"glossary/#crl-certificate-revocation-list","title":"CRL (Certificate Revocation List)","text":"<p>A list of certificates that have been revoked by the issuing certificate authority (CA) before their scheduled expiration date. CRL certificates should no longer be trusted.</p>"},{"location":"glossary/#cryptography-bill-of-materials-cbom","title":"Cryptography Bill of Materials (CBOM)","text":"<p>A cryptography bill of materials (CBOM) is a cybersecurity asset that lists all software and hardware components within a system, including their dependencies, vulnerabilities, and security risks.</p>"},{"location":"glossary/#cryptographic-boundary","title":"Cryptographic boundary","text":"<p>The clearly defined perimeter (physical or logical) that encloses all hardware, software, and firmware components of a cryptographic module. Everything inside this boundary is protected by the module\u2019s cryptographic functions.</p>"},{"location":"glossary/#cryptographic-call-site","title":"Cryptographic call site","text":"<p>A location in the code where cryptographic functions (such as encryption, decryption, or hashing) are invoked.</p>"},{"location":"glossary/#cryptographic-key","title":"Cryptographic key","text":"<p>A typically short piece of data that is used in most cryptographic algorithms to perform their operations with. The key type depends on the algorithm and determines its confidentiality requirements. For example, secret keys and private keys must remain confidential, while public keys are intended to be shared.</p>"},{"location":"glossary/#cryptographically-relevant-quantum-computer-crqc","title":"Cryptographically relevant quantum computer (CRQC)","text":"<p>A quantum computer capable of breaking cryptographic systems used in real-world applications, which are deemed infeasible to break with conventional computers.</p>"},{"location":"glossary/#cve-common-vulnerabilities-and-exposures","title":"CVE (Common Vulnerabilities and Exposures)","text":"<p>CVE, maintained by MITRE, is a system providing standardized identifiers for publicly known cybersecurity vulnerabilities. It catalogs and assigns unique IDs to vulnerabilities, facilitating information sharing and vulnerability management.</p>"},{"location":"glossary/#cwe-common-weakness-enumeration","title":"CWE (Common Weakness Enumeration)","text":"<p>CWE, maintained by MITRE, is a community-developed list of common software and hardware weakness types. It provides a standardized language for describing security flaws, aiding in software security analysis and development.</p>"},{"location":"glossary/#d","title":"D","text":""},{"location":"glossary/#decryption","title":"Decryption","text":"<p>The process of converting ciphertext back into plaintext, reverting encryption.</p>"},{"location":"glossary/#deprecated-cryptography","title":"Deprecated (cryptography)","text":"<p>A cryptographic algorithm or key length that was previously approved for a specific application but is no longer recommended due to identified security weaknesses.</p>"},{"location":"glossary/#digital-signature","title":"Digital signature","text":"<p>A value computed with a cryptographic process using a private key and appended to a data object to digitally sign it. The digital signature enables verification of the data object\u2019s authenticity and integrity using the signer\u2019s public key. A secure signature scheme guarantees the computational infeasibility of creating forgeries (refer to EUF-CMA).</p>"},{"location":"glossary/#e","title":"E","text":""},{"location":"glossary/#elliptic-curve-cryptography-ecc","title":"Elliptic Curve Cryptography (ECC)","text":"<p>A type of public key cryptography.</p>"},{"location":"glossary/#encryption","title":"Encryption","text":"<p>The process of transforming plaintext into ciphertext.</p>"},{"location":"glossary/#entropy","title":"Entropy","text":"<p>A measure of randomness or unpredictability in data, often used in cryptography to ensure secure key generation and encryption.</p>"},{"location":"glossary/#existential-unforgeability-under-chosen-message-attack-euf-cma","title":"Existential Unforgeability under Chosen Message Attack (EUF-CMA)","text":"<p>The standard security notion for a digital signature scheme. A scheme is EUF-CMA secure if it is computationally infeasible for an adversary to forge a valid signature, even when they can obtain valid signatures for messages they select or craft themselves.</p>"},{"location":"glossary/#f","title":"F","text":""},{"location":"glossary/#filesystem-scanner","title":"Filesystem scanner","text":"<p>An AQtive Guard sensor that scans the filesystem for cryptographic artifacts, like cryptographic keys or certificates.</p>"},{"location":"glossary/#forward-secrecy","title":"Forward secrecy","text":"<p>A property of cryptographic protocols that guarantees the secrecy of previously transmitted data remains protected, even if a private key is compromised.</p>"},{"location":"glossary/#g","title":"G","text":""},{"location":"glossary/#galoiscounter-mode-gcm","title":"Galois/Counter Mode (GCM)","text":"<p>A block cipher mode of operation that provides both encryption for confidentiality and authentication for data authenticity and integrity. It is widely used and recommended.</p>"},{"location":"glossary/#gdpr-general-data-protection-regulation","title":"GDPR (General Data Protection Regulation)","text":"<p>A European Union regulation that sets strict guidelines on collecting, processing, and storing personal data, granting individuals more control over their data and requiring organizations to ensure privacy and security compliance.</p>"},{"location":"glossary/#h","title":"H","text":""},{"location":"glossary/#cryptographic-hash-function","title":"(Cryptographic) Hash function","text":"<p>A core cryptographic building block that converts input data of arbitrary length into short, fixed-length outputs, known as a hash value or digest. A secure hash function ensures it is computationally infeasible to find two inputs that produce the same output collision, determine an input from its output, or to distinguish the output from random values. </p>"},{"location":"glossary/#hipaa-health-insurance-portability-and-accountability-act","title":"HIPAA (Health Insurance Portability and Accountability Act)","text":"<p>HIPAA, the Health Insurance Portability and Accountability Act, is a U.S. federal law designed to ensure the privacy and security of patients\u2019 health information. It sets standards for how healthcare organizations and their business associates handle, share, and protect sensitive medical data. </p>"},{"location":"glossary/#hardware-security-module-hsm","title":"Hardware Security Module (HSM)","text":"<p>A physical computing device that safeguards and manages cryptographic keys and provides cryptographic processing. An HSM is or contains a cryptographic module.</p>"},{"location":"glossary/#i","title":"I","text":""},{"location":"glossary/#integrity","title":"Integrity","text":"<p>A security property that guarantees that data or a system are not (maliciously) altered.</p>"},{"location":"glossary/#iv","title":"IV","text":"<p>A unique, unpredictable value used once (IV, Initialization Vector), typically as input to cryptographic operations to ensure different outputs for identical plaintexts. IVs help randomize encryption, preventing patterns and enhancing security. They are often generated randomly or sequentially, depending on the encryption mode.</p>"},{"location":"glossary/#j","title":"J","text":""},{"location":"glossary/#java-tracer","title":"Java tracer","text":"<p>An AQtive Guard sensor that records cryptographic operations in Java applications for analysis.</p>"},{"location":"glossary/#k","title":"K","text":""},{"location":"glossary/#key-agreement","title":"Key agreement","text":"<p>Synonym: key exchange.</p>"},{"location":"glossary/#key-derivation","title":"Key derivation","text":"<p>The process of deriving a secret key in a non-reversible manner from shared information, at least some of which is secret.</p>"},{"location":"glossary/#key-derivation-function-kdf","title":"Key Derivation Function (KDF)","text":"<p>A cryptographic algorithm that derives one or more secret keys from an input secret bit string of sufficient entropy, optionally incorporating additional context information.</p>"},{"location":"glossary/#key-encapsulation-mechanism-kem","title":"Key Encapsulation Mechanism (KEM)","text":"<p>A set of three cryptographic algorithms (called key generation, encapsulation, and decapsulation) that can be used to establish shared secret keys over a public channel.</p>"},{"location":"glossary/#key-establishment","title":"Key establishment","text":"<p>Synonym: key exchange. </p>"},{"location":"glossary/#key-exchange","title":"Key exchange","text":"<p>A process that allows two parties to securely establish a shared secret key over a public channel in order to establish secure communications.</p>"},{"location":"glossary/#key-management","title":"Key management","text":"<p>The process of creating, storing, distributing, and securely handling cryptographic keys throughout their lifecycle to protect sensitive data and ensure secure communications.</p>"},{"location":"glossary/#key-pair","title":"Key pair","text":"<p>Pair consisting of a private key and a public key. Used in public key cryptography.</p>"},{"location":"glossary/#key-transport","title":"Key transport","text":"<p>A procedure whereby one party (the sender) selects a value for the secret keying material and then securely distributes that value to another party or device (the receiver). Contrast with key exchange.</p>"},{"location":"glossary/#key-wrapping","title":"Key wrapping","text":"<p>A method of protecting secret keying material (along with associated integrity information) that provides both confidentiality and integrity protection when using symmetric-key algorithms.</p>"},{"location":"glossary/#l","title":"L","text":""},{"location":"glossary/#legacy-use","title":"Legacy use","text":"<p>Approval to continue using an algorithm or key length only in scenarios where legacy applications cannot be updated or replaced without unreasonable effort. </p>"},{"location":"glossary/#m","title":"M","text":""},{"location":"glossary/#machine-in-the-middle-mitm-attack","title":"Machine-In-The-Middle (MITM) Attack","text":"<p>A Machine-in-the-Middle attack is a cyberattack where the attacker secretly relays and possibly alters the communications between two parties who believe that they are directly communicating. Historically, this class of attacks was known as a man-in-the-middle (MITM) attack. The term attacker in the middle is more common, although some prefer Machine-in-the-Middle or Mallory-in-the-Middle to retain the well-known acronym MITM and still using gender-neutral language. </p>"},{"location":"glossary/#message-authentication-code-mac","title":"Message Authentication Code (MAC)","text":"<p>Cryptographic data, created with a symmetric key, that is attached to a message and can be used to verify its integrity and authenticity using the same key. </p>"},{"location":"glossary/#mode-of-operation","title":"Mode of operation","text":"<p>An algorithm for the cryptographic transformation of data that uses a block cipher.</p>"},{"location":"glossary/#n","title":"N","text":""},{"location":"glossary/#nhi-non-human-identity","title":"NHI (Non-Human Identity)","text":"<p>A digital identity used by automated systems such as applications, services, devices, or scripts to communicate with other machines and access resources. Managing NHIs involves securing their credentials (API keys, certificates, tokens, etc.) and controlling their access with policies.</p>"},{"location":"glossary/#nist-national-institute-of-standards-and-technology","title":"NIST (National Institute of Standards and Technology)","text":"<p>The agency responsible for U.S. Federal standards in various technical fields, including cryptography. NIST cryptography standards and recommendations have included algorithms such as AES, SHA-2, SHA-3, ML-KEM, ML-DSA, SLH-DSA, block cipher modes, key management, randomness generation, and statistical tests.</p>"},{"location":"glossary/#nonce","title":"Nonce","text":"<p>A unique number used once (nonce), typically as input to cryptographic operations to ensure different outputs for identical inputs. Since uniqueness can be challenging, nonces are often randomly generated. Authenticated ciphers use nonces to prevent attackers from detecting repeated messages.</p>"},{"location":"glossary/#o","title":"O","text":""},{"location":"glossary/#ocsp-online-certificate-status-protocol","title":"OCSP (Online Certificate Status Protocol)","text":"<p>An interactive protocol used to verify the revocation status of a certificate. </p>"},{"location":"glossary/#p","title":"P","text":""},{"location":"glossary/#pci-dss-payment-card-industry-data-security-standard","title":"PCI-DSS (Payment Card Industry Data Security Standard)","text":"<p>PCI DSS, the Payment Card Industry Data Security Standard, is a set of security requirements designed to ensure the safe processing, storage, and transmission of cardholder data. It applies to all businesses and organizations that accept, process, or store payment card information. </p>"},{"location":"glossary/#pkcs11","title":"PKCS#11","text":"<p>A standard defining cryptographic tokens, often used for smart cards and hardware security modules (HSMs).</p>"},{"location":"glossary/#post-quantum-cryptography-pqc","title":"Post-quantum cryptography (PQC)","text":"<p>Cryptographic algorithms designed to resist attacks not only from classical but also quantum computers.</p>"},{"location":"glossary/#private-key","title":"Private key","text":"<p>The secret part of a key pair that must be kept confidential. It\u2019s used to perform operations in (public key) cryptographic algorithms such as decryption or signing.</p>"},{"location":"glossary/#public-key","title":"Public key","text":"<p>The public part of a key pair. that does not need to be kept confidential and can be shared openly. It\u2019s used to perform operations in (public key) cryptographic algorithms such as encryption or signature verification. </p>"},{"location":"glossary/#public-key-cryptography","title":"Public key cryptography","text":"<p>Cryptography that uses a key pair consisting of a private key and a public key to secure communications. Also known as asymmetric cryptography.</p>"},{"location":"glossary/#public-key-infrastructure-pki","title":"Public Key Infrastructure (PKI)","text":"<p>A framework consisting of standards and services to enable secure communication. </p>"},{"location":"glossary/#q","title":"Q","text":""},{"location":"glossary/#quantum-bit-qubit","title":"Quantum bit (Qubit)","text":"<p>A fundamental unit of quantum information used in quantum computing, leveraging principles of quantum physics. Unlike classical bits, which are strictly 0 or <code>1</code>, qubits can exist in both states simultaneously. When measured, a qubit resolves to either <code>0</code> or <code>1</code>, with the outcome determined probabilistically based on its quantum state. Qubits enable quantum computers to store, process, and transmit data in ways that differ fundamentally from classical systems.</p>"},{"location":"glossary/#r","title":"R","text":""},{"location":"glossary/#root-certificate","title":"Root certificate","text":"<p>Certificate for the highest public key in a certificate hierarchy. Root certificates are self-signed and serve as the foundation of trust in the hierarchy. Root certificates from major Certificate Authorities (CAs) are commonly preinstalled in the trust stores of web browsers and operating systems, enabling trust in the identity of a public key owner. </p>"},{"location":"glossary/#certificate-revocation","title":"(Certificate) Revocation","text":"<p>Process of revoking a previously issued certificate, typically due to potential private key compromise or inaccuracies in the certificate\u2019s data, such as a change in domain ownership.</p>"},{"location":"glossary/#s","title":"S","text":""},{"location":"glossary/#salt","title":"Salt","text":"<p>A random value added to a password or input before hashing to ensure that identical inputs produce unique hashes. Salts protect against attacks like dictionary or rainbow table attacks by making hashed values harder to precompute and match.</p>"},{"location":"glossary/#self-signed-certificate","title":"Self-signed certificate","text":"<p>A certificate signed by the private key corresponding to its own public key, rather than being authenticated by a trusted third party. As a result, it does not provide any guarantee of authenticity.</p>"},{"location":"glossary/#security-strength","title":"Security strength","text":"<p>A number associated with the amount of resources (such as the number of operations, or amount of memory) required to break a cryptographic algorithm or system for a given key length. </p>"},{"location":"glossary/#shared-secret","title":"Shared secret","text":"<p>A secret value known to two or more parties, often used as input to a key-derivation method to produce (shared) secret keys.</p>"},{"location":"glossary/#secret-key","title":"Secret key","text":"<p>A cryptographic key used in symmetric cryptography. It does not require additional key derivation. The secret key must be kept confidential.</p>"},{"location":"glossary/#secret-keying-material","title":"Secret keying material","text":"<p>Synonym: Shared secret</p>"},{"location":"glossary/#signing-digital-signatures","title":"Signing (digital signatures)","text":"<p>The act of creating a digital signature for a data object.</p>"},{"location":"glossary/#software-bill-of-materials-sbom","title":"Software Bill of Materials (SBOM)","text":"<p>An SBOM (Software Bill of Materials) is a comprehensive list detailing all software components, their dependencies, and associated metadata within a software package or application, designed to aid vendors and developers in understanding and managing software dependencies. A Cryptography Bill of Materials (CBOM) is a security-focused extension of an SBOM, providing deeper visibility into cryptographic components.</p>"},{"location":"glossary/#sox-sarbanes-oxley-act","title":"SOX (Sarbanes-Oxley Act)","text":"<p>A U.S. federal law aimed at enhancing corporate governance and accountability. It requires public companies to ensure accurate financial reporting, conduct independent audits, and implement effective internal controls. These obligations include safeguarding the integrity of relevant financial and operational data.  </p>"},{"location":"glossary/#static-analysis","title":"Static analysis","text":"<p>A method of examining an application\u2019s code without executing it to identify potential security vulnerabilities, such as hard-coded cryptographic values or weak algorithms.</p>"},{"location":"glossary/#stream-cipher","title":"Stream cipher","text":"<p>A type of symmetric encryption that generates a continuous keystream to encrypt data one bit or byte at a time, rather than in blocks (contrast with block cipher). </p>"},{"location":"glossary/#strong-existential-unforgeability-under-chosen-message-attack-suf-cma","title":"Strong Existential Unforgeability under Chosen Message Attack (SUF-CMA)","text":"<p>A  security property for digital signature schemes. Beyond the guarantees of EUF-CMA, where adversaries cannot forge signatures even with access to valid signatures for chosen messages, SUF-CMA ensures that adversaries cannot modify or tweak an existing valid signature in any way that it remains valid.</p>"},{"location":"glossary/#symmetric-cryptography","title":"Symmetric cryptography","text":"<p>A type of cryptography where the same cryptographic key, called a secret key, is used for both encryption and decryption, or message authentication and verification. It is efficient for securing large amounts of data, but both parties must securely share the secret key in advance. For historical reasons, this term often is meant to also cover cryptographic hash functions, key derivation functions, and other similar key-less cryptographic algorithms.</p>"},{"location":"glossary/#t","title":"T","text":""},{"location":"glossary/#tlsssl","title":"TLS/SSL","text":"<p>Protocols used to secure data transmitted over a network by encrypting and authenticating the connection between a client and server. TLS (Transport Layer Security) is the modern, secure successor to the insecure SSL (Secure Sockets Layer) and is widely used to ensure data confidentiality, integrity, and authentication in web traffic.</p>"},{"location":"glossary/#u","title":"U","text":""},{"location":"glossary/#v","title":"V","text":""},{"location":"glossary/#verification-digital-signatures","title":"Verification (digital signatures)","text":"<p>The act of confirming the validity of a digital signature for a data object.</p>"},{"location":"glossary/#vpn-virtual-private-network","title":"VPN (Virtual Private Network)","text":"<p>A logical network that simulates the properties of a closed local area network, enabling secure communication between remote participants or network segments over public Internet connections. Traffic is protected using cryptographic protocols like IPSec, TLS, or WireGuard.</p>"},{"location":"glossary/#w","title":"W","text":""},{"location":"glossary/#x","title":"X","text":""},{"location":"glossary/#y","title":"Y","text":""},{"location":"glossary/#z","title":"Z","text":""},{"location":"inventory/","title":"Inventory","text":"<p>Select Inventory from the main menu to view detailed, itemized lists of your discovered cryptography. </p> <p>The tabs organize your cryptography by:</p> <ul> <li>Keys - a sequence of bits used as input to a cryptographic algorithm to encrypt, decrypt, sign, or authenticate data.</li> <li>Certificates  - a digitally signed document, linking a public key to an identity.</li> <li>Operations - a specific process or function, such as encryption, decryption, signing, or verification, that is performed on data using a key.</li> <li>Handshakes - processes to establish a secure connection between networked applications.</li> <li>Secrets - non-standard or user-defined objects retrieved from an external data source.</li> </ul> <p></p> <p>The following sections provide details for each tab.</p>"},{"location":"inventory/#keys","title":"Keys","text":"<p>The Keys tab provides a comprehensive view of the keys within your cryptography inventory, including the following information for each:</p> <ul> <li>Type - the algorithm used to generate the key, such as RSA, DSA, DH, etc.</li> <li>Length - the length of the key, in bits (1024, 2048, etc.)</li> <li>Has private key - indicates whether the key has an associated private key.</li> <li>Last scanned - the date and time of the most recent scan where the object was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> <li>Severity - when applicable, indicates the degree of non-compliance with policy as determined by the rule used for analysis. </li> </ul> <p>Use the local filters at the top of the table to filter by key Type or key Length. You may also apply one or more global filters at the top of the screen.</p>"},{"location":"inventory/#key-details","title":"Key details","text":"<p>To view more detailed information about a specific key, select Details. The details panel provides additional insights, including associated:</p> <ul> <li>Locations - where the item was discovered. </li> <li>Issues - generated when a rule flags the item.</li> <li>Certificates - a digitally signed document, linking a public key to an identity.</li> <li>Operations - a specific process or function, such as encryption, decryption, signing, or verification, that is performed on data using a key.</li> <li>Hosts - a physical or virtualized computing system, such as a server, workstation, or virtual machine, that serves as the operating environment for applications and container images. </li> <li>Apps - a software program or service running on a host.</li> <li>Container images - a packaged software component that was scanned, containing an application\u2019s filesystem and configuration.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>To access a JSON object containing all key metadata, scroll to the bottom of the panel and select View full key.</p>"},{"location":"inventory/#certificates","title":"Certificates","text":"<p>The certificate inventory is summarized in the Certificates tab, which includes the following information:</p> <ul> <li>Subject - the entity that this certificate represents, e.g. web site, application, individual.</li> <li>Issuer - the authority that issued the certificate.</li> <li>Valid until - the certificate expiration date.</li> <li>Validity period - the certificate validity period.</li> <li>Digest algorithm - the hash function used.</li> <li>Last scanned - the date and time of the most recent scan where the object was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> <li>Severity - when applicable, indicates the degree of non-compliance with policy as determined by the rule used for analysis. </li> </ul> <p>Use the local filters at the top of the table to filter by certificate Subject or Issuer. You may also apply one or more global filters at the top of the screen.</p>"},{"location":"inventory/#certificate-details","title":"Certificate details","text":"<p>To view more detailed information about a specific certificate, select Details. The details panel provides additional insights, including associated:</p> <ul> <li>Locations - where the item was discovered. </li> <li>Issues - generated when a rule flags the item. </li> <li>Keys - a sequence of bits used as input to a cryptographic algorithm to encrypt, decrypt, sign, or authenticate data.</li> <li>Operations - a specific process or function, such as encryption, decryption, signing, or verification, that is performed on data using a key.</li> <li>Handshakes - processes to establish a secure connection between networked applications.</li> <li>Hosts - a physical or virtualized computing system, such as a server, workstation, or virtual machine, that serves as the operating environment for applications and container images. </li> <li>Apps - a software program or service running on a host.</li> <li>Container images - a packaged software component that was scanned, containing an application\u2019s filesystem and configuration.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>To access a JSON object containing all certificate metadata, scroll to the bottom of the panel and select View full certificate.</p>"},{"location":"inventory/#operations","title":"Operations","text":"<p>The Operations tab provides a comprehensive view of the cryptographic operations discovered during analysis, including the following information:</p> <ul> <li>Type - the cryptographic operation performed.</li> <li>Timestamp - the date and time of the operation (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC. </li> <li>Algorithms - the algorithm used, such as AES, RSA, HMAC, etc.</li> <li>Last scanned - the date and time of the most recent scan where the object was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> <li>Severity - when applicable, indicates the degree of non-compliance with policy as determined by the rule used for analysis. </li> </ul> <p>Note</p> <p>If an operation involves multiple calls, the timestamp of the operation reflects the final call.</p> <p>Use the local filters at the top of the table to filter by operation Type. You may also apply one or more global filters at the top of the screen.</p>"},{"location":"inventory/#operations-details","title":"Operations details","text":"<p>To view more detailed information about a specific operation, select Details. The details panel provides additional insights, including associated:</p> <ul> <li>Stack trace - the sequence of function calls at the point the item was detected.</li> <li>Issues - generated when a rule flags the item. </li> <li>Keys - a sequence of bits used as input to a cryptographic algorithm to encrypt, decrypt, sign, or authenticate data.</li> <li>Certificates - a digitally signed document, linking a public key to an identity.</li> <li>Hosts - a physical or virtualized computing system, such as a server, workstation, or virtual machine, that serves as the operating environment for applications and container images. </li> <li>Apps - a software program or service running on a host.</li> <li>Container images - a packaged software component that was scanned, containing an application\u2019s filesystem and configuration.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>To access a JSON object containing all operations metadata, scroll to the bottom of the panel and select View full operation.</p>"},{"location":"inventory/#handshakes","title":"Handshakes","text":"<p>The Handshakes tab provides details about the negotiation of cryptographic parameters during data transmission:</p> <ul> <li>Source IP - the IP address of the host that initiated the connection.</li> <li>Target IP - the IP address of the destination host. </li> <li>TLS Version - the version of Transport Layer Security used.</li> <li>Selected ciphersuite - the ciphersuite selected for use in this connection.</li> <li>Min. client TLS Version - the earliest version of TLS that the client will accept.</li> <li>Status - whether the handshake was performed successfully or not.</li> <li>Target port - the port number used by the target host to receive the connection.</li> <li>Last scanned - the date and time of the most recent scan where the object was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> <li>Severity - when applicable, indicates the degree of non-compliance with policy as determined by the rule used for analysis. </li> </ul> <p>You may apply one or more global filters at the top of the screen.</p>"},{"location":"inventory/#handshake-details","title":"Handshake details","text":"<p>To view more detailed information about a specific handshake, select Details. The details panel provides additional insights, including associated:</p> <ul> <li>Client supported ciphersuites - the ciphersuites the client indicated it could support during the scanned handshake.</li> <li>Client supported key exchange algorithms - the key exchange protocols the client indicated it could support during the scanned handshake.</li> <li>Client supported server authentication algorithms - the authentication protocols the client indicated it could support during the scanned handshake.</li> <li>Client supported versions - the security protocol versions, such as TLS 1.2 or TLS 1.3, the client indicated it could support during the scanned handshake.</li> <li>Issues - generated when a rule flags the item.</li> <li>Certificates - a digitally signed document, linking a public key to an identity.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>Note</p> <p>An \u2018UNKNOWN\u2019 value in a handshake indicates AQG was unable to translate a numerical identifier (NID) due to missing data, a non-standard NID, or a recently introduced NID.</p> <p>To access a JSON object containing all handshake metadata, scroll to the bottom of the panel and select View full handshake.</p>"},{"location":"inventory/#secrets","title":"Secrets","text":"<p>The Secrets tab displays an inventory of non-standard or user-defined objects retrieved from an external data source.</p> <ul> <li>Type - the secret type.</li> <li>Last scanned - the date and time of the most recent scan where the object was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> <li>Severity - when applicable, indicates the degree of non-compliance with policy as determined by the rule used for analysis. </li> </ul> <p>You may apply one or more global filters at the top of the screen.</p>"},{"location":"inventory/#secrets-details","title":"Secrets details","text":"<p>To view more detailed information about a specific operation, select Details. The details panel provides additional insights, including associated:</p> <ul> <li>Locations - where the item was discovered. </li> <li>Issues - generated when a rule flags the item. </li> <li>Hosts - a physical or virtualized computing system, such as a server, workstation, or virtual machine, that serves as the operating environment for applications and container images. </li> <li>Apps - a software program or service running on a host.</li> <li>Container images - a packaged software component that was scanned, containing an application\u2019s filesystem and configuration.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>To access a JSON object containing all secrets metadata, scroll to the bottom of the panel and select View full secret.</p>"},{"location":"inventory/exclusions/","title":"Exclusions","text":"<p>The Exclusions feature in AQtive Guard allows you to prevent specific cryptographic objects from generating issues at the organization level. This helps reduce informational noise from findings your organization can\u2019t readily act on, allowing your team to focus their efforts on the most relevant cryptographic risks.</p>"},{"location":"inventory/exclusions/#when-to-use-exclusions","title":"When to use exclusions","text":"<p>Consider using exclusions when discovered cryptographic objects fall outside of your organization\u2019s direct control or immediate remediation scope. Appropriate use cases can include:</p> <ul> <li>Third-party certificates embedded within hardware appliances.</li> <li>Cryptography associated with applications your team can\u2019t modify or update.</li> <li>Objects related to systems managed by different departments or teams.</li> <li>Components within legacy systems awaiting deprecation.</li> </ul>"},{"location":"inventory/exclusions/#how-it-works","title":"How it works","text":"<p>Excluding an object impacts its visibility within AQtive Guard in the following ways:</p> <ul> <li>Inventory: The excluded object remains visible within your AQtive Guard inventory list. It is not deleted.</li> <li>Issues: AQtive Guard will stop generating or displaying any issues associated with the excluded object. These issues will be hidden from dashboards, reports, and issue lists across your entire organization.</li> </ul>"},{"location":"inventory/exclusions/#considerations","title":"Considerations","text":"<p>To minimize potential security risks, use the Exclusions feature only to reduce noise from genuinely non-actionable items. Never use exclusions to suppress findings for vulnerabilities that your team is responsible for remediating.</p> <p>If you need to narrow your focus temporarily without hiding issues from organization-wide dashboards or reports (such as when investigating an issue), use AQtive Guard\u2019s filtering capabilities instead of exclusions.</p> <p>Caution</p> <p>Exercise extreme care when excluding cryptographic objects. Misuse of this feature can hide critical vulnerabilities, creating an inaccurate picture of your true security posture and potentially masking significant risks from oversight. </p>"},{"location":"inventory/exclusions/#exclude-an-individual-cryptographic-object","title":"Exclude an individual cryptographic object","text":"<p>Follow these steps to exclude a specific cryptographic object from generating issues:</p> <ol> <li>Navigate to Inventory from the main menu.</li> <li>Identify the target cryptographic item using filters or by browsing the inventory list. </li> <li>Select Details at the end of the item row to review its properties, scan history, issues, and contextual information, such as associated hosts or locations. </li> <li>After you\u2019ve validated that the item is safe to exclude, select Exclude item at the top right of the details panel. </li> <li>To complete the process, select Exclude item in the Warning dialogue to remove the item from any dashboards, reports, and exports for your entire organization. </li> <li>Return to the Inventory table and locate the item row. The severity column will now indicate the item has been Excluded. </li> </ol>"},{"location":"inventory/exclusions/#remove-an-exclusion","title":"Remove an exclusion","text":"<p>If an object was previously excluded in error or if it becomes relevant again, follow these steps to remove the exclusion.</p> <ol> <li>Navigate to Inventory from the main menu.</li> <li>Identify the excluded cryptographic item using filters or by browsing the inventory list. </li> <li>Select Details at the end of the item row to open its details panel. </li> <li>Select Remove exclusion at the top right of the details panel. </li> <li>To complete the process, select Remove exclusion in the Warning dialogue to reinstate the item in dashboards and exports for your entire organization. </li> <li>Return to the Inventory table and locate the item row. The severity column will now indicate the highest severity level of any issue(s) associated with the item. </li> </ol>"},{"location":"issues/","title":"Issues","text":"<p>When AQtive Guard analyzes data, it evaluates detected cryptographic objects against the currently active rules. If an object fails to meet the defined rule parameters, an issue is generated.</p> <p>Issues alert you to potential security risks or compliance violations, enabling you to take prompt action to address deviations from your organization\u2019s cryptographic standards and ensure the continued security and integrity of your data.</p>"},{"location":"issues/#view-issues","title":"View issues","text":"<p>Select Issues from the main menu to view a full list of issues discovered during analysis. The columns organize issues by:</p> <ul> <li>Rule - The out-of-policy rule that was flagged in analysis.</li> <li> <p>Severity - The severity of the issue, as determined by parameters set in the associated rule. Issue severity is categorized as:</p> <p></p> </li> </ul> <ul> <li>Occurrences - Indicates the number of cryptographic objects impacted by the issue.</li> <li>Details - Opens AQG Knowledge Base information about the rule that triggered the issue. Refer to Knowledge base for details. </li> </ul>"},{"location":"issues/knowledge-base/","title":"Knowledge base","text":"<p>When reviewing issues in AQtive Guard, you can access several resources to help you understand and address the problem. </p> <p></p> <p>Select Details at the end of an issue row to open AQG Knowledge Base information about the rule that triggered the issue: </p> <ol> <li>Rule summary - A brief description of the violated rule, along with high-level recommendations for remediation.</li> <li>How to resolve - Guidance on how to resolve the issue. </li> <li>Risk factors - An overview of the potential risks associated with the issue, enabling you to assess your exposure and prioritize remediation efforts.</li> <li>References -  Links to in-depth articles that offer up-to-date cryptographic guidance and authoritative resources.</li> <li>Specifications - Specifications for the rule pass/fail policy and severity criteria.</li> </ol> <p>Tip</p> <p>The AQtive Guard knowledge base is a valuable resource for report writing, recommendation guidance, or issue logging. You can browse and search all rules, reference articles, and the glossary at any time by selecting User guide in the menu panel.</p>"},{"location":"it-assets/","title":"IT assets","text":"<p>Select IT Assets from the main menu to view details about the scanned hosts, images, and applications that contribute to your cryptography inventory.  </p> <p>The tabs organize your scanned IT assets by:</p> <ul> <li>Hosts - a physical or virtualized computing system, such as a server, workstation, or virtual machine, that serves as the operating environment for applications and container images. </li> <li>Apps - a software program or service running on a host.</li> <li>Container images - a packaged software component that was scanned, containing an application\u2019s filesystem and configuration.</li> </ul> <p></p> <p>The following sections provide details for each tab.</p>"},{"location":"it-assets/#hosts","title":"Hosts","text":"<p>The Hosts tab provides an overview of your scanned laptops and servers, including:</p> <ul> <li>Name - the default or user-assigned name of the scanned host.  </li> <li>OS - Operating System (Windows, Cygwin or Unix)  </li> <li>Last scanned - the date and time of the most recent scan where the item was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> </ul> <p>Use the local filters at the top of the table to filter by host OS. You may also apply one or more global filters at the top of the screen.</p>"},{"location":"it-assets/#host-details","title":"Host details","text":"<p>Selecting Details provides an in-depth view of the cryptography discovered on the associated host, including:</p> <ul> <li>Keys - a sequence of bits used as input to a cryptographic algorithm to encrypt, decrypt, sign, or authenticate data.</li> <li>Certificates  - a digitally signed document, linking a public key to an identity.</li> <li>Operations - a specific process or function, such as encryption, decryption, signing, or verification, that is performed on data using a key.</li> <li>Secrets  - secrets are digital credentials providing identity authentication and authorizing access to privileged accounts, applications, and services.</li> <li>Apps  - scanned software applications found on host.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>Within the details panel, you can also select View full host to access a JSON object containing all host metadata. </p>"},{"location":"it-assets/#apps","title":"Apps","text":"<p>The Apps tab presents information about scanned applications, including:</p> <ul> <li>Name - the application name, as provided by the data source. For application data uploaded from CBOM, this is the user-provided name of the application or codebase that this CBOM data belongs to, typically referencing the root name of the codebase.</li> <li>Language - the programming language used for the application, as provided by the CBOM or the tool used to scan the application.   </li> <li>Last scanned - the date and time of the most recent scan where the item was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC.</li> <li>Data sources - the integration or sensor that provided the data for the item.</li> </ul> <p>You may apply one or more global filters at the top of the screen.</p>"},{"location":"it-assets/#app-details","title":"App details","text":"<p>Selecting Details provides an in-depth view of the cryptography discovered in the application, including:</p> <ul> <li>Keys - a sequence of bits used as input to a cryptographic algorithm to encrypt, decrypt, sign, or authenticate data.</li> <li>Certificates  - a digitally signed document, linking a public key to an identity.</li> <li>Operations - a specific process or function, such as encryption, decryption, signing, or verification, that is performed on data using a key.</li> <li>Secrets  - container secrets are digital credentials providing identity authentication and authorizing access to privileged accounts, applications, and services.</li> <li>Hosts  - scanned laptops and servers where this app was detected.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>Within the details panel, you can also select View full application to access a JSON object containing all application metadata. </p>"},{"location":"it-assets/#container-images","title":"Container images","text":"<p>The Container images tab displays the following information for scanned container images:</p> <ul> <li>Image ID - the name assigned to the image at creation, as supplied by the data source.  </li> <li>Repository - the storage location of the container image, such as a Git repository.  </li> <li>Layer SHA - the hash or digest for the scanned container layer.   </li> <li>Last scanned - the date and time of the most recent scan where the item was found (<code>MM/DD/YYYY HH:MM:SS AM/PM</code>), based on the time zone set in your browser, with system time in UTC. </li> <li>Data sources - the integration or sensor that provided the data for the item.</li> </ul> <p>You may also apply one or more global filters at the top of the screen.</p>"},{"location":"it-assets/#container-image-details","title":"Container image details","text":"<p>Selecting Details provides an in-depth view of the cryptography discovered in the associated image, including:</p> <ul> <li>Keys - a sequence of bits used as input to a cryptographic algorithm to encrypt, decrypt, sign, or authenticate data.</li> <li>Certificates  - a digitally signed document, linking a public key to an identity.</li> <li>Operations - a specific process or function, such as encryption, decryption, signing, or verification, that is performed on data using a key.</li> <li>Secrets  - container secrets are digital credentials providing identity authentication and authorizing access to privileged accounts, applications, and services.</li> <li>Sessions - the date, time, and data source(s) for a specific item that was ingested or detected, providing a detailed breakdown of its Last scanned history.</li> </ul> <p>Within the details panel, you can also select View full container to access a JSON object containing all container metadata. </p>"},{"location":"kb-articles/","title":"Knowledge base","text":"<p>This AQtive Guard Knowledge Base provides in-depth information on cryptographic concepts, protocols, and standards.  Authored by the expert SandboxAQ Cryptography R&amp;D team, these articles provide authoritative guidance and reference links to industry-recognized standards to support you in working with the cryptography discovered by the AQtive Guard platform.  Refer to the guide menu for the growing list of articles. </p> <p>Explore this library of articles to deepen your understanding of encryption methods, cryptographic protocols, and security standards, and get the most out of AQtive Guard discovery and analysis.</p>"},{"location":"kb-articles/block-and-stream-ciphers/","title":"Block and stream ciphers","text":"<p>Symmetric encryption uses two main types of algorithms: block ciphers and stream ciphers. Although both are used for symmetric encryption, they operate differently. A block cipher maps a fixed length data string\u2014a block\u2014to a string of the same length, such as the 128-bit blocks in AES, using a secret key. A stream cipher, in contrast, gradually encrypts data often byte-by-byte.</p> <p>To turn a block cipher into an encryption algorithm, it must use a mode of operation.<sup>1</sup> A mode of operation specifies how a block cipher encrypts and decrypts data. Common modes include GCM, CCM, and CBC. For example, AES256-GCM uses the AES block cipher with 256-bit secret keys in GCM mode. Each mode has distinct security properties, trade-offs, and usage requirements. For instance, GCM is insecure if initialization vectors (IVs) are ever repeated. Since stream ciphers are already encryption algorithms, no mode of operation is required.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#introduction-to-symmetric-encryption","title":"Introduction to symmetric encryption","text":"<p>Symmetric encryption, which uses the same key for both encryption and decryption, relies on two main types of categories: block ciphers and stream ciphers.</p> <p>Block ciphers operate on fixed-size data blocks and require a mode of operation to encrypt  data. The mode impacts properties like security guarantees, efficiency, and ability to process data in parallel. Stream ciphers, on the other hand, encrypt data continuously, making them ideal for real-time applications where minimal latency and flexibility are essential.</p> <p>The Advanced Encryption Standard (AES), defined in FIPS 197, is the most common block cipher and is widely deployed across various applications, with most modern devices supporting hardware acceleration. In the context of lightweight cryptography, recent efforts have explored alternatives optimized for resource-constrained environments. Notably, NIST SP 800-232 (IPD) proposes a series of constructions based on Ascon, a family of lightweight cryptographic primitives designed for efficiency, security, and adaptability in constrained settings, with authenticated encryption as a key use case.</p> <p>Now that we\u2019ve outlined the key characteristics of block and stream ciphers, we\u2019ll examine block ciphers  in detail, followed by stream ciphers.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#block-ciphers","title":"Block ciphers","text":"<p>Block ciphers transform fixed-size data units known as blocks. The input and output blocks have the same length, or block size. For example, AES, as defined in FIPS 197, uses 128-bit blocks, while the obsolete DES (Data Encryption Standard), previously specified in FIPS 46-3 (withdrawn), used  64-bit blocks.</p> <p>The block size is one of many factors that affect security. In general, larger block sizes provide stronger security by reducing attack risks. A 128-bit block size is currently considered sufficient for modern encryption.</p> <p>Block ciphers require a secret key to operate, and can support different key lengths to provide varying levels of security. Key length is not necessarily related to the block size. AES, for example, uses 128-bit blocks, but supports 128-, 192-, and 256-bit secret keys but always operates on 128-bit blocks. In AES, the key length determines the number of encryption rounds: </p> <ul> <li>AES-128 has 10 rounds  </li> <li>AES-192 has 12 rounds  </li> <li>AES-256 has 14 rounds</li> </ul> <p>A round refers to a series of cryptographic transformations applied to the input data (at the first round, this data would be the block cipher\u2019s input) to progressively convert it into the output, as defined by the cipher\u2019s design. More rounds (which basically repeat the same round) generally improve resistance to attacks.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#block-cipher-modes-of-operation","title":"Block cipher modes of operation","text":"<p>A block cipher mode of operation, often referred to as a mode, turns a block cipher into an actual encryption scheme that handles variable-length data and, critically, provides security assurances expected from an encryption scheme. However, modes vary in security, and choosing the appropriate mode for the application is crucial. For example, the Electronic Code Book (ECB) mode, which simply uses the block cipher to encrypt data in chunks of length of the block size, is generally considered insecure.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#overview-of-existing-modes","title":"Overview of existing modes","text":"<p>Table 1 summarizes the modes specified in NIST SP 800-38, which spans eight parts (SP 800-38A to SP 800-38G). </p> <p>Each entry includes:</p> <ol> <li>The mode\u2019s acronym.  </li> <li>The mode\u2019s full name.  </li> <li>Whether it supports encryption and/or authentication.  </li> <li>The required additional input such as Initialization Vector (IV), nonce, or tweak.  </li> <li>Padding requirements.  </li> <li>A brief description.  </li> <li>A reference to the corresponding publication. </li> </ol> <p>Note</p> <p>While the Publication column highlights the main section, other sections may also contain relevant information.</p> Mode Mode Full Name Enc Auth Additional Input Padding Short Description Publication ECB Electronic Codebook \u2716 \u2716 - \u2714 This mode does not provide sufficient security except for in rare circumstances. 800-38A  \u00a7 6.1 CBC Cipher Block Chaining Weak \u2716 IV - Unpredictable No reuse \u2714 General purpose encryption without authentication. Can only be used for general purposes when composed generically with authentication. 800-38A \u00a7 6.2 / Appendix C CFB Cipher Feedback Weak \u2716 IV -\u000bUnpredictable No reuse \u2716 Stream-like encryption, self-synchronizing. Can only be used for general purposes when composed generically with authentication. 800-38A \u00a7 6.3 /\u000bAppendix C OFB Output Feedback Weak \u2716 IV - Unique/ No reuse \u2716 Stream-like encryption, does not propagate bit errors. Can only be used for general purposes when composed generically with authentication. 800-38A \u00a7 6.4 /\u000bAppendix C CTR Counter Weak \u2716 Nonce - Unique/ No reuse \u2716 Fast encryption, parallelizable.  Can only be used for general purposes when composed generically with authentication. 800-38A \u00a7 6.5 / Appendix B CMAC Cipher-based Message Authentication Code \u2716 \u2714 - \u2716 Message authentication code. 800-38B \u00a7 6 CCM Counter with Cipher Block Chaining-Message Authentication Code \u2714 \u2714 Nonce - Unique/ No reuse \u2714 Authenticated encryption for constrained environments. 800-38C \u00a7 6 GCM Galois/Counter Mode \u2714 \u2714 IV<sup>2</sup> - Unique/ No reuse \u2716 Authenticated encryption. 800-38D \u00a7 7 XTS- AES XEX-based Tweaked CodeBook Mode with CipherText Stealing using AES Weak \u2716 Tweak<sup>3</sup> \u2716 Disk Encryption.  Can only be used for general purposes when composed generically with authentication. 800-38E<sup>4</sup> AES- KW AES Key Wrap \u2714 \u2714 - \u2716 Encrypting cryptographic keys. 800-38F  \u00a7 6.2 AES- KWP AES Key Wrap With Padding \u2714 \u2714 - \u2714 Encrypting cryptographic keys with arbitrary length. 800-38F  \u00a7 6.3 FF1 / FF3 Format Preserving Encryption \u2714 \u2716 Tweak<sup>5</sup> \u2716 Encrypting structured data while preserving the format. 800-38G  \u00a7 5 / Appendix C <p>Table 1. Listing of the block cipher modes of operation specified in NIST 800-38 series. </p> <p>Different modes combine the block cipher in different ways, balancing security, efficiency, and functional properties. For example, Cipher Block Chaining (CBC) chains ciphertext blocks, requiring sequential processing. Counter (CTR) mode, on the other hand, generates a keystream from a nonce and counter, enabling parallelism and precomputation.</p> <p>Some modes, including CBC, require adding extra bytes, also known as padding, to align the input data with the block size. One common padding scheme is PKCS#7 (refer to RFC 5652). However, padding can introduce vulnerabilities, such as \u201cpadding oracle\u201d attacks on CBC.</p> <p>For general applications, authenticated encryption modes, such as GCM, are recommended. These modes provide both confidentiality and integrity, protecting data from eavesdropping and unauthorized modifications. Preventing unauthorized modification also defends against attacks that try to break confidentiality by modifying ciphertexts during transmission. </p> <p>Using encryption without authentication,  such as CBC without a message authentication code, can leave data vulnerable to devastating attacks, like the padding-related vulnerabilities mentioned above. Therefore, authenticated encryption modes should be used unless separate authentication mechanisms are specifically required.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#ivs-nonces-tweaks-and-vulnerabilities","title":"IVs, nonces, tweaks, and vulnerabilities","text":"<p>Symmetric encryption requires additional inputs, such as initialization vectors (IVs), nonces, or tweaks to ensure security. These inputs prevent identical plaintexts from producing identical ciphertexts. Their exact size varies depending on the encryption mode, but they\u2019re typically around 128 bits for AES. For instance, AES-GCM IVs are commonly 96 bits, while other AES-based modes such as CBC typically use 128-bit IVs. </p> <p>To ensure security in some cases, these inputs must be unpredictable (for example, not deterministically guessable by an attacker)<sup>6</sup> and/or unique (which means they must never be reused with the same secret key). In general, IVs, nonces, or tweaks are publicly transmitted or stored alongside ciphertexts, as their secrecy isn\u2019t required for security.</p> <p>Improper usage however can lead to critical security vulnerabilities. Some important highlights are:</p> <ul> <li>AES-GCM IVs must never be reused with the same key, as doing so would allow attackers to recover authentication keys and forge ciphertexts.  </li> <li>In AES-CTR and AES-OFB, IV reuse is a critical vulnerability because it causes keystream reuse, allowing attackers to recover the relation (such as the XOR) between two plaintexts by relating (XORing) their ciphertexts.  </li> <li>In AES-CBC, IV reuse makes ciphertexts more susceptible to cryptanalysis and inference attacks by revealing patterns across messages.</li> </ul> <p>This discussion emphasizes the need for careful consideration when making decisions about IVs, nonces, or tweaks. The relevant documentation should always be consulted to ensure compliance with requirements. For example:</p> <ul> <li>NIST SP 800-38A provides guidance on IV generation in Appendix C, \u201cGeneration of Initialization Vectors\u201d. It also includes considerations applicable to nonces for the CTR mode in Appendix B, \u201cAppendix B: Generation of Counter Blocks\u201d.   </li> <li>NIST SP 800-38D discusses GCM IV constructions in section 8.2. Additionally, section 8.3 highlights an important point: \u201cThe total number of invocations of the authenticated encryption function shall not exceed 232** \u201d when non-96-bit deterministic IVs or random IVs are used.   </li> <li>NIST SP 800-38G discusses tweaks in Appendix C, \u201cTweaks\u201d, in the context of format-preserving encryption. It also provides useful considerations in Appendix A, \u201cParameter Choices and Security\u201d.</li> </ul>"},{"location":"kb-articles/block-and-stream-ciphers/#stream-ciphers","title":"Stream ciphers","text":"<p>Stream ciphers encrypt data by processing it as a continuous stream of bits or bytes, unlike block ciphers which operate on fixed-size blocks. They generate a keystream\u2014a pseudorandom sequence of bits that is XORed with the plaintext to produce ciphertext.<sup>7</sup> This approach allows for high-speed encryption, making stream ciphers well-suited for real-time communications, such as secure network protocols, voice and video transmissions, and embedded systems.</p> <p>One of the main advantages of stream ciphers is their efficiency. Since they don\u2019t require buffering an entire block before encryption, they can operate with low latency and minimal memory usage. This makes them ideal for environments where computational resources are limited, such as IoT devices and wireless communications. </p> <p>However, stream ciphers also have disadvantages\u2014the most critical being that they require a truly unique keystream for each encryption session. Reusing the same keystream for multiple messages results in loss of confidentiality because attackers can recover non-trivial relations between plaintexts by XORing the corresponding ciphertexts encrypted with the same keystream. Also, some older stream ciphers, like RC4, have known weaknesses, making them unsuitable for modern applications.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#notable-stream-ciphers","title":"Notable stream ciphers","text":"<p>ChaCha20 is a modern stream cipher that improves upon the Salsa20 family. It uses a simple but highly efficient structure based on add-rotate-xor (ARX) operations, making it resistant to cryptanalysis while performing well on a variety of hardware architectures. Unlike RC4, ChaCha20 is secure against known attacks and has been standardized in RFC 8439 for use in TLS and other security protocols. However, ChaCha20 requires an additional authentication mechanism, such as Poly1305.</p> <p>RC4 (Rivest Cipher 4) was one of the most widely used stream ciphers, employed in protocols like SSL/TLS, WEP, and WPA. Designed by Ron Rivest in 1987, RC4 generates a keystream using a permutation-based approach. Although initially considered secure, vulnerabilities were later discovered, including output biases that allowed plaintext recovery under certain conditions. These weaknesses led to RC4\u2019s deprecation for use in cryptographic applications.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#other-stream-ciphers","title":"Other stream ciphers","text":"<p>In addition to RC4 and ChaCha20, several other stream ciphers have been designed for specific security and performance needs. The eSTREAM portfolio, which was developed as part of the ECRYPT project, features ciphers like HC-128, Rabbit, and Grain, which were designed to provide strong security with efficient performance on resource-constrained devices.</p>"},{"location":"kb-articles/block-and-stream-ciphers/#sources","title":"Sources","text":"<ul> <li>NIST SP 800-38A: Recommendation for Block Cipher Modes of Operation: Methods and Techniques</li> <li>NIST SP 800-38B: Recommendation for Block Cipher Modes of Operation: the CMAC Mode for Authentication</li> <li>NIST SP 800-38C: Recommendation for Block Cipher Modes of Operation: the CCM Mode for Authentication and Confidentiality</li> <li>NIST SP 800-38D: Recommendation for Block Cipher Modes of Operation: Galois/Counter Mode (GCM) and GMAC</li> <li>NIST SP 800-38E: Recommendation for Block Cipher Modes of Operation: the XTS-AES Mode for Confidentiality on Storage Devices</li> <li>NIST SP 800-38F: Recommendation for Block Cipher Modes of Operation: Methods for Key Wrapping</li> <li>NIST SP 800-38G: Recommendation for Block Cipher Modes of Operation: Methods for Format-Preserving Encryption</li> <li>FIPS 197: Advanced Encryption Standard (AES)</li> <li>FIPS 46-3 (Withdrawn on May 19, 2005) Data Encryption Standard (DES)</li> <li>1619-2007 - IEEE Standard for Cryptographic Protection of Data on Block-Oriented Storage Devices</li> <li>RFC 5652 - Cryptographic Message Syntax (CMS)</li> <li>RFC 8439 - ChaCha20 and Poly1305 for IETF Protocols</li> <li>Knowledge base: Message Authentication Codes (MACs)</li> <li>Knowledge base: Chosen ciphertext attacks</li> </ul> <ol> <li> <p>The term \u201cblock cipher\u201d is used for historical reasons, from when block ciphers were treated as encryption algorithms. Modern treatments speak instead of \u201cpseudorandom permutations\u201d (PRPs)\u2014this communicates the purpose of these objects directly.\u00a0\u21a9</p> </li> <li> <p>IV is used in the specification from 800-38D, but section 5.2.1.1 states: \u201cThe IV is essentially a nonce (\u2026)\u201d.\u00a0\u21a9</p> </li> <li> <p>The tweak is derived from the data\u2019s logical position, ensuring uniqueness per disk sector.\u00a0\u21a9</p> </li> <li> <p>Relies on the specification provided in 1619-2007 - IEEE Standard for Cryptographic Protection of Data on Block-Oriented Storage Devices \u21a9</p> </li> <li> <p>Here the tweak is a user-specified value that separates encrypted values, preventing identical plaintexts from producing the same ciphertext.\u00a0\u21a9</p> </li> <li> <p>In practice, such unpredictable inputs are generated with cryptographically secure randomness.\u00a0\u21a9</p> </li> <li> <p>Note that CTR mode effectively turns a block cipher into a stream cipher, highlighting that a block cipher is a building block for encryption, but a stream cipher itself is an encryption scheme.\u00a0\u21a9</p> </li> </ol>"},{"location":"kb-articles/certificates/","title":"Certificates","text":"<p>Failure to properly manage and validate certificates risks data theft, service outages or impersonation attacks.  Certificates are the Achilles\u2019 heel of public-key cryptography, as they bind an identity (for example, a domain name, software provider, or user credential) to a cryptographic key.  Without this binding, no security guarantees can be given. Effective certificate management is essential and involves:</p> <ul> <li>Securely storing all signing keys.</li> <li>Regularly renewing organizational certificates before they expire.</li> <li>Carefully validating third-party certificates.</li> </ul> <p>Neglecting these practices compromises the security of cryptographic systems.</p>"},{"location":"kb-articles/certificates/#the-role-of-certificates","title":"The role of certificates","text":"<p>Consider two of the most fundamental security goals in computing:</p> <ol> <li> <p>Confidentiality: Public-key cryptography is commonly used to establish secure communication  between parties who have not previously shared a confidential channel.     For example, TLS relies on this protocol whenever a new connection is initiated.     To ensure security, the sender must verify that the public key they are using to encrypt messages belongs to the intended recipient.    Failure to do so can result in an attacker-in-the-middle attack<sup>1</sup> that compromises the communication. </p> </li> <li> <p>Authenticity: Digital signatures are widely used to verify the authenticity of documents, messages or software.     To be able to validate a signature, the receiver must confirm that the signer is who they claim to be.</p> </li> </ol> <p>In both cases, certificates are the standard method for ensuring authenticity.  A certificate is a digital signature that binds cryptographic key material (such as encryption or verification keys) to some identity information.  For example, a web certificate binds a verification key to a domain name or organization.  Certificates are issued by a trusted entity known as a Certification Authority (CA). </p>"},{"location":"kb-articles/certificates/#public-key-infrastructure-pki","title":"Public Key Infrastructure (PKI)","text":"<p>Digital signature schemes enable the holder of a private signing key to create signatures that can be verified using a corresponding verification key.  This implies, however, that certificates introduce a recursive challenge: they use signatures to validate key material, but verifying these signatures requires confidence in the origin of the verification key to verify the certificate itself.</p> <p>This challenge is addressed through Public Key Infrastructure (PKI).  PKI relies on a hierarchy of trust, starting with root certificates issued by trusted root Certification Authorities (CAs).  These root certificates, distributed with major software (such as web browsers), serve as the foundation of trust.  Root CAs sign the public verification keys of intermediate CAs, which can further sign other intermediate keys or end-entity certificates, such as those used for websites or applications.</p> <p>A PKI is essential for ensuring both confidentiality and authentication, whether on the global internet or within corporate networks.  By establishing a trusted chain of certificates, a PKI enables secure communication and identity verification.</p>"},{"location":"kb-articles/certificates/#potential-vulnerabilities","title":"Potential vulnerabilities","text":"<p>Failures in certificate management and validation can introduce several vulnerabilities, including the following:</p> <ul> <li>Compromised signing keys. Secret signing keys must be carefully protected, as their compromise gives malicious actors significant power.      Certificates include expiration dates to limit the exposure window of a compromised key.      Historically, web server certificates had lifetimes measured in years, but shorter durations of 45\u201390 days are now standard.      For long-term use cases, such as root CA certificates or embedded devices, longer lifetimes remain necessary.     Regardless of duration, certificates nearing expiration should be replaced promptly, typically around 80% of their validity period.</li> </ul> <ul> <li>Rogue Certificate Authorities (CAs). Some Certificate Authorities (CAs) have acted maliciously or negligently, issuing certificates that enable malicious third parties to impersonate legitimate entities.     To mitigate this risk, certificate transparency logs are used.     These secure, append-only databases record information about issued certificates, making it possible to detect and verify potentially rogue certificates.</li> </ul> <ul> <li> <p>Weak digital signature algorithms. The security of certificates relies on robust digital signature algorithms. This includes:</p> <ul> <li>Avoiding key lengths that are too weak to resist known attacks. </li> <li>Using algorithms resistant to attacks by quantum-equipped adversaries when quantum readiness is a security goal. </li> </ul> </li> </ul> <ul> <li> <p>Hash Function Vulnerabilities. Digital signatures on certificates work by signing a digest (hash) of the data being authenticated, such as some key material and an identity.      Consequently, the security of hash functions directly impacts certificate security.     Key considerations include:</p> <ul> <li>Avoid weak hash functions that lack collision resistance, such as MD5 and SHA-1.</li> <li>Avoid truncated hash outputs. These allow \u201cbirthday-style\u201d attacks, where an attacker attempts to find two different inputs that produce the same hash. This is because shorter hashes allow for fewer possible values.</li> </ul> </li> </ul> <ul> <li>Self-Signed Certificates. Many technologies require public-key material provided in a certificate.      To meet this requirement, developers sometimes use a self-signed certificate\u2014a certificate which validates itself instead of being verified by a trusted CA.     However, self-signed certificates provide no authentication assurances and should not be used in production systems, such as public-facing websites, API endpoints, VPNs and remote access services, or code signing.     In these and similar scenarios, a self-signed certificate can disrupt functionality, introduce security vulnerabilities, or degrade user experience.     Instead, production environments should rely on certificates issued by trusted CAs.  </li> </ul> <ul> <li> <p>Expired or invalid certificates. The most common issues related to certificates include:</p> <ul> <li>Expired certificates that have not been replaced before their expiration date. That is, the day of verification is later than the end date of the validity period. </li> <li>Invalid Certificates, such as certificates where the domain name does not match the \u201cCommon Name (CN)\u201d field.</li> </ul> </li> </ul>"},{"location":"kb-articles/certificates/#sources","title":"Sources","text":"<ul> <li>sslmate. Timeline of Certificate Authority Failures, accessed 2024-10-25. </li> <li>CA/Browser Forum. Baseline Requirements for the Issuance and Management of Publicly\u2010Trusted TLS Server Certificates, November 8, 2024. </li> <li>CA/Browser Forum. Network and Certificate System Security Requirements, November 8, 2024.</li> </ul> <ol> <li> <p>Historically, this class of attacks was known as a Man-in-the-Middle (MITM) attack.      The term attacker in the middle is more common, although some prefer Machine-in-the-Middle or Mallory-in-the-Middle to retain the well-known acronym MITM.\u00a0\u21a9</p> </li> </ol>"},{"location":"kb-articles/chosen-ciphertext-attacks/","title":"Chosen ciphertext attacks","text":"<p>Chosen ciphertext attacks compromise encryption by manipulating ciphertexts and observing how systems respond to decryption attempts. Although initially considered a theoretical concern, these attacks have been used to breach real-world systems, including SSL/TLS and major web frameworks.</p> <p>An example of this type of attack is the padding oracle attack, which exploits how systems respond to padding errors during decryption. These attacks have been used to break PKCS#1v1.5 RSA encryption\u2014in this case the attack is often referred to as Bleichenbacher\u2019s attack\u2014and CBC-mode encryption in older SSL/TLS versions. </p> <p>To prevent chosen ciphertext attacks, use cryptographic schemes with IND-CCA<sup>1</sup> security. This means using OAEP<sup>2</sup> padding for RSA instead of PKCS#1v1.5, and adopting authenticated encryption modes like GCM instead of CBC. Minimizing information leakage, such as through padding error messages, is also important when implementing secure systems. IND-CCA security is the standard for modern encryption solutions, and any deviations require careful consideration. </p>"},{"location":"kb-articles/chosen-ciphertext-attacks/#general-introduction","title":"General introduction","text":"<p>Chosen ciphertext attacks are a broad class of cryptography attacks that aim to break the confidentiality provided by encryption schemes. These attacks involve an attacker manipulating a target ciphertext and querying decryptions of the resulting malformed ciphertexts such that the responses help the attacker obtain useful information about the target plaintext\u2014in the worst case, even allowing the attacker to recover the entire plaintext.</p> <p>Although the notion of chosen ciphertext attacks are, strictly speaking, an abstract attack model used by cryptographers to formally analyze cryptographic schemes, they\u2019re also relevant in practice. They were used to compromise TLS implementations, XML frameworks, Android KeyStore, and more.</p> <p>A key ingredient of chosen ciphertext attacks is a decryption oracle that is queried by the attacker on their manipulated ciphertexts. The real-world attacks mentioned earlier became possible because attackers could construct approximations of these oracles in practice by using methods such as malware exploits and padding oracles. The latter method is common in practical chosen ciphertext attacks and will be discussed in the next section.</p>"},{"location":"kb-articles/chosen-ciphertext-attacks/#chosen-ciphertext-attacks-in-practice-padding-oracle-attacks","title":"Chosen ciphertext attacks in practice: padding oracle attacks","text":"<p>Padding oracle attacks are a form of chosen-ciphertext attack that exploits how padding errors are handled during decryption. They serve as building blocks for many cryptographic attacks, including the BEAST attack on TLS, attacks on web frameworks, and attacks on PKCS#11 cryptographic hardware.</p> <p>At a high level, messages are padded with redundant data before encryption for various reasons, such as to ensure the message has the correct length for the underlying encryption function (as in block cipher modes such as CBC), or randomizing the message to prevent certain attacks (as in RSA encryption). During decryption, an additional check is typically performed on the decrypted plaintext to determine whether its padding is valid before the message is returned.</p> <p>In some cases, certain systems, such as servers, exhibit different behavior when they receive messages with valid padding compared to messages with invalid padding. For example, a protocol may return an error message if it receives a ciphertext with invalid padding, while proceeding with the rest of the protocol if the padding is valid. Attackers exploit these oracles that reveal padding validity\u2014hence the name, padding oracles\u2014to compromise the security of the system. </p> <p>It\u2019s important to note that padding oracles, strictly speaking, are weaker than decryption oracles\u2014as discussed in the context of chosen ciphertext attacks above\u2014since they do not return the entire decryption of an attacker\u2019s query, but only indicate whether the decrypted query contains valid padding. However, as we\u2019ll see in the following examples, padding oracles can be sufficient to completely break security in real-world scenarios.</p>"},{"location":"kb-articles/chosen-ciphertext-attacks/#attacks-on-pkcs1v15-rsa-encryption","title":"Attacks on PKCS#1v1.5 RSA encryption","text":"<p>This class of attacks against RSA encryption with PKCS#1v1.5 padding was first proposed by Daniel Bleichenbacher in 1998, and many attack variants and improvements have since been reported. TLS versions up to 1.2 include RSA PKCS#1v1.5 as a key-establishment method, and are therefore vulnerable to these attacks without additional countermeasures.</p> <p>Specifically, PKCS#1v1.5 RSA encryption is used to encrypt a premaster secret. This serves as an input to derive the final session key and is then transmitted from the client to the server.</p> <p>Bleichenbacher-style attacks require access to a padding oracle, which allows an attacker to determine whether a particular ciphertext is accepted as a valid PKCS#1v1.5-padded plaintext after being decrypted. An attacker can then manipulate a target ciphertext in different ways and query this padding oracle to see if it\u2019s accepted or not. From this pattern of acceptance and rejection, the attacker can recover the entire plaintext. </p> <p>Returning to the TLS example above, an attacker can iteratively decrypt the premaster secret, which is used to derive the session key if the server\u2019s decryption behavior reveals padding errors.</p> <p>Such attacks are generally efficient. For example, for an implementation that strictly follows PKCS#1v1.5 padding rules, the attack typically requires around 15,000 queries to the padding oracle.</p>"},{"location":"kb-articles/chosen-ciphertext-attacks/#attacks-on-cbc-mode-encryption","title":"Attacks on CBC-mode encryption","text":"<p>This category of padding oracle attacks against the CBC mode of operation was first proposed by Serge Vaudenay in 2002, and subsequent variants have since led to real-world security vulnerabilities, including the Lucky Thirteen attack against CBC in TLS (up to version 1.2) and the POODLE attack, which exploited CBC weaknesses in SSL v3.0.</p> <p>CBC-mode encrypts data in chunks of b bits, where b is typically 128\u2014the block size of AES, the most widely used block cipher. Messages, however, do not necessarily have a length that is a multiple of b bits. Thus, padding schemes are employed to extend the message to the required length. One such padding scheme is PKCS#7. </p> <p>The attack exploits:</p> <ul> <li>the absence of guarantees for the integrity of ciphertexts.</li> <li>the fact that PKCS#7 padding can be valid or invalid. </li> </ul> <p>Put simply, ensuring ciphertext integrity prevents an attacker from manipulating ciphertexts in a way that produces meaningful responses. The lack of these guarantees is the root vulnerability, while the presence of a padding oracle\u2014which reveals whether padding is valid or invalid\u2014is the means by which this vulnerability can be efficiently exploited.</p> <p>The attack relies on querying such a padding oracle. Specifically, the attacker proceeds by flipping bits in the ciphertext and observing whether this produces a valid or invalid padding. The attacker can then infer the plaintext from the pattern of valid or invalid padding responses.</p> <p>The attack is efficient. The required number of padding oracle queries is linear in the number of bits in the target message. Extracting each byte of plaintext typically requires around b oracle queries. </p>"},{"location":"kb-articles/chosen-ciphertext-attacks/#security-recommendations","title":"Security recommendations","text":"<p>Use cryptographic schemes that are proven to offer security against chosen-ciphertext attacks, that is, use schemes which achieve IND-CCA security. </p> <p>For example, Bleichenbacher-style attacks can be mitigated against RSA encryption by switching from PKCS#1 v1.5 padding to OAEP (PKCS#1 v2.0) padding. This has been shown to offer IND-CCA security in an idealised model known as the random oracle model. PKCS#1v1.5 encryption should also not be made available to attackers anywhere in the API. Starting in TLS 1.3, PKCS#1 v1.5 encryption has been removed.</p> <p>Similarly, use authenticated encryption modes such as GCM or CCM to prevent CBC-mode attacks. For more information on block cipher modes, refer to Block and Stream Ciphers.</p>"},{"location":"kb-articles/chosen-ciphertext-attacks/#sources","title":"Sources","text":"<ul> <li>Gage Boyle, and Kenneth G. Paterson. 20 Years of Bleichenbacher\u2019s Attack. Royal Holloway University of London \u2013 ISG MSc Information Security Thesis Series 2019</li> <li>Tibor Jager, and Juraj Somorovsky. How to Break XML Encryption. Proceedings of the 18th ACM Conference on Computer and Communications Security\u2013CCS 2011.</li> <li>Mohamed Sabt, and Jacques Traor\u00e9. Breaking into the Keystore: A Practical Forgery Attack Against Android Keystore. Proceedings of the 21st European Symposium on Research in Computer Security\u2013ESORICS 2016.</li> <li>Thai Duong, and Juliano Rizzo. Here Come The \u2295 Ninjas.</li> <li>Juliano Rizzo, and Thai Duong. Practical Padding Oracle Attacks. Proceedings of the 4th USENIX Conference on Offensive Technologies\u2013WOOT 2010.</li> <li>Romain Bardou, Riccardo Focardi, Yusuke Kawamoto, Lorenzo Simionato, Graham Steel, and Joe-Kai Tsay. Efficient Padding Oracle Attacks on Cryptographic Hardware. Proceedings of the 32nd Annual International Cryptology Conference\u2013CRYPTO 2012.</li> <li>Daniel Bleichenbacher. Chosen Ciphertext Attacks Against Protocols Based on the RSA Encryption Standard PKCS #1. Proceedings of the 18th Annual International Cryptology Conference\u2013CRYPTO 1998.</li> <li>Nadhem J. Al Fardan, and Kenneth G. Paterson. Lucky Thirteen: Breaking the TLS and DTLS Record Protocols. Proceedings of the 2013 IEEE Symposium on Security and Privacy.</li> <li>Bodo M\u00f6ller, Thai Duong, and Krzysztof Kotowicz. This POODLE Bites: Exploiting The SSL 3.0 Fallback.</li> <li>Eiichiro Fujisaki, Tatsuaki Okamoto, David Pointcheval, and Jacques Stern. RSA-OAEP Is Secure under the RSA Assumption. Proceedings of the 21st Annual International Cryptology Conference\u2013CRYPTO 2001.</li> </ul> <ol> <li> <p>Indistinguishability under Chosen-Ciphertext Attack\u00a0\u21a9</p> </li> <li> <p>Optimal Asymmetric Encryption Padding\u00a0\u21a9</p> </li> </ol>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/","title":"Computational resources in cryptanalysis","text":"<p>Cryptographic security is fundamentally tied to the computational effort required to break cryptographic protections, including encryption and digital signatures. As computing power increases and new attack techniques emerge, cryptographic algorithms and key sizes that were once considered secure may become vulnerable. To ensure long-term security, cryptographic best practices must evolve alongside advancements in hardware, distributed computing, and mathematical optimizations.</p> <p>This article explores the computational resources necessary for breaking cryptographic keys and bypassing cryptographic protections, emphasizing brute-force feasibility, real-world cryptanalysis records, and the role of specialized hardware such as GPUs, ASICs, and quantum computers. We\u2019ll analyze how current cryptographic key sizes withstand modern attacks, identify algorithms that have already been broken, and assess potential future vulnerabilities.</p> <p>It\u2019s important to note that this article focuses only on direct computational attacks, omitting attacks that rely on side channels or implementation flaws. It aims to provide a realistic assessment of the impact of advances in computational power on cryptographic security , using real-world cryptanalysis records and practical attack feasibility as a foundation.</p>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#security-levels-and-bit-strength","title":"Security levels and bit strength","text":"<p>Cryptographic strength is often measured in bits of security, indicating that an attacker would require on the order of 2<sup>n</sup> operations (where n is the bit security level) to compromise the cryptographic key. Key size does not always directly correspond to security strength, as different cryptographic systems require different key lengths to provide equivalent security. </p> <p>For a detailed explanation of key sizes and security levels, including post-quantum considerations and NIST recommendations, refer to Knowledge base: Cryptographic keylengths.</p> <p>Common Security Levels Security levels commonly used to measure cryptographic strength can be categorized as follows:</p> <ul> <li>80-bit security \u2013 Deprecated; equivalent to breaking 1024-bit RSA.</li> <li>112-bit security \u2013 Acceptable for limited use, provided by 2048-bit RSA or ECC P-224.</li> <li>128-bit security \u2013 Modern baseline security, provided by AES-128 or ECC P-256.</li> <li>192-bit security \u2013 Medium security level, provided by AES-192 or ECC P-384.</li> <li>256-bit security \u2013 High security level for long-term protection, used in AES-256 and ECC P-521.</li> </ul> <p>It\u2019s important to reiterate that this is a snapshot in time. The bit security required to guarantee a reasonable level of security, as well as the bit security provided by a specific algorithm change over time. Consequently, the motivation of using higher security levels is at least partially to have a well-sized buffer to absorb such future changes. </p> <p>Additionally, there is no universal rule for how key size translates to security strength\u2014it depends on the mathematical structure of the algorithm. For example, ECC requires significantly smaller keys than RSA to provide the same level of security. </p>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#brute-force-cost-estimates","title":"Brute-force cost estimates","text":"<p>Brute-force attacks rely on trying to guess a solution and repeat until one works. A common example of this is to systematically guess cryptographic keys until the correct one is found. The difficulty of such attacks is measured in bit security, which in this case represents the number of computational operations expected to find a correct solution, for example to exhaustively search a keyspace. A key of n bits requires up to 2<sup>n</sup> operations to be broken using brute-force, assuming no cryptanalytic shortcuts exist.</p>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#computational-effort-required-for-different-bit-security-levels","title":"Computational effort required for different bit security levels","text":"<p>To estimate the practical cost of attacks, we model computations using CPU core-hours. We use a rate of $0.005 per hour for a two-core unit (based on EC2 t4g.nano instances, prices as of Q1 2025). We then reference a real-world example: the SHA-1 attack. While technically this wasn\u2019t a brute-force attack because it exploited cryptanalytic shortcuts, it still required trying many candidate solutions, similar to a brute-force attack.</p> <p>CWI Amsterdam and Google announced the SHA-1 \u2018SHAttered\u2019 attack on 23 February 2017, generating two different PDF files with the same SHA-1 hash in roughly 2<sup>63.1</sup> operations. This was significantly fewer than the 2<sup>80</sup> originally anticipated, due to the cryptanalytic shortcuts mentioned above, but 2<sup>63.1</sup> still represents a significant work effort. According to their estimates, the attack required approximately:</p> <ul> <li>6,500 CPU years, translating to $142,350 based on today\u2019s commercial cloud pricing at the $0.005 per core-hour referenced above (cloud costs in 2017 may have been higher).</li> <li>100 GPU years, for simplicity we do not take this cost into account.</li> </ul> <p>To put this 2<sup>63.1</sup> operations into context:</p> <ul> <li>2<sup>64</sup> operations would cost roughly double that amount, bringing it to $285,000.</li> <li>2<sup>80</sup> operations, the effort originally anticipated for finding collisions in SHA-1, would require roughly $35 billion USD in computational resources.</li> <li>2<sup>128</sup> operations, which represents the baseline standard for modern cryptographic security, would be completely infeasible using AWS resources. This represents the estimated brute-force resistance of SHA-256 and AES-128, both of which are considered secure against all practical attacks.</li> </ul> <p>These figures illustrate the computational costs of attacks assuming rented cloud resources. Although specialized hardware, such as GPUs or ASICs, could reduce these costs significantly, scaling attacks to cryptographically relevant security levels remains prohibitively expensive, even for highly funded nation-state actors. For context, at the time of writing, the entire Bitcoin network performs approximately 2<sup>90</sup> SHA-256 hashes per second, though the total operational cost isn\u2019t publicly known.</p>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#notable-cves-related-to-weak-cryptography","title":"Notable CVEs related to weak cryptography","text":"<p>Several cryptographic failures have demonstrated how small key sizes and outdated cryptographic algorithms lead to real-world vulnerabilities that can be exploited with sufficient computational resources. Below are notable CVEs and CWEs caused by cryptographic algorithms where the workload of an attacker to break them had been found to be below a level considered secure.</p> <ul> <li>CVE-2015-0204 (FREAK Attack, RSA) \u2013 RSA implementations downgraded to 512-bit \u201cexport-grade\u201d encryption, making them breakable in real-time.</li> <li>CVE-2015-4000 (Logjam Attack, Diffie-Hellman) \u2013 Weak 512-bit Diffie-Hellman groups allowed attackers to decrypt traffic using precomputed discrete logarithm attacks.</li> <li>CVE-2016-2183 (SWEET32 Attack, 3DES) \u2013 Exploited 3DES\u2019s small 64-bit block size, enabling practical collision attacks with enough ciphertext.</li> <li>CVE-2005-2730 (DES Weakness, DES) \u2013 56-bit DES keys found to be brute-forceable in under 24 hours using commercial hardware.</li> <li>CVE-2005-4900 (SHA-1 Weakness, Hashing) \u2013 SHA-1 proven weak against collision attacks, leading to its deprecation in security protocols.</li> <li>CVE-2004-2761 (MD5 Collision Vulnerability, Hashing) \u2013 MD5 demonstrated to produce collisions in minutes, rendering it unsuitable for cryptographic integrity.</li> <li>CWE-326 (Inadequate Encryption Strength) \u2013 Use of cryptographic keys that are too small for modern security standards.</li> <li>CWE-327 (Use of a Broken or Risky Cryptographic Algorithm) \u2013 Usage of outdated, weak cryptographic schemes like 512-bit RSA, DES, or MD5.</li> </ul>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#post-quantum-security-levels-and-key-size-considerations","title":"Post-Quantum security levels and key size considerations","text":"<p>With the advent of quantum computing, traditional cryptographic security assumptions need to be re-evaluated. Quantum attacks, such as Shor\u2019s algorithm, threaten widely used public-key cryptosystems, including RSA, ECC, and Diffie-Hellman, by reducing their effective security to a fraction of their classical strength. Grover\u2019s algorithm, on the other hand, formally weakens symmetric cryptography somewhat by offering a quadratic speedup in brute-force key search.</p>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#nist-pqc-security-strength-categories","title":"NIST PQC security strength categories","text":"<p>Post-quantum cryptographic (PQC) standards introduced by NIST define security levels based on their equivalence to AES security strength.</p> <ul> <li>Category 1 \u2013 Provides at least 128-bit security, equivalent to AES-128.<ul> <li>This is the minimum acceptable level for modern security, ensuring resistance against classical brute-force attacks while being computationally efficient.</li> <li>Example: ML-KEM-512 (Kyber-512) in post-quantum key encapsulation.</li> </ul> </li> <li>Category 3 \u2013 Provides at least 192-bit security, equivalent to AES-192.<ul> <li>Suitable for applications that require stronger long-term security.</li> <li>Example: ML-KEM-768 (Kyber-768) and ML-DSA-65 (Dilithium-65).</li> </ul> </li> <li>Category 5 \u2013 Provides at least 256-bit security, equivalent to AES-256.<ul> <li>Intended for high-assurance environments (government, military, financial infrastructure) where security must hold up against future cryptanalytic advances.</li> <li>Example: ML-KEM-1024 (Kyber-1024) and ML-DSA-87 (Dilithium-87).</li> </ul> </li> </ul> <p>While Grover\u2019s algorithm weakens symmetric cryptography by reducing effective key security by half the bits (for example, AES-256 would offer 128-bit quantum security), increasing key sizes generally mitigates this effect. As a result, AES-256 remains viable for long-term security against quantum threats. Additionally, a caveat of Grover\u2019s algorithm is that it cannot be efficiently parallelised which alleviates concerns of its impact, in contrast to Shor\u2019s algorithm.</p>"},{"location":"kb-articles/computational-resources-in-cryptanalysis/#sources","title":"Sources","text":"<ul> <li>Knowledge base: Timing attacks and broader side-channel attacks</li> <li>Knowledge base: Cryptographic keylengths</li> <li>Knowledge base: Quantum attack</li> <li>AWS EC2 On-Demand Pricing</li> <li>NIST Post-Quantum Cryptography Standardization Project</li> <li>NIST SP 800-57 Part 1 Revision 5 \u2013 Recommendation for Key Management</li> <li>NIST SP 800-131A Revision 2 \u2013 Transitioning the Use of Cryptographic Algorithms and Key Lengths</li> <li>Google &amp; CWI Amsterdam (2017) \u2013 \u201cSHAttered\u201d SHA-1 Collision Attack</li> <li>CVE-2015-0204 \u2013 FREAK (RSA downgrade attack)</li> <li>CVE-2015-4000 \u2013 Logjam (weak Diffie-Hellman attack)</li> <li>CVE-2016-2183 \u2013 SWEET32 (3DES collision attack)</li> <li>CVE-2005-2730 \u2013 DES Weakness (Brute-force feasibility)</li> <li>CVE-2005-4900 \u2013 SHA-1 Weakness (Collision attacks)</li> <li>CVE-2004-2761 \u2013 MD5 Collision Vulnerability</li> </ul>"},{"location":"kb-articles/cryptographic-keylengths/","title":"Cryptographic keylengths","text":"<p>Cryptographic keys can be thought of as sequences of bits, and their length plays a significant role in determining their security, which varies by algorithm. The terms key length and key size are often used interchangeably to refer to the number of bits in a key. Security strength is typically measured in bits; for example, AES-128, which uses 128-bit keys, provides 128 bits of security. However, key length doesn\u2019t always directly correlate with security strength. For instance, RSA-1024, with 1024-bit keys, provides only 80 bits of security, which is now considered inadequate according to NIST guidelines. In addition to key length, several other factors must be considered when dealing with cryptographic keys. Refer to Key management for details.</p> <p>Caution</p> <p>The following information and examples are provided for general reference. Given the rapid evolution of standards and cryptographic guidance, it is crucial to consult the latest versions of the referenced publications before making decisions or implementation changes. Always verify that you are using the most up-to-date information available and check that you have the latest revision numbers for each document.</p>"},{"location":"kb-articles/cryptographic-keylengths/#detailed-description","title":"Detailed description","text":"<p>Cryptographic keys can be viewed as sequences of bits, with longer keys generally providing more security. However, the actual security strength depends on the underlying cryptographic algorithm. For instance:</p> <ul> <li>AES supports key lengths of 128, 192, and 256 bits</li> </ul> <ul> <li>RSA supports key lengths of 2048, 3072, 4096 and 7680 bits, among others</li> </ul> <p>In terms of security strength, both AES-128 and RSA-3072 offer approximately the same level of security, equivalent to 128 bits.</p>"},{"location":"kb-articles/cryptographic-keylengths/#measuring-security-strength","title":"Measuring security strength","text":"<p>The security strength, or security level, of a cryptographic algorithm corresponds to the approximate number of operations an attacker would need to perform to compromise its security. The security strength is often measured in bits. For the given example of 128 bits of security, this means that breaking AES-128 or RSA-3072 would require approximately 2<sup>128</sup> operations. To put that number into context, it\u2019s roughly 3.4*10<sup>38</sup>, which is a larger number than all the planets and stars in the universe. This is the modern baseline or minimum for security standards, and extends even higher to 2<sup>192</sup> and 2<sup>256</sup> for even stronger security. To put these numbers in perspective; if every planet in the universe was the same as Earth, 2<sup>192</sup> is still significantly larger than all the grains of sand on every planet combined; 2<sup>256</sup> is so vastly large, it is close to the total number of atoms in the universe.</p>"},{"location":"kb-articles/cryptographic-keylengths/#nist-security-strength-guidelines","title":"NIST security strength guidelines","text":"<p>A security strength of 80 bits is no longer considered adequate, as noted in NIST SP 800-57 Part 1 Revision 5 (Recommendation for Key Management: Part 1 - General, published May 2020, Section 5.6.1). For example, RSA-1024 provides only 80 bits of security, which does not meet current security requirements. Table 1, reproduced from NIST SP 800-57 Part 1 Revision 5, outlines the transition to a minimum security strength of 128 bits by 2030.</p> <p>Caution</p> <p>The content in Table 1 is accurate at the time of writing (Q1 2025).</p> <p>Table 1. Reproduction of Table 4, Section 5.6.3, of NIST SP 800-57 Part 1 Revision 5.</p> Security Strength Applying and/or Processing Through 2030 2031 and Beyond Less than 112 bits Applying protection Disallowed Disallowed Less than 112 bits Processing Legacy use Legacy use 112 bits Applying protection Acceptable Disallowed 112 bits Processing Acceptable Legacy use 128 bits Both applying and processing Acceptable Acceptable 192 bits Both applying and processing Acceptable Acceptable 256 bits Both applying and processing Acceptable Acceptable <p>Note that algorithms that provide a security strength of less than 112 bits should no longer be used to protect new data (Applying protection), although they may still be used for legacy purposes, such as recovering previously encrypted data (Processing). A security strength of 112 bits (for example, RSA-2048) is currently the minimum allowed, but from 2030 onwards, it will be permitted only for legacy processing purposes, i.e. decryption and verification.</p> <p>A notable example of an exception to the 112-bit rule is 3TDEA Encryption, which provides 112 bits of security strength but was deprecated in 2023 in NIST SP 800-131A Revision 2 (Table 1, page 7).</p>"},{"location":"kb-articles/cryptographic-keylengths/#post-quantum-cryptography-security-categories","title":"Post-quantum cryptography security categories","text":"<p>In the context of Post-Quantum Cryptography Standardization, NIST has introduced a separate categorization system, consisting of five security categories, numbered from 1 to 5, to classify quantum-resistant cryptographic algorithms. This scale ranges from Category 1 (the minimal and lowest considered security strength) to Category 5 (the highest security strength).</p>"},{"location":"kb-articles/cryptographic-keylengths/#category-definitions","title":"Category definitions","text":"<p>These categories are defined relative to established cryptographic standards:</p> <ul> <li>Category 1: Provides at least 128 bits of security, equivalent to AES-128.</li> <li>Category 2: Offers security comparable to finding a collision in SHA-256 or SHA3-256.</li> <li>Category 3: Provides at least 192 bits of security, equivalent to AES-192.</li> <li>Category 4: Offers security comparable to finding a collision in SHA-384 or SHA3-384.</li> <li>Category 5: Provides at least 256 bits of security, equivalent to AES-256.</li> </ul>"},{"location":"kb-articles/cryptographic-keylengths/#usage-in-post-quantum-cryptography-schemes","title":"Usage in post-quantum cryptography schemes","text":"<p>These categories are already used in recently standardized quantum-resistant schemes, such as:</p> <ul> <li>FIPS 203: ML-KEM (Module-Lattice-based Key Encapsulation Mechanism), formerly known as Kyber.</li> <li>FIPS 204: ML-DSA (Module-Lattice-based Digital Signature Algorithm), formerly known as Dilithium.</li> <li>FIPS 205: SLH-DSA (Stateless Hash-based Digital Signature Algorithm), formerly known as SPHINCS+.</li> </ul> <p>For example, FIPS 203 specifies three versions of ML-KEM, each providing a different security level:</p> <ul> <li>ML-KEM-512: Category 1</li> <li>ML-KEM-768: Category 3</li> <li>ML-KEM-1024: Category 5</li> </ul> <p>Note that the number following the algorithm designation does not directly correspond to the key length. In these schemes, key lengths are measured in bytes, not bits, due to their large size. For instance:</p> <ul> <li>The encapsulation key (public key) of ML-KEM-768 is 1184 bytes.</li> <li>The decapsulation key (secret key) of ML-KEM-768 is 2400 bytes.</li> </ul> <p>Detailed information on key lengths for ML-KEM can be found in FIPS 203 in Table 3 on page 39.</p>"},{"location":"kb-articles/cryptographic-keylengths/#security-recommendations","title":"Security recommendations","text":"<p>The transition to post-quantum cryptographic standards is driving rapid changes in security recommendations, which may be updated with short notice. </p> <p>To illustrate this point, we highlight two NIST publications under revision at the time of writing:</p> <ul> <li>NIST SP 800-131A Transitioning the Use of Cryptographic Algorithms and Key Lengths, (Revision 2, published March 2019). The initial public draft of Revision 3 is available at the time of writing. Revision 3 includes:  <ul> <li>Mentions of quantum-resistant algorithms specified in FIPS 203, 204, and 205  </li> <li>Updated guidelines.</li> </ul> </li> </ul> <ul> <li>NIST IR 8547 Transition to Post-Quantum Cryptography Standards (initial public draft) states that after 2035, cryptographic standards involving quantum-vulnerable algorithms will be disallowed, including:  <ul> <li>Digital signature algorithms: ECDSA, EdDSA, and RSA (as specified in FIPS 186-5), across all security strengths.   </li> <li>Key establishment schemes: NIST SP 800-56A and NIST SP 800-56B (RSA).</li> </ul> </li> </ul> <p>Caution</p> <p>The content in Table 2 is accurate at the time of writing (Q1 2025).</p> <p>Table 2. Digital Signatures: FIPS 186-5<sup>7</sup>, FIPS 204, and  FIPS 205.</p> Digital Signature Algorithms Example Instantiations Security Strength Status in NIST SP 800 131A Revision 2 Status in NIST SP 800 131A Revision 3 (IPD) ECDSA<sup>1</sup> Generation ECDSA P-192  ECDSA P-224  ECDSA P-256 &lt; 112 bits  \u2248112 bits  &gt;= 128 bits Disallowed  Acceptable  Acceptable Disallowed  Acceptable until 2030<sup>10</sup>  Acceptable ECDSA Verification &lt; 112 bits  &gt;= 112 bits Legacy use  Acceptable Legacy use   Acceptable* EdDSA<sup>2</sup> Ed25519 &gt;= 128 bits Acceptable Acceptable* RSA<sup>3</sup> generation (PKCS #1 v1.5 &amp; PSS) RSA-1024  RSA-2048  RSA-3072 &lt; 112 bits  \u2248112 bits  &gt;= 128 bits Disallowed  Acceptable  Acceptable Disallowed  Acceptable until 2030<sup>10</sup>  Acceptable RSA verification (PKCS #1 v1.5 &amp; PSS) &lt; 112 bits  &gt;= 112 bits Legacy use  Acceptable Legacy use   Acceptable* ML-DSA<sup>4</sup> ML-DSA-44<sup>5</sup>   ML-DSA-65   ML-DSA-87 Category 2   Category 3   Category 5 N/A Acceptable SLH-DSA<sup>6</sup> SLH-DSA-128s<sup>11</sup>  SLH-DSA-192s<sup>11</sup>  SLH-DSA-256s<sup>11</sup> Category 1   Category 3   Category 5 N/A Acceptable <p>Table 2 presents the guidelines from NIST according to NIST SP 800 131A Revision 2 and NIST SP 800 131A Revision 3 (IPD) for digital signature algorithms specified in FIPS 186-5, FIPS 204, and FIPS 205. The rightmost column indicates that the transition to quantum-resistant digital signature algorithms will impact algorithms corresponding to fields marked with \u2018*\u2019. </p> <p>Caution</p> <p>The content in Table 3 is accurate at the time of writing (Q1 2025).</p> <p>Table 3. Key Establishment Schemes: NIST SP 800-56B Rev. 2 and NIST SP 800-56A Rev. 3.</p> Key Agreement Algorithms Example Instantiations Security Strength Status in NIST SP 800 131A Revision 2 Status in NIST SP 800 131A Revision 3 (IPD) RSA<sup>8</sup> RSA-1024  RSA-2048  RSA-3072 &lt; 112 bits  112 bits  &gt;= 128 bits Disallowed  Acceptable  Acceptable Legacy use  Acceptable until 2030<sup>10</sup>  Acceptable ECC<sup>9</sup> ECC P-192  ECC P-224  ECC P-256 &lt; 112 bits  112 bits  &gt;= 128 bits Disallowed  Acceptable  Acceptable Disallowed  Acceptable until 2030<sup>10</sup>  Acceptable Diffie-Hellman<sup>9</sup> MODP-1024  MODP-2048  MODP-3072 &lt; 112 bits  112 bits  &gt;= 128 bits Disallowed  Acceptable  Acceptable Disallowed  Acceptable until 2030<sup>10</sup>  Acceptable <p>The guidelines for key-establishing algorithms are shown in Table 3 and are similar to those for digital signature algorithms, with 128 bits of security being preferable until the post-quantum transition.</p> <p>NIST has not set a hard deadline for post-quantum cryptographic migration but is developing a transition schedule. Further guidance will be provided in future revisions of NIST SP 800-131A Revision 3. In contrast, the NSA\u2019s CNSA 2.0 guidance outlines specific deadlines, requiring all National Security Systems (NSS) to be quantum-resistant by 2035. The NSA\u2019s timeline mandates that new acquisitions must be CNSA 2.0 compliant by January 1, 2027, and all non-compliant equipment and services must be phased out by December 31, 2030.</p>"},{"location":"kb-articles/cryptographic-keylengths/#sources","title":"Sources","text":"<ul> <li>NSA CNSA Suite 2.0 and Quantum Computing FAQ</li> <li>NIST SP 800-56A Rev. 3 Recommendation for Pair-Wise Key-Establishment Schemes Using Discrete Logarithm Cryptography</li> <li>NIST SP 800-56B Rev. 2 Recommendation for Pair-Wise Key-Establishment Using Integer Factorization Cryptography</li> <li>NIST SP 800-57 Part 1 Rev. 5 - Recommendation for Key Management: Part 1 \u2013 General</li> <li>NIST SP 800-131A Rev. 2 - Transitioning the Use of Cryptographic Algorithms and Key Lengths</li> <li>NIST SP 800-131A Rev. 3 (Initial Public Draft) Transitioning the Use of Cryptographic Algorithms and Key Lengths</li> <li>NIST SP 800-186 Recommendations for Discrete Logarithm-based Cryptography: Elliptic Curve Domain Parameters</li> <li>NIST IR 8547 (Initial Public Draft) Transition to Post-Quantum Cryptography Standards</li> <li>Public Comments on NIST IR 8547 (ipd), Transition to Post-Quantum Cryptography Standards</li> <li>FIPS 186-4 Digital Signature Standard (DSS) - Withdrawn on February 03, 2024. Superseded by FIPS 186-5</li> <li>FIPS 186-5 Digital Signature Standard (DSS)</li> <li>FIPS 203 Module-Lattice-Based Key-Encapsulation Mechanism Standard</li> <li>FIPS 204 Module-Lattice-Based Digital Signature Standard</li> <li>FIPS 205 Stateless Hash-Based Digital Signature Standard</li> <li>NIST Post-Quantum Cryptography Project</li> </ul> <ol> <li> <p>Source: NIST SP 800-186, Table 1 and Table 2, pages 6 and 7.\u00a0\u21a9</p> </li> <li> <p>Source: FIPS 186-5, Section 7.1, page 26.\u00a0\u21a9</p> </li> <li> <p>Source: NIST SP 800-57 Part 1 Revision 5, Table 2, pages 54 and 55 (FIPS 186-5 refers in section 5.1 that the previous document should be considered).\u00a0\u21a9</p> </li> <li> <p>Source: FIPS 204, Section 4, page 15.\u00a0\u21a9</p> </li> <li> <p>FIPS 204, Section 3.6.1, page 12, discusses the requirements for ML-DSA-44 to be classified as Category 2.\u00a0\u21a9</p> </li> <li> <p>Source: FIPS 205, Section 11, page 43.\u00a0\u21a9</p> </li> <li> <p>DSA was removed in FIPS 186-5. It is no longer allowed for digital signature generation. It may be used to verify signatures generated before the implementation date of FIPS 186-5. DSA specifications are available in FIPS 186-4.\u00a0\u21a9</p> </li> <li> <p>Source: NIST Special Publication 800-56B Revision 2, Table 2 and 4, page 38 and 117.\u00a0\u21a9</p> </li> <li> <p>Source: NIST Special Publication 800-56A Revision 3, Table 24 and 25, page 132 and 133.\u00a0\u21a9\u21a9</p> </li> <li> <p>Use of these cryptography schemes is acceptable until 2030 and is depreciated after 2030*, however quantum computers will impact these algorithms.\u00a0\u21a9\u21a9\u21a9\u21a9\u21a9</p> </li> <li> <p>Technically these parameter names are SLH-DSA-SHAKE-128s, SLH-DSA-SHAKE-192s, and SLH-DSA-SHAKE-256s, but they were shortened to save space in the table.\u00a0\u21a9\u21a9\u21a9</p> </li> </ol>"},{"location":"kb-articles/cryptography-bill-of-materials/","title":"Cryptography Bill of Materials (CBOM)","text":"<p>A cryptographic bill of materials (CBOM) is a cybersecurity asset that lists all software and hardware components within a system, including their dependencies, vulnerabilities, and security risks. CBOMs extend software bill of materials (SBOM) by defining an object model to describe cryptographic assets and their dependencies. </p> <p>When integrated into reporting and software development practices, CBOMs enable organizations to manage and report usage of cryptography such as enforcing a secure cryptographic policy across IT infrastructure, reacting quickly to security issues, efficiently carrying out strategic transformations (for example, migrating cryptography services to the cloud, or deploying post-quantum cryptography.</p>"},{"location":"kb-articles/cryptography-bill-of-materials/#cryptography-inventories","title":"Cryptography inventories","text":"<p>A cryptography inventory is a comprehensive catalog of all cryptographic components used within an organization. It provides a detailed map of:</p> <ul> <li>Which applications utilize cryptography</li> <li>The purpose of each cryptographic implementation</li> <li>Technical details, including:<ul> <li>Algorithms and usage modes</li> <li>Keys and key storage solutions</li> <li>Certificates</li> <li>Protocol versions</li> <li>Library versions</li> </ul> </li> <li>Non-technical information, such as: <ul> <li>Data classification</li> <li>Business purpose</li> <li>Responsible personnel</li> </ul> </li> </ul> <p>The following sections will outline key layers of cryptographic inventories</p>"},{"location":"kb-articles/cryptography-bill-of-materials/#application-cryptography","title":"Application cryptography","text":"<p>In a typical enterprise, applications use cryptography to:</p> <ul> <li>Encrypt sensitive data at rest or in transit.</li> <li>Sign or verify documents and files.</li> <li>Protect passwords and credentials.</li> <li>Secure access sensitive resources.</li> <li>Participate in other protocols such as single sign-on, TLS, SSH, or Kerberos.</li> <li>Perform business-specific cryptography such as authorising payments.</li> </ul> <p>These cryptographic operations involve a range of technical components, including: </p> <ul> <li>Specific algorithms and keys.</li> <li>Modes of operation.</li> <li>Protocol versions.</li> <li>Cryptographic libraries. </li> </ul> <p>To access and manage these components, applications typically retrieve keys and certificates from a variety of sources such as filesystem keystores or hardware security modules (HSMs).</p>"},{"location":"kb-articles/cryptography-bill-of-materials/#infrastructure-cryptography","title":"Infrastructure cryptography","text":"<p>Enterprise infrastructure typically makes extensive use of network cryptography protocols such as TLS, IPSec, and SSH. Each of these requires an infrastructure of keys and certificates. To ensure effective management and security, these components must be included in a comprehensive cryptographic inventory. </p> <p>Additionally, various hardware devices including HSMs, VPN appliances, firewalls, and intrusion detection systems all contain cryptographic artifacts and support specific algorithms. Often these systems control the most sensitive and business-critical cryptography in the organization, such as the secret keys used to protect high value data and assets. </p>"},{"location":"kb-articles/cryptography-bill-of-materials/#cryptographic-assets","title":"Cryptographic assets","text":"<p>The following list represents a selection of common cryptographic assets, categorized into two main groups: executable and non-executable file objects. Executables are components of runnable software, and non executables are filesystem objects referenced by executables. Non-executables include keystores and other formats that may contain both public and private asymmetric keys. </p>"},{"location":"kb-articles/cryptography-bill-of-materials/#executable-file-objects","title":"Executable file objects","text":"<p>These are components of runnable software that may utilize or interact with cryptographic assets:</p> <ul> <li>Application binaries - Compiled programs that execute on a system.</li> <li>Cryptographic libraries - Software libraries that provide cryptographic functionality.</li> <li>Software archives - Packages that contain executable code, such as JAR (Java Archive) files.</li> </ul>"},{"location":"kb-articles/cryptography-bill-of-materials/#non-executable-file-objects","title":"Non-executable file objects","text":"<ul> <li>Keystores <ul> <li>PKCS#12 </li> <li>Java Keystores</li> <li>Key Data Sets </li> </ul> </li> <li>Other key formats:<ul> <li>OpenPGP Keys </li> <li>X.509 Certificates </li> <li>OpenSSH Keys </li> <li>PKCS#1</li> <li>PKCS#8</li> </ul> </li> </ul>"},{"location":"kb-articles/cryptography-bill-of-materials/#cboms","title":"CBOMs","text":"<p>A CBOM typically includes a cryptography inventory tailored to specific goals. For example:</p> <ul> <li>Crypto-agility planning - The report should distinguish between algorithms used for cryptographic and non-cryptographic purposes, enabling planning and decision making during migration.  </li> <li>Cryptographic policy control - The CBOM must be as detailed as the policy itself, providing a thorough understanding of the cryptographic landscape. </li> </ul>"},{"location":"kb-articles/cryptography-bill-of-materials/#benefits-of-an-up-to-date-cbom","title":"Benefits of an up-to-date CBOM","text":"<p>An updated CBOM can help identify and prioritize actions to remediate availability and security issues related to cryptographic objects. For example, a regularly generated CBOM can:</p> <ul> <li>Detecting expiring certificates - Allow for timely renewal and minimizing potential disruptions. </li> <li>Prioritize critical certificates - Identify the most frequently used certificates that are approaching expiration and should be addressed first. </li> <li>Identify vulnerable cryptographic components - Detect issues such as: <ul> <li>ECDSA signing certificates using the SHA-1 hash function that are vulnerable to collision attacks.</li> <li>ECDSA certificates with insufficient key length, or other weak keys, that need to be renewed.</li> </ul> </li> </ul>"},{"location":"kb-articles/cryptography-bill-of-materials/#cbom-in-aqtive-guard","title":"CBOM in AQtive Guard","text":"<p>AQtive Guard provides a single, unified repository for storing and managing all your CBOM scans, ensuring centralized visibility into your cryptographic assets. With support for the Cryptographic Bill of Materials (CBOM) format, AQtive Guard seamlessly ingests CBOM files (JSON format) to catalog cryptographic components, identify vulnerabilities, and assess risks across software supply chains.</p> <p>This integration gives organizations a real-time, structured view of their cryptographic inventories, enhancing security posture and compliance efforts. For more details on using CBOM with AQtive Guard, refer to the user guide. </p>"},{"location":"kb-articles/cryptography-bill-of-materials/#recommendations-for-implementing-cbom","title":"Recommendations for implementing CBOM","text":"<p>Using CBOMs has recently become more mainstream due to recommendations from standards bodies and industry analysts, and the need for visibility into deployed cryptography to facilitate the post-quantum migration. </p> <p>Preparing a CBOM should leverage automated tools to identify the cryptographic algorithms used in hardware and software modules, libraries, and embedded code (as recommended in NIST Special Publication 800-128 or NCCoE), as well as data at rest, in transit, and in use.</p> <p>After vulnerable cryptography components and associated assets are identified, the next objective is to prioritize initial remediation efforts using a risk management methodology informed by the sensitivity and criticality of the information being protected over time.</p>"},{"location":"kb-articles/cryptography-bill-of-materials/#post-quantum-cryptography-migration","title":"Post-Quantum cryptography migration","text":"<p>CBOMs play a particularly important role when tackling the migration from classical asymmetric cryptography to  post-quantum cryptography. An updated and detailed CBOM allows organizations to find vulnerable algorithms such as:</p> <ul> <li>Signature algorithms used in TLS certificates.</li> <li>Key agreement algorithms, such as elliptic curve and finite field Diffie-Hellman.</li> <li>Public key encryption algorithms, such as RSA.</li> </ul> <p>Although not all protocols and schemes have post-quantum options available, knowing which vulnerable algorithms are currently deployed, and how frequently they are used is a crucial first step in migrating away from them. As noted in  NIST SP 1800-38B, this initial step can be taken now. A regularly updated CBOM will help prioritize which classical cryptography can and must be upgraded to post-quantum options as they become available. </p>"},{"location":"kb-articles/cryptography-bill-of-materials/#sources","title":"Sources","text":"<ul> <li>NIST SP 1800-38B, \u201cMigration to Post-Quantum Cryptography Quantum Readiness: Cryptographic Discovery: Approach, Architecture, and Security Characteristics of Public Key Application Discovery Tools\u201d, December 2023</li> <li>NIST SP 800-57 Part 1 Rev. 5  - \u201cRecommendation for Key Management: Part 1 \u2013 General\u201d, May 2020</li> <li>NIST Special Publication 800-128</li> <li>Gartner report, Better Safe than Sorry: Preparing for Crypto-Agility</li> <li>Knowledge base: RSA algorithm</li> <li>Knowledge base: Elliptic curve cryptography</li> <li>Knowledge base: Certificates</li> <li>Knowledge base: Weak cryptography keys</li> <li>Knowledge base: TLS</li> <li>Knowledge base: Quantum threat</li> </ul>"},{"location":"kb-articles/elliptic-curve-cryptography/","title":"Elliptic curve cryptography","text":"<p>This document is under development and will be available in a future release.</p>"},{"location":"kb-articles/hash-functions/","title":"Hash functions","text":""},{"location":"kb-articles/hash-functions/#high-level-description","title":"High-level description","text":"<p>Using an insecure hash function can lead to a wide range of business risks, including system tampering, data breaches, unauthorized access, and legal non-compliance. </p> <p>That\u2019s because cryptographic hash functions are a core cryptographic building block ubiquitous in cryptographic systems across a wide range of applications. </p> <p>Their main job is to transform long input data into a short, much harder to predict, fixed-length output, known as a hash value or digest. This output is always the same for the same input, but it should appear unpredictable or \u201crandom-looking\u201d for security purposes.</p> <p>Assets that depend on hash functions include but are not limited to: </p> <ul> <li>(public-key) certificates, such as X.509.</li> <li>Secure software updates.</li> <li>Digital signatures for document signing documents, like PDF signatures. </li> <li>Message integrity and authentication in communications and data at rest, for example, HMAC.</li> <li>Password hashing, such as SCrypt.</li> </ul> <p>Because these applications have different security requirements, the severity of a specific attack against a hash function and its resulting business threat will vary based on the application.</p> <p>In general, secure hash functions are widely available. Therefore, migrating from a weak to a secure hash function, such as SHA-2 or SHA-3, is strongly recommended.</p>"},{"location":"kb-articles/hash-functions/#detailed-description","title":"Detailed description","text":"<p>As the basic building block for many cryptographic protocols, a hash function is used in a variety of ways for different applications. A secure cryptographic hash function behaves like a random function, meaning:</p> <ul> <li>The output appears random, with no apparent structure.</li> <li>It\u2019s hard to find two different inputs that provide the same output (a collision).</li> <li>Given the output, it\u2019s hard to determine the input that generated it.</li> </ul> <p>Given these properties, a hash value can be used as a computationally unique fingerprint of a data object, or to generate random-looking values (also called pseudo-random values), such as when deriving cryptographic keys. Hash functions are commonly used in communication protocols for transcript hashing and key derivation, while in digital signature schemes, they\u2019re used for message compression. </p> <p>Hash functions also have many other applications, such as constructing password hashing functions like Scrypt or message authentication codes like HMAC. </p> <p>Because different uses of hash functions have varying security requirements, the severity of an attack against a hash function can vary depending on the application.  </p>"},{"location":"kb-articles/hash-functions/#security-properties","title":"Security properties","text":"<p>A secure hash function must guarantee three key security properties: </p> <ol> <li>Collision resistance: Essentially, collision resistance prevents attackers from creating two different pieces of data that appear identical when hashed. It\u2019s practically infeasible to find two different inputs X1 and X2 that produce the same hash value, H(X1) = H(X2), where X1 \u2260 X2. This is important because if an attacker can find such a pair as X1, X2 (a collision), they could potentially use it to forge a digital signature or to produce a false positive to pass an integrity check, like in checksum validation. </li> <li>Preimage resistance: It\u2019s computationally infeasible to find an input X that produces a given hash value Y. This input X is referred to as a preimage and can be used to overcome a password check if the password hash is known.</li> <li>Pseudorandom outputs: The output of the hash function should be indistinguishable from a truly random value to anyone who doesn\u2019t know the input. This means that even if someone sees the output, they shouldn\u2019t be able to detect any patterns or predict future outputs. If an attacker can distinguish the hash output, they could potentially break cryptographic keys derived using the hash function, or forge message authentication codes.</li> </ol>"},{"location":"kb-articles/hash-functions/#security-considerations","title":"Security considerations","text":"<p>The security of cryptographic hash functions with respect to the above Security properties is established by cryptanalysis. The more cryptographers attempt to break a specific hash function and fail, the more convinced we are of its security.  A hash function is considered secure if the required resources to attack the function are so enormous that the computation is considered practically infeasible.  Computations that currently require 2<sup>128</sup> operations are commonly considered infeasible. For the highest security level, a cost of 2<sup>256</sup> is typically required.</p> <p>A hash function is considered theoretically broken the moment an attack is significantly more efficient than a brute-force attack.  While such a theoretical attack does not pose an immediate threat, it demonstrates a critical weakness.  In the past, known weaknesses have often been extended into practical attacks within relatively short timeframes.  As a result, theoretically broken hash functions are typically deprecated by regulators. </p> <p>Over the past two decades, cryptanalysis of hash functions has made significant progress due to the break of MD5 and SHA1, and the following SHA3 competition run by the National Institute of Standards and Technology (NIST).  This process drove substantial research funding into studying hash functions and, as a result, the security of hash functions is now well understood.</p>"},{"location":"kb-articles/hash-functions/#severity-of-vulnerabilities","title":"Severity of vulnerabilities","text":"<p>The severity of a hash function vulnerability largely depends on the application of the hash function. A vulnerability that allows for finding collisions in a hash function directly impacts its applicability as a fingerprinting function.  For example, a hash function vulnerable to collision attacks must not be used for message compression in digital signatures or as a fingerprint for software downloads.  However, uses like message authentication codes, password hashing, or key derivation are not directly threatened. </p> <p>Even though some applications may not be immediately at risk, a collision attack that performs significantly better than a generic attack weakens the security of the hash function, potentially indicating a broader structural weakness.  As a result, it\u2019s common practice to deprecate a hash function whenever a collision attack is discovered, even though some applications are not immediately threatened.    </p> <p>It should be noted that the scientific community around hash functions uses the term \u201cattack\u201d quite loosely.  Often, an \u201cattack\u201d refers to a partial or theoretical attack on an artificially weakened version of a hash function.  These partial attacks play a crucial role in understanding the security of a hash function but don\u2019t necessarily threaten the actual, full-strength function. </p>"},{"location":"kb-articles/hash-functions/#security-recommendations","title":"Security recommendations","text":"<p>As hash functions are an integral part of cryptographic systems, virtually all cryptographic guidelines include recommendations for hash functions.  The security of a hash function against generic attacks depends on its output (digest) length.  Hence, recommendations typically focus on two aspects: the choice of cryptographic hash function and the appropriate digest length. </p> <p>Recent guidelines recommend using SHA-2 and SHA-3 with varying output lengths for current and future use.  For specific requirements on output length from different organizations, refer to Cryptographic key length and the keylength.com website<sup>1</sup>. </p>"},{"location":"kb-articles/hash-functions/#summary-tables","title":"Summary tables","text":"<p>The tables below summarize recommendations from NSA<sup>2</sup> (CNSA 2.0), BSI<sup>3</sup>, and ANSSI<sup>4</sup> as well as the status of known attacks against the previously widely deployed hash functions SHA1 and MD5.  More detailed explanations follow.</p>"},{"location":"kb-articles/hash-functions/#recommended-and-approved-hash-functions","title":"Recommended and approved hash functions","text":"CNSA 2.0 BSI ANNSI SHA2 SHA-384 / SHA-512 SHA-256, SHA-512/256, SHA-384, and SHA-512 SHA-256, SHA-512/256, SHA-384, and SHA-512 SHA3 Not approved SHA3-256, SHA3-384, and SHA3-512 SHA3-256, SHA3-384, and SHA3-512"},{"location":"kb-articles/hash-functions/#known-attack-status-for-sha1-and-md5","title":"Known attack status for SHA1 and MD5","text":"Collisions Preimages Pseudorandomness SHA1 Practical attacks No attacks known No attacks known MD5 Practical attacks Theoretical attacks known Theoretical attacks known <p>Note</p> <p>Recommendations for hash function use will vary depending on your specific use case. Consult the relevant authorities within your organization for your application to ensure secure and appropriate usage.</p>"},{"location":"kb-articles/hash-functions/#overview-of-commonly-used-hash-function-families","title":"Overview of commonly used hash function families","text":""},{"location":"kb-articles/hash-functions/#sha2","title":"SHA2","text":"<p>SHA2<sup>5</sup> is a family of hash functions that includes SHA-224, SHA-256, SHA-384, SHA-512, and the truncated output functions SHA-512/224, SHA-512/256.  The number after \u201cSHA\u201d indicates the output (digest) length in bits: 224, 256, 384, or 512. </p> <p>SHA2 is widely recommended as a secure hash function by many institutions publishing cryptographic recommendations, including NSA<sup>6</sup>, BSI<sup>7</sup>, and ANSSI<sup>8</sup>.  Requirements for output length vary based on the application, timeframe, and guidelines of the recommending institution.</p>"},{"location":"kb-articles/hash-functions/#sha3-and-shake","title":"SHA3 (and SHAKE)","text":"<p>SHA3<sup>9</sup> is a family of hash functions based on the Sponge construction, including SHA3-224, SHA3-256, SHA3-384, SHA3-512, where the number indicates the output length in bits: 224, 256, 384, or 512. </p> <p>SHA3 is recommended as a secure hash function by many institutions publishing cryptographic recommendations, including BSI<sup>10</sup> and ANSSI<sup>11</sup>. While it is permitted as part of specific algorithms in CNSA 2.0, it is not approved as a standalone cryptographic hash function under CNSA.  Requirements for output length vary based on the application, timeframe, and guidelines of the recommending institution. </p>"},{"location":"kb-articles/hash-functions/#sha1-broken","title":"SHA1 (broken)","text":"<p>SHA1<sup>12</sup> is an older hash function that was previously widely used and is the predecessor of SHA2.  However, practical collision attacks have been demonstrated<sup>13</sup>, leading to the deprecation of SHA1. </p> <p>Applications using SHA1 as a cryptographic fingerprint, such as in digital signatures, certificates, software updates, entity authentication or file integrity should be migrated immediately to a secure hash function like SHA2 or SHA3 as attacks are now possible (although they still require significant computational resources). </p> <p>Migrating to a secure hash function is recommended, based on your application and security risk.</p>"},{"location":"kb-articles/hash-functions/#md5-broken","title":"MD5 (broken)","text":"<p>MD5<sup>14</sup> was once a widely used hash function, but collision attacks have been demonstrated and even observed in the wild<sup>15</sup>.  As a result, the use of MD5 is now deprecated. Applications that use MD5 for cryptographic fingerprints such as in digital signatures, certificates, software updates, entity authentication, or file integrity should be migrated immediately to a secure hash function like SHA2 or SHA3 as practical attacks are possible. </p> <p>Even applications that rely solely on the one-way or pseudo-random properties of MD5 should be migrated to a secure hash function quickly, since MD5 has also shown weaknesses in resisting preimage attacks<sup>16</sup>.      </p>"},{"location":"kb-articles/hash-functions/#blake2","title":"BLAKE2","text":"<p>BLAKE2<sup>17</sup> was one of the finalists in NIST\u2019s SHA3 competition and has undergone extensive cryptanalysis.  BLAKE2 is also used internally by the winner of the password hashing competition Argon2. </p> <p>The BLAKE2 RFC describes two variants (blake2b and blake2s) with different output lengths. While BLAKE2 is considered secure, it is not included in the recommendations of NSA, BSI, or ANSSI.  As a result, there are no specific recommendations for its use by these authorities.  </p>"},{"location":"kb-articles/hash-functions/#ripemd-partially-broken","title":"RIPEMD (partially broken)","text":"<p>RIPEMD is a legacy hash function. The 128-bit variant RIPEMD-128 is considered broken because of its short output length, and practical attacks are possible.  Applications that use RIPEMD-128 should be migrated immediately to a secure hash function like SHA2 or SHA3. </p> <p>RIPEMD-160, with 160-bit outputs, is also considered weak due to its short output length.  It is accepted for legacy use by some institutions but never for future use.  Therefore, applications that rely on RIPEMD-160 should be migrated to a secure hash function like SHA2 or SHA3.  </p>"},{"location":"kb-articles/hash-functions/#further-reading","title":"Further reading","text":"<ul> <li>eHash - Website giving an overview of most known cryptographic hash functions and the status of their cryptanalysis with a special page for the SHA3 competition contenders</li> </ul> <ul> <li>Keylength - Website giving an overview of the recommended cryptographic algorithms according to different authorities and researchers</li> </ul> <ol> <li> <p>https://www.keylength.com/ \u21a9</p> </li> <li> <p>NSA. Commercial National Security Algorithm (CNSA) Suite https://media.defense.gov/2021/Sep/27/2002862527/-1/-1/0/CNSS%20WORKSHEET.PDF \u2013 last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>BSI TR-02102-1: \u201cCryptographic Mechanisms: Recommendations and Key Lengths\u201d Version: 2024-1, https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Publications/TechGuidelines/TG02102/BSI-TR-02102-1.pdf?__blob=publicationFile&amp;v=7 \u2013 last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>ANSSI. GUIDE DE S\u00c9LECTION D\u2019ALGORITHMES CRYPTOGRAPHIQUES, https://cyber.gouv.fr/publications/mecanismes-cryptographiques -  last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>https://doi.org/10.6028/NIST.FIPS.180-4 \u21a9</p> </li> <li> <p>NSA. Commercial National Security Algorithm (CNSA) Suite https://media.defense.gov/2021/Sep/27/2002862527/-1/-1/0/CNSS%20WORKSHEET.PDF \u2013 last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>BSI TR-02102-1: \u201cCryptographic Mechanisms: Recommendations and Key Lengths\u201d Version: 2024-1, https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Publications/TechGuidelines/TG02102/BSI-TR-02102-1.pdf?__blob=publicationFile&amp;v=7 \u2013 last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>ANSSI. GUIDE DE S\u00c9LECTION D\u2019ALGORITHMES CRYPTOGRAPHIQUES, https://cyber.gouv.fr/publications/mecanismes-cryptographiques -  last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>https://doi.org/10.6028/NIST.FIPS.202 \u21a9</p> </li> <li> <p>BSI TR-02102-1: \u201cCryptographic Mechanisms: Recommendations and Key Lengths\u201d Version: 2024-1, https://www.bsi.bund.de/SharedDocs/Downloads/EN/BSI/Publications/TechGuidelines/TG02102/BSI-TR-02102-1.pdf?__blob=publicationFile&amp;v=7 \u2013 last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>ANSSI. GUIDE DE S\u00c9LECTION D\u2019ALGORITHMES CRYPTOGRAPHIQUES, https://cyber.gouv.fr/publications/mecanismes-cryptographiques -  last accessed Oct 16, 2024\u00a0\u21a9</p> </li> <li> <p>https://doi.org/10.6028/NIST.FIPS.180-4 \u21a9</p> </li> <li> <p>Stevens, M., Bursztein, E., Karpman, P., Albertini, A., Markov, Y. (2017). The First Collision for Full SHA-1. In: Katz, J., Shacham, H. (eds) Advances in Cryptology \u2013 CRYPTO 2017. LNCS, vol 10401. Springer, Cham. https://doi.org/10.1007/978-3-319-63688-7_19. Freely available online: https://eprint.iacr.org/2017/190.pdf \u21a9</p> </li> <li> <p>https://www.ietf.org/rfc/rfc1321.txt \u21a9</p> </li> <li> <p>https://en.wikipedia.org/wiki/Flame_(malware) \u21a9</p> </li> <li> <p>Sasaki, Y., Aoki, K. (2009). Finding Preimages in Full MD5 Faster Than Exhaustive Search. In: Joux, A. (eds) Advances in Cryptology - EUROCRYPT 2009. LNCS, vol 5479. Springer, Berlin, Heidelberg. https://doi.org/10.1007/978-3-642-01001-9_8.  Freely available online: https://iacr.org/archive/eurocrypt2009/54790136/54790136.pdf \u21a9</p> </li> <li> <p>https://datatracker.ietf.org/doc/html/rfc7693.html \u21a9</p> </li> </ol>"},{"location":"kb-articles/key-management-and-keystores/","title":"Key management and keystores","text":"<p>Cryptographic key management encompasses the processes and systems used to securely generate, store, distribute, renew, and revoke cryptographic keys to protect sensitive data.  Effective key management ensures the confidentiality, integrity, and authenticity of data throughout the key lifecycle.</p> <p>As one of the most complex and critical aspects of cryptography, key management is essential for mitigating security risks, including data breaches, unauthorized access, and compliance failures.  Proper key management involves choosing secure key generation methods, enforcing strict access controls, and ensuring keys are rotated or retired when necessary to maintain security over time.</p> <p>Keystores play a fundamental role in securing cryptographic keys by enforcing access controls, encryption policies, and lifecycle management.  They are commonly used in software applications and cloud services to protect private keys used for TLS encryption, digital signatures, and authentication.  Dedicated hardware security modules (HSMs) provide an additional layer of security by managing cryptographic keys in a tamper-resistant environment and performing secure cryptographic operations. The security of a keystore directly impacts cryptographic confidentiality, integrity, and availability, making proper key storage and access management essential for preventing key exposure and unauthorized access.</p> <p>This document provides a comprehensive overview of key management, keystores, and best practices for secure implementation.  It highlights the importance of robust key security, explores potential vulnerabilities, and outlines practical strategies for ensuring cryptographic integrity in various applications and industries.</p>"},{"location":"kb-articles/key-management-and-keystores/#introduction-to-key-management","title":"Introduction to key management","text":"<p>Key management is the process of managing the entire lifecycle of cryptographic keys, from generation to destruction, to ensure the security and integrity of sensitive data and applications.  Effective key management is essential for protecting sensitive data, ensuring the security of various applications and systems, and maintaining compliance with regulatory requirements. </p> <p>Effective key management is crucial for maintaining the security and trust of various systems and applications, including:</p> <ul> <li>Securing TLS communications. </li> <li>Encrypting data in databases.</li> <li>Managing API keys for applications.</li> </ul>"},{"location":"kb-articles/key-management-and-keystores/#key-management-lifecycle","title":"Key management lifecycle","text":"<p>The key management lifecycle encompasses the entire lifespan of a cryptographic key, from its generation to its eventual destruction, and includes the following stages:</p> <ol> <li>Generation: creating cryptographic keys using secure algorithms and entropy sources to ensure their uniqueness, strength, and resistance to attacks.</li> <li>Distribution: securely transferring cryptographic keys to authorized entities using methods such as key exchange protocols or public key infrastructure (PKI) to maintain confidentiality and integrity</li> <li>Storage: protecting cryptographic keys in secure environments, such as keystores or Hardware Security Modules (HSMs), to prevent unauthorized access and ensure compliance with security policies.</li> <li>Usage: making cryptographic keys available for their intended cryptographic operations, such as encryption, decryption, authentication, and digital signatures, while ensuring proper access control.</li> <li>Rotation: replacing old cryptographic keys with new ones for future use to limit exposure. Key rotation does not typically re-encrypt existing data; re-keying is required to apply a new key to past data.</li> <li>Key Revocation: marking cryptographic keys as no longer trusted due to expiration, policy updates, or suspected compromise. Revoked keys should no longer be used for cryptographic operations and should be removed from active use.</li> <li>Key Destruction: securely erasing cryptographic keys that are no longer needed to prevent recovery and misuse. Secure destruction should follow recognized erasure standards (e.g., NIST SP 800-88 for digital media or zeroization for HSMs).</li> </ol>"},{"location":"kb-articles/key-management-and-keystores/#common-key-management-issues","title":"Common key management issues","text":"<p>Mismanagement of cryptographic keys can introduce significant security vulnerabilities. Common mistakes include:</p> <ul> <li>Storing keys in plaintext.</li> <li>Using outdated or insecure key generation methods.</li> <li>Failing to restrict access to unauthorized personnel.</li> <li>Neglecting to renew or revoke keys when necessary. </li> </ul> <p>These weaknesses can lead to data breaches, unauthorized decryption of sensitive data, and compromised systems. For example, hardcoded keys in IoT devices have been exploited to access encrypted communications and tamper with device functionality.  Implementing robust key management practices helps mitigate these risks and ensures compliance with security standards.</p>"},{"location":"kb-articles/key-management-and-keystores/#key-management-best-practices","title":"Key management best practices","text":"<p>To mitigate risks and ensure reliable cryptographic operations, organizations should follow best practices, such as:</p> <ul> <li>Adhering to established standards like NIST SP 800-57.</li> <li>Utilizing secure storage solutions, including Hardware Security Modules (HSMs).  </li> <li>Regularly rotating keys to prevent expiration and maintain security.</li> </ul> <p>Regular key rotation\u2014sometimes referred to as key refreshing or rekeying\u2014reduces the risk of long-term exposure by replacing old cryptographic keys with new ones at scheduled intervals, ensuring continued security. This practice: </p> <ul> <li>Minimizes the impact of key compromise.</li> <li>Prevents cryptographic weaknesses caused by key overuse.</li> <li>Ensures compliance with regulatory and industry security standards.</li> <li>Enhances system security by adapting to evolving threats.</li> </ul> <p>By adopting these best practices, organizations can protect critical systems, ensure compliance, and maintain business continuity. Additionally, engineers will gain actionable guidance for maintaining secure implementations.</p>"},{"location":"kb-articles/key-management-and-keystores/#introduction-to-keystores","title":"Introduction to keystores","text":"<p>To prevent security vulnerabilities caused by poor key management, organizations rely on keystores as secure storage mechanisms for cryptographic keys, certificates, and other sensitive credentials.  Keystores play a critical role in protecting these assets from unauthorized access, misuse, or compromise.  As a fundamental component of cryptographic key management, keystores provide a secure environment for storing and accessing encryption keys used in applications, network security protocols, and authentication systems.</p> <p>Keystores enforce key access controls, encryption policies, and secure storage practices. They are commonly integrated into software applications and cloud services to safeguard private keys used for TLS encryption, digital signatures, and authentication mechanisms.  Additionally, organizations may use hardware security modules (HSMs) as a more advanced solution, offering tamper-resistant storage and secure cryptographic operations beyond what traditional software-based keystores provide.</p> <p>The security of a keystore is crucial, as it directly impacts the confidentiality, integrity, and availability of cryptographic operations.  Properly configured keystores help mitigate key exposure risks while ensuring that cryptographic keys remain protected throughout their lifecycle.</p>"},{"location":"kb-articles/key-management-and-keystores/#keystore-formats-and-security","title":"Keystore formats and security","text":"<p>Different keystore formats, such as PKCS#12, JKS, BKS, and BCFKS, offer varying levels of security, compatibility, and compliance with industry standards like FIPS 140-2 and NIST SP 800-57.  Using outdated or misconfigured keystores can introduce serious vulnerabilities, including weak encryption, improper key handling, and lack of integrity protection.</p> <p>To mitigate these risks, organizations must:</p> <ul> <li>Select secure keystore formats.</li> <li>Enforce strong encryption settings.</li> <li>Follow best practices for key storage and access control. </li> </ul> <p>The following sections provide an in-depth examination of different keystore types, their security implications, and recommendations for secure implementation within a cryptographic key management strategy.</p>"},{"location":"kb-articles/key-management-and-keystores/#legacy-keystores","title":"Legacy keystores","text":"<p>Some keystore formats that were once widely used have become obsolete due to weak encryption, poor key protection, and deprecated cryptographic algorithms.  Legacy keystores such as JKS, JCEKS, and older versions of BKS often rely on insecure hashing functions such as SHA-1 or MD5, outdated encryption schemes like RC4 or 3DES, and weak password protection mechanisms.</p> <p>These weaknesses make legacy keystores highly vulnerable to brute-force attacks, unauthorized modifications, and key exposure.  Organizations still relying on them should assess their security risks and migrate to modern keystore formats that offer stronger encryption, integrity protection, and compliance with current security standards.</p>"},{"location":"kb-articles/key-management-and-keystores/#java-keystore-jks","title":"Java KeyStore (JKS)","text":"<p>The Java KeyStore (JKS) was the original keystore format introduced in the Sun Java provider.  It used a custom encryption scheme where SHA-1 was employed to derive a keystream for encrypting keys, verifying, and keystore integrity. </p> <p>Significant vulnerabilities have been identified in JKS.  The encryption method exposes predictable patterns when the same password and salt are reused, allowing attackers to exploit overlaps between stored keys. Its reliance on SHA-1, which is now considered insecure,enables efficient dictionary-based cracking, as only a single SHA-1 computation is needed to verify guesses against the keystream.</p> <p>Due to these significant vulnerabilities, JKS is no longer considered secure and should not be used in modern applications.</p>"},{"location":"kb-articles/key-management-and-keystores/#java-cryptography-extension-keystore-jceks","title":"Java Cryptography Extension KeyStore (JCEKS)","text":"<p>JCEKS was introduced as a successor to the legacy Java KeyStore (JKS), offering improved security through Triple-DES (TDEA/3DES) encryption to protect serialized keys written to disk.  While this was a step forward compared to JKS, 3DES is now considered obsolete and insecure for most applications due to its 64-bit block size, which limits security in high-volume scenarios and makes it vulnerable to collision attacks.</p> <p>JCEKS also relies on password-based encryption, deriving a 3DES key and initialization vector (IV) using a 64-bit salt and repeated hashing with MD5.  In Java versions prior to 2018, the key derivation process used only 20 iterations of MD5, significantly weakening the keystore\u2019s security.  Later versions increased this iteration count to 200,000, but the reliance on MD5, a cryptographically weak hash function, remains a fundamental issue.</p> <p>While JCEKS improved upon JKS by introducing encryption, its reliance on outdated cryptographic algorithms like 3DES and MD5 makes it unsuitable for protecting sensitive keys in modern environments.</p>"},{"location":"kb-articles/key-management-and-keystores/#bouncy-castle-keystore-bks","title":"Bouncy Castle KeyStore (BKS)","text":"<p>The Bouncy Castle KeyStore (BKS) is a keystore format provided by the Bouncy Castle cryptographic library, primarily used in Java applications requiring advanced cryptographic features and flexibility.  Initially designed as an alternative to the Java KeyStore (JKS), BKS has faced notable security challenges over time.</p> <p>BKS-V2, the second version of Bouncy Castle KeyStore, addressed several security concerns present in its predecessor, including replacing the weaker integrity check in BKS-V1 with a 160-bit HMAC.  However, BKS-V2 continues to rely on obsolete cryptographic practices, including Triple-DES (3DES) encryption and PBKDFs based on SHA-1.  These cryptographic methods are considered insufficient for modern security requirements due to vulnerabilities in 3DES and the weakened status of SHA-1.  For example, an analysis of outdated keystore mechanisms highlights the risks of using legacy cryptographic components in keystores like BKS.</p> <p>To ensure security when using BKS, avoid BKS-V1 entirely due to critical vulnerabilities in its integrity verification.  If BKS is required, use BKS-V2 with the latest version of the Bouncy Castle library and verify that cryptographic configurations meet modern standards.  Whenever possible, consider transitioning to the Bouncy Castle FIPS KeyStore (BCFKS), which implements AES for encryption and PBKDF2 with HMAC-SHA512 for key derivation, offering enhanced security suitable for contemporary applications.</p>"},{"location":"kb-articles/key-management-and-keystores/#bouncy-castle-uber-keystore","title":"Bouncy Castle UBER Keystore","text":"<p>The Bouncy Castle UBER keystore format provides a lightweight approach to storing cryptographic keys and certificates, designed for simpler use cases compared to formats like BKS or PKCS#12.  While UBER shares some design similarities with BKS, it also introduces unique choices in encryption and integrity protection.</p> <p>UBER uses the Twofish cipher to encrypt the entire keystore.  Twofish, a finalist in the AES competition, has been subjected to extensive cryptanalysis since the late 1990s, and no significant weaknesses have been identified. Like BKS, UBER employs a Password-Based Key Derivation Function (PBKDF) for private key encryption, but the iteration count remains low (randomly set between 1024 and 2047).  These parameters are considered outdated by modern cryptographic standards, leaving UBER less resistant to brute-force attacks.</p> <p>For integrity verification, UBER calculates a SHA-1 hash of the keystore contents before encryption.  After decryption, this hash is used to check the keystore\u2019s integrity.  This \u201cMAC-then-encrypt\u201d design is discouraged in modern cryptography because it is susceptible to padding oracle attacks, where subtle differences in error messages or execution time can leak information to attackers.</p> <p>UBER keystores are not recommended for securing sensitive cryptographic material due to their reliance on outdated practices like low iteration counts, SHA-1, and MAC-then-encrypt.  If UBER is required for lightweight use cases, ensure the latest Bouncy Castle library is used and assess the security risks carefully.  For critical applications, consider transitioning to more secure formats such as BCFKS or PKCS#12, which offer stronger encryption algorithms and better alignment with modern cryptographic standards.</p>"},{"location":"kb-articles/key-management-and-keystores/#modern-keystores","title":"Modern keystores","text":"<p>Modern keystore formats are designed to address the weaknesses of legacy implementations by incorporating stronger encryption, integrity protection, and improved key management practices.  Formats such as PKCS#12, BCFKS, and hardware-backed keystores provide AES-based encryption, higher iteration counts for password protection, and robust mechanisms for access control and integrity verification.</p> <p>These enhancements reduce the risk of key exposure, unauthorized modifications, and brute-force attacks, making modern keystores suitable for securing sensitive cryptographic material in compliance with current security standards.</p>"},{"location":"kb-articles/key-management-and-keystores/#bouncy-castle-fips-keystore-bcfks","title":"Bouncy Castle FIPS KeyStore (BCFKS)","text":"<p>The Bouncy Castle FIPS KeyStore (BCFKS) is a modern keystore format introduced by the Bouncy Castle cryptographic library to meet contemporary security standards.  Unlike older formats like JKS or BKS, BCFKS is designed with robust cryptographic practices and is FIPS 140-2 Level 1 compliant, making it suitable for high-assurance environments.</p> <p>BCFKS uses AES in CCM mode for encrypting stored keys, providing both confidentiality and integrity.  For password-based key derivation, it implements PBKDF2 with HMAC-SHA512, allowing for high iteration counts to resist brute-force attacks.  These cryptographic choices ensure strong security and avoid the vulnerabilities present in earlier keystore formats, such as reliance on weak ciphers or low iteration counts.</p> <p>BCFKS is recommended for modern applications requiring strong cryptographic protections.  It is particularly suited for environments that require compliance with FIPS 140-2, such as financial systems or government applications.  When using BCFKS, ensure that the latest version of the Bouncy Castle library is applied to maintain alignment with current cryptographic standards.</p>"},{"location":"kb-articles/key-management-and-keystores/#pkcs12","title":"PKCS#12","text":"<p>PKCS#12 is a widely used standard for securely storing private keys, certificates, and other sensitive data.  It became the default keystore format in Java in version 9 and is now mandatory for all Java implementations.  Unlike legacy keystore formats such as JKS and JCEKS, PKCS#12 supports modern encryption algorithms like AES-256, making it a more secure choice when properly configured.</p> <p>The cryptographic defaults used by PKCS#12 depend on the Java version.  Prior to Java 9, weaker algorithms like RC2 and RC4 were supported and often used as defaults, alongside lower iteration counts (such as 1024 iterations) for password-based encryption. These weak configurations leave keystores vulnerable to brute-force and cryptographic attacks.  For instance, RC4 support was marked as a legacy algorithm in 2022, causing a warning to be printed from the Java 19 release. </p> <p>Starting with Java 9, PKCS#12 configurations align more closely with modern security standards, including the use of stronger encryption algorithms, like AES-256, and improved iteration counts for key derivation.  Using recent Java versions ensures that insecure defaults are avoided, but keystores created or maintained with older Java runtimes may still rely on outdated and vulnerable cryptographic settings.</p> <p>To ensure the security of PKCS#12 keystores, it\u2019s critical to use an up-to-date Java version and verify the cryptographic configuration, such as the choice of encryption algorithm and iteration count.  This is especially important when migrating or managing keystores originally created with older versions of Java.</p>"},{"location":"kb-articles/key-management-and-keystores/#risk","title":"Risk","text":"<p>Inadequate cryptographic key management poses severe security risks, compromising the confidentiality, integrity, and authenticity of sensitive data.  Exposure of cryptographic keys can result in large-scale data breaches and unauthorized access to encrypted information.  Weak key practices, such as insecure key derivation methods, failure to renew or revoke keys, or poor storage practices, increase the risk of exploitation and compromise.</p> <p>The effort required to exploit weak key management varies depending on the specific vulnerability. For example:</p> <ul> <li>Insecure key storage - Attackers can extract plaintext keys stored in unprotected databases or files.</li> <li>Poorly generated keys -  Exploiting weak keys may require brute-forcing, which varies in complexity based on the algorithm and key length.</li> <li>Inadequate randomness - predictable patterns can emerge when keys are generated with insufficient randomness or reused, making cryptographic systems more vulnerable to targeted attacks.</li> </ul>"},{"location":"kb-articles/key-management-and-keystores/#consequences","title":"Consequences","text":"<p>Real-world examples highlight the consequences of poor key management. For instance:</p> <ul> <li>Static or poorly protected keys - Extracting and misusing keys from software or devices can enable impersonation, unauthorized access, malicious actions, and compromise of data integrity.</li> <li>Weak encryption schemes -  Inadequate encryption in keystores or insufficient randomness during key generation can compromise private keys, undermining the integrity of cryptographic systems.</li> <li>Failure to rotate or revoke keys -  Allows attackers to maintain long-term unauthorized access to sensitive data or systems.</li> </ul> <p>Weak key management has led to real-world security breaches. Notable examples include: </p> <ul> <li>CVE-2017-10356 exposed private keys in Java KeyStore (JKS) due to weak encryption.</li> <li>CVE-2018-5382 revealed flaws in Bouncy Castle KeyStore (BKS) integrity protection.</li> </ul> <p>These vulnerabilities highlight the importance of adopting modern key management strategies to mitigate the risks associated with outdated keystores and weak key management practices.</p>"},{"location":"kb-articles/key-management-and-keystores/#security-recommendations","title":"Security recommendations","text":"<p>To mitigate the risks associated with cryptographic key mismanagement, organizations should adhere to established best practices to securely generate, store, and rotate cryptographic keys.  Follow standards such as NIST SP 800-57, and avoid deprecated or insecure methods, such as storing keys in plaintext, hardcoding keys in source code, or using weak key generation algorithms.  Ensure that secure encryption methods, such as AES-CCM for storage and HMAC-SHA256 or HMAC-SHA512 for authentication, are used in place of legacy algorithms like SHA-1, which no longer provide sufficient security.</p> <p>HSMs can enhance key management by providing tamper resistance and secure key handling, with options like on-premises devices or cloud-based solutions such as AWS KMS and Azure Key Vault.  These solutions offer scalable, cost-effective alternatives for secure key management.</p> <p>Ensure your key management system or provider is using up-to-date cryptographic methods.  If your provider is using outdated cryptography, require them to adopt modern cryptographic standards.  Keystores like JKS and BKS-V1 are considered insecure due to weak encryption, like SHA-1, and shouldn\u2019t be used.  Instead, PKCS#12 with strong encryption algorithms like AES-256, JCEKS with recent Java versions, or Bouncy Castle FIPS for FIPS-compliant systems are recommended for secure key storage.  Specify required versions to avoid known vulnerabilities. Cloud-based KMS options like AWS KMS and Azure Key Vault are managed and automatically updated by the service provider.</p> <p>Beyond selecting a secure keystore format, organizations should enforce access control measures to prevent unauthorized access to cryptographic keys. This includes:</p> <ul> <li>Restricting keystore access based on user roles and permissions.</li> <li>Monitoring and logging keystore activity to detect unauthorized access.</li> <li>Ensuring only approved users and systems can manage stored keys.</li> </ul> <p>Weak or misconfigured keystore policies can expose sensitive keys, even if a secure format is used.  Proper configuration, combined with authentication and monitoring, helps reduce these risks.</p> <p>Implement proper access control and monitoring to ensure that only authorized personnel and systems can use or manage cryptographic keys.  Enforce key rotation and revocation policies to reduce the risk of long-term key exposure, ensuring that compromised keys are quickly rendered ineffective.</p> <p>Additionally, adopt modern cryptographic protocols like TLS 1.3, which integrate advanced key management practices and mitigate vulnerabilities present in older versions.</p> <p>Replace outdated encryption methods with stronger alternatives, such as AES-GCM, to enhance data security.</p> <p>By implementing these measures, organizations can significantly reduce their risk exposure and enhance the security and reliability of their cryptographic systems.</p>"},{"location":"kb-articles/key-management-and-keystores/#sources","title":"Sources","text":"<ul> <li>NIST Key Management Guidelines overview</li> <li>NIST SP 800-57 Part 1 Rev. 5, Recommendation for Key Management: Part 1 \u2013 General</li> <li>CVE-2017-10356: Weak Encryption in Java KeyStore</li> <li>CVE-2018-5382: Bouncy Castle BKS Keystore Weak HMAC</li> <li>ISO/IEC 11770-1:2010, Information technology \u2014 Security techniques \u2014 Key management, Part 1: Framework</li> <li>Java Keystores Explained</li> <li>Mind Your Keys? A Security Evaluation of Java Keystores</li> <li>Advanced Encryption Standard process</li> <li>Adding RC2/RC4 to list of legacy algorithms in JDK</li> <li>Consolidated JDK 19 Release Notes</li> <li>Knowledge base: Hash functions</li> </ul>"},{"location":"kb-articles/lack-of-strong-unforgeability/","title":"Lack of strong unforgeability","text":"<p>Higher-level cryptographic protocols, such as those used in secure communication or blockchain systems, rely on foundational building blocks like digital signature schemes.  It\u2019s crucial to assess whether these schemes meet the expected properties.  For example, using a scheme that lacks the \u2018strong unforgeability\u2019 property can open the door to attacks, as demonstrated by some notable cases in cryptocurrency systems.</p>"},{"location":"kb-articles/lack-of-strong-unforgeability/#context","title":"Context","text":"<p>The ability of an attacker to turn valid signatures into a new, still valid signature can compromise the security of a higher-level application or protocol.  Whether this is the case depends on the specific security requirements of the higher-level system.</p> <p>There are two formal definitions of \u201cunforgeability\u201d in the cryptographic literature: </p> <ol> <li>Existential Unforgeability under Chosen Message Attacks (EUF-CMA) says that it should be impossible for an adversary to produce a valid signature on a message that has not been previously signed by the legitimate signer, even if the adversary can obtain signatures for any messages they specify.</li> <li>Strong Unforgeability under Chosen Message Attacks\u201d (SUF-CMA) states that an adversary cannot produce a new valid signature for a message, even if they already have access to valid signatures for that message. Like in EUF-CMA, the adversary can submit messages of its choice to a signing oracle and will receive valid signatures in return.</li> </ol> <p>Many signature schemes, both theoretical and practical, target only EUF-CMA.  For example, if a verification algorithm strips off trailing zeros of a signature, it would be trivial to violate SUF-CMA by adding zeros to create a \u201cnew\u201d signature, even though breaking EUF-CMA might remain difficult.</p>"},{"location":"kb-articles/lack-of-strong-unforgeability/#severity-and-recommendations","title":"Severity and recommendations","text":"<p>An example of a vulnerability exploiting the lack of SUF-CMA security is transaction malleability in Bitcoin.</p> <p>Formal analysis, often called a \u201cproof of security\u201d in cryptographic literature, of the higher-level protocol establishes which properties are demanded of the underlying building block.  In particular, this analysis establishes whether SUF-CMA is required for the protocol or whether EUF-CMA suffices.  For example, a recent analysis relied on SUF-CMA to argue that the SSH protocol is secure. </p> <p>Alternatively, instead of relying on a formal analysis to confirm that EUF-CMA is adequate, signature schemes can be upgraded to schemes that satisfy the stronger SUF-CMA security.  For example, ECDSA is not SUF-CMA secure. In contrast, EdDSA, as specified in RFC 8032, achieves SUF-CMA security under established cryptographic assumptions. </p> <p>However, it\u2019s important to note that this guarantee only applies to EdDSA implementations that strictly follow RFC 8032. Furthermore, even when deploying an SUF-CMA secure signature scheme, it\u2019s critical to enforce unique encoding of signatures.  Without this, the SUF-CMA guarantees are compromised.</p>"},{"location":"kb-articles/lack-of-strong-unforgeability/#resources","title":"Resources","text":"<p>Refer to Matt Green\u2019s blog post on EUF-CMA and SUF-CMA.</p>"},{"location":"kb-articles/message-authentication-codes/","title":"Message Authentication Codes (MACs)","text":"<p>Message Authentication Codes (MACs) are essential tools in cryptography for ensuring the integrity and authenticity of data in secure communications.  A MAC is generated by processing a message and a secret key through a cryptographic function, producing a fixed-length tag.  This tag enables the recipient to verify the message\u2019s authenticity by detecting unauthorized modifications and confirming that it originated from a trusted source with access to the shared secret key.</p> <p>Using secure MAC constructions such as HMAC<sup>1</sup> and MAC modes like CMAC<sup>2</sup> helps prevent unauthorized data modification.  Weak MAC implementations can expose vulnerabilities such as data manipulation, unauthorized access, and compliance violations.</p> <p>By following best practices\u2014such as adopting standardized MAC algorithms, managing cryptographic keys securely, and ensuring protocols are up-to-date\u2014organizations can significantly reduce their exposure to security risks.  Secure MAC implementation helps prevent unauthorized message modification and data corruption in communication protocols like TLS, IPsec, and SSH, and supports compliance with security standards and regulations.</p>"},{"location":"kb-articles/message-authentication-codes/#introduction-to-macs","title":"Introduction to MACs","text":"<p>Message Authentication Codes (MACs) play a vital role in secure cryptographic systems.  They work by applying a cryptographic function\u2014typically a hash function (HMAC) or block cipher (CMAC)\u2014to a combination of a message and a secret key, generating a tag that allows recipients to verify message authenticity and integrity.</p> <p>MACs are used in secure communication protocols such as IPsec (for secure network communication), SSH (for secure remote access), and TLS 1.3 (for secure web browsing) to protect message authentication and integrity.  In TLS 1.3, MACs are also used within key derivation and handshake authentication. Common MAC constructions include HMAC, CMAC, GMAC, and Poly1305. </p> <p>Improper MAC implementation\u2014such as using deprecated algorithms, mismanaging keys, or introducing side-channel vulnerabilities\u2014can weaken data security.  Side-channel vulnerabilities leak information through the physical implementation of a cryptographic system, such as power consumption or timing.  This allows attackers to manipulate messages or compromise authentication mechanisms. Ensuring proper MAC implementation is crucial for secure data transmission, protecting sensitive information, and supporting compliance with cryptographic security standards.</p>"},{"location":"kb-articles/message-authentication-codes/#risk","title":"Risk","text":"<p>The severity of insecure MAC implementations varies depending on the context.  Vulnerabilities stem from incorrect usage, poorly designed constructions, and insecure implementation.</p> <p>Several attacks exploit timing irregularities, allowing attackers to compromise data integrity or authenticity.  Other risks include improper key management, using obsolete algorithms, or failure to enforce strict verification, such as constant-time comparison or proper nonce handling.  These weaknesses can make MACs ineffective against forgery or tampering, enabling attackers to bypass integrity checks, alter data, or expose sensitive information.</p> <p>CBC-MAC, when used with variable-length messages, is vulnerable to forgery attacks. While it provides strong security for fixed-length inputs, variable-length messages introduce weaknesses that attackers can exploit.  NIST acknowledges these flaws in SP 800-38B<sup>3</sup> and recommends CMAC as a secure alternative. Practical attacks exploiting these issues have been demonstrated in TLS and DTLS, with some known since 2002.</p> <p>Some of these attacks exploit predictable patterns between message blocks, bypassing integrity checks and enabling tampering or forgery.  Attacks such as these emphasize the importance of addressing timing irregularities and adhering to robust cryptographic standards.  While such attacks often require specialized expertise and precise timing measurements, they can severely impact systems with flawed implementations.  To mitigate these risks, organizations should adopt recommended MAC algorithms, such as HMAC-SHA256 or AES-CMAC, implement modern protocols like TLS 1.3, and rely on well-tested cryptographic libraries designed to prevent such vulnerabilities.</p>"},{"location":"kb-articles/message-authentication-codes/#security-recommendations","title":"Security recommendations","text":"<p>To mitigate risks associated with insecure MAC implementations, follow these straightforward recommendations:</p> <ol> <li>Adopt Standardized and Secure MAC Algorithms: Use recommended algorithms like HMAC-SHA256 or AES-CMAC, which are supported by cryptographic standards and provide strong security guarantees.  </li> <li>Avoid Deprecated and Weak Algorithms: Discontinue the use of insecure algorithms, such as MD5-based or SHA1-based HMAC, or raw CBC-MAC without additional safeguards. These algorithms are vulnerable to known attacks and should not be used in modern systems.</li> </ol> <p>The use of HMAC and CMAC are recommended in NIST SP 800-224, amongst others. NIST-approved versions of HMAC are listed in Table 2 in NIST SP 800-224, which also approves versions of CMAC using AES.  By following these recommendations, organizations can effectively address the risks associated with insecure MAC usage while maintaining a practical and manageable approach.</p>"},{"location":"kb-articles/message-authentication-codes/#sources","title":"Sources","text":"<ul> <li>NIST SP 800-38A: Recommendation for Block Cipher Modes of Operation: Methods and Techniques</li> <li>NIST Special Publication 800-38B: Recommendation for Block Cipher Modes of Operation: The CMAC Mode for Authentication </li> <li>ISO/IEC 9797-1:2011, Information technology \u2014 Security techniques \u2014 Message Authentication Codes (MACs), Part 1: Mechanisms using a block cipher </li> <li>NIST SP 800-224 (Initial Public Draft) Keyed-Hash Message Authentication Code (HMAC): Specification of HMAC and Recommendations for Message Authentication </li> <li>ISO/IEC 9797-1:2011, Information technology \u2014 Security techniques \u2014 Message Authentication Codes (MACs), Part 2: Mechanisms using a dedicated hash-function</li> <li>Lucky Thirteen: Breaking the TLS and DTLS Record Protocols: IEEE Security &amp; Privacy 2013 </li> <li>Lucky Microseconds: A Timing Attack on Amazon\u2019s s2n Implementation of TLS </li> <li>CBC-MAC: Wikipedia Overview </li> <li>Security Flaws Induced by CBC Padding Applications to SSL, IPSEC, WTLS </li> <li>Why I hate CBC-MAC</li> </ul> <ol> <li> <p>HMAC is specified in FIPS 198-1, which is currently being converted into a NIST Special Publication 800-224. NIST SP 800-224 is in an initial public draft state, and FIPS 198-1 will be withdrawn once the final version of NIST SP 800-224 is published.\u00a0\u21a9</p> </li> <li> <p>CMAC is a block cipher mode specified in NIST SP 800-38B, \u201cRecommendation for Block Cipher Modes of Operation: the CMAC Mode for Authentication\u201d.\u00a0\u21a9</p> </li> <li> <p>Section 3, Introduction, page 1.\u00a0\u21a9</p> </li> </ol>"},{"location":"kb-articles/password-based-key-derivation/","title":"Password-based key derivation","text":"<p>A password-based key-derivation function (PBKDF) accepts a password as input and generates a cryptographic key for use in encryption, authentication, or other cryptographic purposes.  It is specifically designed to compensate for the typically low entropy of passwords.  NIST recommends PBKDF2, standardized in RFC 2898 and PKCS#5.</p>"},{"location":"kb-articles/password-based-key-derivation/#detailed-description","title":"Detailed description","text":"<p>Passwords or passphrases are often used as a convenient means of deriving cryptographic keys.  However, using a passphrase as a cryptographic key is risky because passwords usually are not sufficiently random.  In practice, this means passwords are vulnerable to dictionary attacks, where attackers systematically try passwords based on their frequency of use. Additionally, cryptographic primitives typically guarantee security only when used with random keys, meaning using passwords as cryptographic keys can compromise the entire cryptographic protocol. </p> <p>To mitigate these risks, a password needs to be passed through a dedicated PBKDF before being fed into any other cryptographic algorithm.  In addition, PBKDFs need to be used to store passwords. Instead of storing passwords in plain text, the PBKDF output should be stored and compared during authentication.</p> <p>PBKDFs are essentially slow, salted hash functions.  That means, a salt\u2014a random string\u2014is added to the password input of the hash function to slow down brute-force search attacks enough to make them uneconomical.  More concretely, it forces an attacker to target each password with a dedicated brute-force attack instead of simply pre-computing common (password, PBKDF output) pairs and checking against this table.  The salt provides domain separation between users and does not need to be kept secret. Given that PBKDFs take in user passwords, it is often not critical that they run as fast as possible.  Put differently, if a PBKDF runs for 100 milliseconds, this slowdown is barely notable to a user, but slows down a brute force attack considerably.  Of course, this comparison is premised on the assumption that the PBKDF implementation relied upon by a server and by an adversary run in a comparable amount of time.</p> <p>A common design principle used for PBKDF is to repeatedly apply a so-called \u201cpseudorandom function\u201d (PRF) for many iterations using the password as the key. A PRF is a function that takes a random key and an arbitrary other input and outputs a string that is indistinguishable from random.</p>"},{"location":"kb-articles/password-based-key-derivation/#pbkdf-choices-and-alternatives","title":"PBKDF choices and alternatives","text":"<p>The only PBKDF currently approved by NIST is PBKDF2, standardized in RFC 2898 and PKCS#5.  A developer using PBKDF2 must choose parameter values for the salt, the PRF, and the number of iterations, i.e. the number of times the PRF will be applied to the password when deriving the key.</p> <p>The RFC 2898 suggests (in Section 4.1) that the salt be (or contain) a 64 bit pseudorandom value.  This makes collisions (i.e. occasions that two stored passwords use the same salt) unlikely.  By the birthday bound, we would expect a collision after a bit more than 4 billion passwords.  It is advisable to pick a larger salt of at least 128 bits, which has very little additional computational cost but prevents this potential attack vector effectively.</p> <p>The PRF mentioned in the specification is HMAC-SHA-1, and in many libraries this is the only choice.  However, using HMAC-SHA-256 or HMAC-SHA-512 increases the memory requirements, which also increases the cost for an attacker wishing to attack using hardware-based password crackers based on GPUs or ASICs.</p> <p>The current NIST Guidelines suggest a minimum of 10,000 iterations of the PRF. As of May 2023, OWASP\u2019s Password Storage Cheat Sheet recommends 600,000 iterations and the use of HMAC-SHA-256.</p> <p>A modern PBKDF is the Argon2 family of password-hashing functions.  As of May 2023, OWASP\u2019s Password Storage Cheat Sheet recommends the use of \u201cArgon2id with a minimum configuration of 19 MiB of memory, an iteration count of 2, and 1 degree of parallelism.\u201d</p> <p>In some settings, \u201cpeppering\u201d may be used to enhance password security.  Refer to the OWASP guide for details.</p> <p>The strongest protection for applications relying on passwords is provided by deploying an oblivious PRF.<sup>1</sup> Refer to RFC 9497 for an introduction.  However, the use of OPRFs introduces additional interaction between a server and a client, are not yet standardized by the IETF and efficient instantiations are only known from assumptions that are not secure against quantum computers.<sup>2</sup></p> <ol> <li> <p>In some applications, primitives like password-based key exchange may also be relied on directly.\u00a0\u21a9</p> </li> <li> <p>Albrecht, M. R., Davidson, A., Deo, A., &amp; Gardham, D. (2024). Crypto Dark Matter on the Torus - Oblivious PRFs from Shallow PRFs and TFHE. In M. Joye, &amp; G. Leander, EUROCRYPT 2024, Part VI (pp. 447\u2013476). : Springer, Cham. https://eprint.iacr.org/2023/232 \u21a9</p> </li> </ol>"},{"location":"kb-articles/preparing-for-fips-validation/","title":"Preparing for FIPS validation with AQtive Guard","text":"<p>Achieving FIPS 140-2/3 validation (or accreditation) is a rigorous process that confirms your cryptographic modules meet stringent security requirements for use in sensitive federal environments.  AQtive Guard (AQG) is an indispensable tool that can significantly streamline and strengthen your organization\u2019s preparation for FIPS validation.</p>"},{"location":"kb-articles/preparing-for-fips-validation/#why-use-aqtive-guard-for-fips","title":"Why use AQtive Guard for FIPS?","text":"<p>The FIPS validation journey requires not only using approved algorithms and key lengths (which AQG excels at verifying), but also independent validation of your cryptographic implementations. AQG empowers you to build a robust foundation for this process.</p> <p>Thorough internal preparation before official submission is crucial for FIPS 140 validation.  By leveraging AQG for extensive testing and documentation, you can significantly minimize findings before engaging a NIST-accredited testing lab.  This directly reduces validation costs, time, and the need for iterative re-submissions, leading to a much smoother and faster path to validation.</p>"},{"location":"kb-articles/preparing-for-fips-validation/#step-by-step-fips-preparation","title":"Step-by-step FIPS preparation","text":"<p>Here\u2019s a recommended step-by-step process for leveraging AQG in your FIPS validation preparation.</p>"},{"location":"kb-articles/preparing-for-fips-validation/#step-1-understand-your-fips-scope-and-requirements","title":"Step 1: Understand Your FIPS scope and requirements","text":"<p>Before you begin scanning for FIPS validation, first clearly define your scope. This involves identifying each system\u2019s cryptographic boundary.  Once these boundaries are identified, pinpoint the specific applications, systems, and cryptographic modules within them that must comply with FIPS 140-2/3.</p> <p>Within these defined boundaries, pinpoint the specific applications, systems, and cryptographic modules that need to comply with FIPS 140-2/3. This will guide your AQG scanning strategy.</p> <p>Important</p> <p>FIPS 140-2/3 validation applies to a specific cryptographic module, not necessarily entire applications or networks. Your validation will focus on proving that where cryptography is required, it is performed by a FIPS-validated module.</p>"},{"location":"kb-articles/preparing-for-fips-validation/#step-2-ingest-relevant-assets-into-aqtive-guard","title":"Step 2: Ingest relevant assets into AQtive Guard","text":"<p>Ensure all applications, network segments, and file systems within your FIPS validation scope are configured in AQG for scanning.  This comprehensive visibility is the starting point for any cryptographic assessment.</p>"},{"location":"kb-articles/preparing-for-fips-validation/#step-3-configure-and-run-scans-with-the-nist-profile","title":"Step 3: Configure and run scans with the NIST profile","text":"<p>The AQtive Guard NIST Profile is your primary tool for FIPS preparation. It\u2019s designed to verify the use of cryptographic best practices derived from the NIST SP 800 series, which are foundational to FIPS validation.</p> <ul> <li>Apply the NIST profile. Apply the NIST Profile to analyze data for applications, networks, and file systems. AQG will analyze cryptographic objects, algorithms, key lengths, and certificate configurations against the rules within this profile.</li> <li>Run scans. Run comprehensive scans across your defined validation scope for any data ingestion that isn\u2019t automated. </li> </ul>"},{"location":"kb-articles/preparing-for-fips-validation/#step-4-analyze-scan-results-and-identify-cryptographic-weaknesses","title":"Step 4: Analyze scan results and identify cryptographic weaknesses","text":"<p>Once your scans are complete, review the AQG reports generated from the NIST Profile. This initial assessment provides immediate visibility into your cryptographic posture.</p> <p>Focus on Critical or High severity issues. Identify and prioritize all flagged issues, particularly those related to:</p> <ul> <li>Weak algorithms. Use of outdated or insecure cryptographic algorithms. FIPS 140-3, Annex A</li> <li>Inadequate key lengths. Keys not meeting minimum security strength requirements, such as keys that are too short. NIST SP 800-131A Rev. 2, NIST SP 800-131A Rev. 3 (Initial Public Draft)</li> <li>Insecure protocols. Use of deprecated TLS/SSL versions. NIST SP 800-52 Rev. 2</li> <li>Weak random number generators or insufficient entropy. Fundamental flaws that can undermine the security of all cryptographic operations that rely on randomness. NIST SP 800-90A Rev. 1, NIST SP 800-90B</li> <li>Expired, invalid, or untrusted certificates, as well as certificate chain issues. NIST SP 800-52 Rev. 2</li> <li>Key management issues, such as key reuse or insecure key storage. NIST SP 800-57 Part 1</li> </ul>"},{"location":"kb-articles/preparing-for-fips-validation/#step-5-prioritize-and-remediate-identified-issues","title":"Step 5: Prioritize and remediate identified issues","text":"<p>Address the cryptographic vulnerabilities identified by AQG. Remediating these foundational issues is a critical prerequisite for FIPS validation.</p> <p>Immediate remediation. Prioritize fixing easily identifiable and high-impact issues such as:</p> <ul> <li>Updating to approved algorithms and sufficient key lengths.</li> <li>Upgrading to secure protocols, such as TLS 1.2/1.3.</li> <li>Replacing expired or invalid certificates.</li> </ul>"},{"location":"kb-articles/preparing-for-fips-validation/#step-6-focus-on-fips-implementation-validation","title":"Step 6: Focus on FIPS implementation validation","text":"<p>This step moves beyond general cryptographic hygiene to address the specific FIPS requirement for validated implementations.  For many federal and regulated environments, using cryptography that has undergone a formal validation process is mandatory.  This validation is performed by the Cryptographic Module Validation Program (CMVP), a joint U.S. (NIST) and Canadian (CCCS) program.  The CMVP tests and validates cryptographic modules against the FIPS 140 series of standards (currently FIPS 140-2 and FIPS 140-3), confirming they meet stringent security requirements for implementing FIPS-approved algorithms and cryptographic functions. </p> <p>Note</p> <p>As of September 21, 2026, all FIPS 140-2 validated cryptographic modules will transition to historical status, requiring a shift to FIPS 140-3 validated modules for new procurements and deployments.</p> <ul> <li>For Java applications (Coming soon). After scanning one or more Java applications in AQG, apply the Java Provider Check rule. If all cryptographic operations within your Java applications successfully pass this rule, it verifies they\u2019re using a FIPS-approved Java Cryptographic Provider operating in FIPS-approved-only mode. This provides strong, direct evidence that those applications meet FIPS compliance requirements under CMVP validation.</li> </ul> <ul> <li>For other applications, network, and file system scans. AQG helps you confirm the use of approved algorithms and sufficient key lengths. This is a necessary foundation for any FIPS validation effort. Address historical modules. Additionally, use these scans to identify any FIPS 140-2 validated modules deemed historical by NIST under NIST SP 800-56A Rev 3. You\u2019ll need to plan for their replacement with currently validated alternatives and document this in your Plan of Action and Milestones (POA\\&amp;M).</li> </ul> <p>For more details on the CMVP and its validation process, refer to the official NIST CMVP website.</p>"},{"location":"kb-articles/preparing-for-fips-validation/#step-7-document-and-prepare-for-validation","title":"Step 7: Document and prepare for validation","text":"<p>FIPS validation requires comprehensive documentation of your module\u2019s design, implementation, and testing. AQG can significantly aid in compiling this evidence.</p> <ul> <li>Export AQG data. Export detailed scan data from AQG to provide a snapshot of your cryptographic inventory, identified issues, and evidence of remediation progress. Reports using this data serve as concrete evidence of your adherence to cryptographic best practices, which an accredited testing lab will review as vendor evidence.</li> <li>Map findings to FIPS requirements (S-13 control). Demonstrating exactly where and how cryptographic functions are performed is a key part of FIPS validation. AQG provides crucial data for this, directly supporting the SC-13 control (a FedRAMP/NIST requirement). Its comprehensive discovery capabilities are invaluable for informing and cross-referencing your cryptographic module inventory, helping you:<ul> <li>Identify where cryptographic functions are being performed, including specific applications, network devices, operating systems.</li> <li>Determine which cryptographic components or libraries are in use.</li> <li>Identify the cryptographic algorithms and key lengths being utilized by these components.</li> <li>Flag if components are using algorithms or modes that are not FIPS-approved or are considered historical.</li> </ul> </li> <li>Maintain a cryptographic module inventory. While AQG identifies usage, you will need to compile a separate inventory of all cryptographic modules, their FIPS validation status (including CMVP certificate numbers), and their role in your system. AQG data helps you populate this inventory accurately and ensures that the modules you claim are FIPS-validated are indeed the ones actively in use and correctly configured.</li> </ul>"},{"location":"kb-articles/preparing-for-fips-validation/#step-8-engage-with-formal-validation","title":"Step 8: Engage with formal validation","text":"<p>Once outstanding issues are addressed and documentation compiled using AQG data, you\u2019re ready for the formal validation of your cryptographic modules.</p> <ul> <li>Formal FIPS Validation Process. For full FIPS validation, your underlying cryptographic modules must be tested by a NIST-Accredited Cryptographic and Security Testing (CST) Laboratory. These laboratories are specifically accredited by the National Voluntary Laboratory Accreditation Program (NVLAP), which is part of NIST. After successful testing, the module receives a FIPS 140-2/3 validation certificate from NIST\u2019s Cryptographic Module Validation Program (CMVP). Your AQG data will be a crucial asset in demonstrating your due diligence and cryptographic posture during this assessment.</li> <li>Leveraging a Third-Party Assessor (3PAO). While 3PAOs are primarily known for their role in FedRAMP assessments, they can also play a valuable part in preparing for or reviewing your FIPS posture. They can help bridge the gap between your internal FIPS readiness and the formal validation process, leveraging your AQG data to assess your cryptographic implementations before engagement with an NVLAP-accredited lab.</li> </ul>"},{"location":"kb-articles/preparing-for-fips-validation/#step-9-implement-continuous-monitoring","title":"Step 9: Implement Continuous Monitoring","text":"<p>FIPS compliance is not a one-time event. It requires ongoing vigilance.</p> <ul> <li>Regular scans. Implement regular, automated scans with AQG using the NIST Profile.</li> <li>Proactive detection. This ensures that new cryptographic issues are quickly identified, and any drift from your compliant posture is immediately flagged, allowing for proactive remediation.</li> </ul>"},{"location":"kb-articles/quantum-threat/","title":"The quantum threat and Post-Quantum Cryptography","text":""},{"location":"kb-articles/quantum-threat/#high-level-description","title":"High-level description","text":""},{"location":"kb-articles/quantum-threat/#context","title":"Context","text":"<p>While quantum computers carry great potential for a range of applications, large quantum computers also threaten the security of digital communications by breaking the asymmetric cryptographic algorithms used today.  These algorithms are core building blocks of essential security protocols, including public-key certificates (such as X.509), secure software updates, and secure communication protocols like Transport Layer Security (TLS), Secure Shell (SSH), and Internet Key Exchange (IKE).  This makes quantum computers a direct threat to digital communication security. To mitigate this risk, several standardization agencies such as NIST recommend transitioning to post-quantum cryptography.</p>"},{"location":"kb-articles/quantum-threat/#business-risk","title":"Business risk","text":"<p>The severity of the \u201cquantum threat\u201d\u2014the risk posed by using quantum-vulnerable cryptography \u2014depends on the application and the expected lifespan of the data it protects.  For example, communication data that must stay confidential for several years needs to be protected more urgently using \u2018post-quantum\u2019 cryptography as attackers can store encrypted communication now and decrypt it later once they gain access to large quantum computers.  The quantum threat might also raise legal concerns. For example, failing to transition to post-quantum cryptography could potentially violate data privacy regulations and other compliance requirements.</p>"},{"location":"kb-articles/quantum-threat/#recommendations","title":"Recommendations","text":"<p>Post-quantum algorithms already exist for the most critical cryptographic primitives such as digital signatures and key establishment.  However, transitioning all protocols and applications to provide security against quantum attackers requires careful and timely planning.  According to NIST, the recommended first step in the post-quantum transition is to create an inventory of cryptographic algorithms in use and their context.  This enables prioritizing which protocols and applications to migrate and in what order. </p> <p>Since these protocols and applications typically rely on cryptographic libraries, it\u2019s crucial to ensure that library maintainers update their software to support post-quantum cryptography.  If this isn\u2019t possible, replacing the library with one that provides post-quantum capabilities may be necessary. Early engagement with software and hardware suppliers is essential to ensure a smooth transition and maintain business continuity.</p>"},{"location":"kb-articles/quantum-threat/#detailed-description","title":"Detailed description","text":"<p>Quantum computers are a kind of computing device, leveraging  the principles of quantum physics, rather than classical electromagnetic physics, for processing data. They store, process, and transmit data using quantum states known as qubits.  Small quantum computers already exist, and some of them can even be accessed by the general public.  While quantum computers carry great potential for a range of applications, large quantum computers also pose a serious threat to digital communication security by being able to break the asymmetric cryptographic algorithms used today.</p>"},{"location":"kb-articles/quantum-threat/#security-risk","title":"Security risk","text":"<p>Large, or cryptographically-relevant quantum computers will be able to break most currently used asymmetric cryptography used to secure data during communication.  This is made possible by Shor\u2019s quantum algorithm, which can efficiently solve the mathematical problems underlying cryptographic algorithms like RSA, ECDSA, and Diffie-Hellman key exchange. </p> <p>Symmetric cryptography is also affected by cryptographically relevant quantum computers but to a lesser degree.  Grover\u2019s quantum algorithm allows quantum computers to search key spaces more efficiently.  However, based on current knowledge, increasing key lengths may provide sufficient protection against this threat.  An example of key length increase would be using AES-192 instead of AES-128. </p> <p>Since communication data can be intercepted and stored today to be decrypted later using cryptographically relevant quantum computers, transitioning to post-quantum algorithms\u2014believed to remain secure even against quantum attackers\u2014is particularly urgent to protect data confidentiality.  Although authentication systems might seem less critical from a cryptographic perspective, initiating their transition is equally important due to their large scale and complex dependencies.</p> <p>While the timeline for the arrival of cryptographically relevant quantum computers remains uncertain, the potential risks are so significant that proactive measures are essential.  This need for prevention has been recognized by numerous governments, including those of the United States (NIST, NSA), Canada, the United Kingdom, Australia, China, South Korea, the European Union, and many EU members, in particular France, Germany, and The Netherlands.  Organizations like the US NCCoE and US CISA have also published recommendations to guide planning and implementation of the post-quantum transition.</p>"},{"location":"kb-articles/quantum-threat/#mitigations-using-post-quantum-cryptography","title":"Mitigations using Post-Quantum Cryptography","text":"<p>Researchers, standardization bodies, and industry have been developing alternative algorithms that are believed to resist quantum attacks.  Unlike the integer factorization and discrete logarithm problems that quantum algorithms like Shor\u2019s can efficiently solve (used in algorithms such as RSA, ECDSA, and ECDH), these new algorithms rely on mathematical problems that quantum computers cannot currently solve efficiently to the best of our knowledge.</p> <p>This shift in security assumptions fundamentally changes the design of cryptographic algorithms, requiring updates to implementations, including standalone libraries, as well as to protocols and applications such as the Transport Layer Security (TLS) protocol.  Notably, NIST has selected four post-quantum algorithms\u2014one for key agreement and three for digital signatures, three of which have already been standardized.  Additionally, NIST has detailed two more algorithms in a special publication.</p> <p>Depending on policy, organizations may transition directly to post-quantum algorithms or adopt a secure combination of conventional quantum-vulnerable and post-quantum algorithms (such as described in ETSI TS 103 744).  The latter approach, often referred to as using hybrid algorithms, includes techniques such as hybrid key establishment, composite signatures, or dual signatures.  These methods combine the established reliability of classical cryptographic methods with the forward-looking resilience of post-quantum algorithms. These hybrid schemes provide a pragmatic solution to address both current and future security threats, offering a balanced yet urgent path for transitioning to post-quantum algorithms and protocols.</p> <p>Post-quantum algorithms already exist for the most critical cryptographic primitives such as digital signatures and key establishment, such as FIPS 203, FIPS 204, FIPS 205, and Special publication 800-208 by NIST.  However, transitioning all protocols and applications to provide security against quantum attackers is an ongoing effort in different standardization bodies. Therefore, an immediate migration isn\u2019t always possible. </p> <p>According to NIST, the recommended first step in the post-quantum transition is to create an inventory of cryptographic algorithms in use and their context as soon as feasible.  This enables prioritizing which protocols and applications to migrate and in what order. </p> <p>Since these protocols and applications typically rely on cryptographic libraries, it\u2019s crucial to ensure that library maintainers update their software to support post-quantum cryptography.  If this isn\u2019t possible, replacing the library with one that provides post-quantum capabilities may be necessary.</p>"},{"location":"kb-articles/quantum-threat/#resources","title":"Resources","text":""},{"location":"kb-articles/quantum-threat/#nistfips-standards","title":"NIST/FIPS standards","text":"<ul> <li>FIPS 203: This standard, based on Kyber, defines a method for two parties to securely share a secret. It is used in different protocols, such as TLS, and is intended to serve as a replacement for quantum-vulnerable key exchange methods such as the Diffie-Hellman key exchange. However, cryptographically, it is a different primitive, namely a Key Encapsulation Mechanism (KEM). As a result, careful consideration is needed when replacing key exchange methods with KEMs to account for their differing properties.</li> <li>FIPS 204: This standard, based on Dilithium, defines a digital signature scheme designed to replace quantum-vulnerable algorithms like ECDSA and RSA. While it functions the same as current signature schemes, it produces larger signatures (2.5KB) and public keys (1.3KB), which may require careful consideration and adjustments in applications to accommodate the increased sizes.</li> <li>FIPS 205: This standard, based on SPHINCS+, defines a digital signature scheme with a distinct trade-off compared to FIPS 204. It uses smaller public keys (32 bytes) but larger signatures (approximately 7 KB). Its security is considered particularly reliable.</li> <li>Special Publication 800-208: This document specifies two additional digital signature schemes, based on LMS (Leighton-Micali Signature) and XMSS (eXtended Merkle Signature Scheme). Unlike the stateless schemes in FIPS 204 and FIPS 205, these are stateful signature schemes, making them particularly suitable for applications where maintaining a reliable state is feasible. For a detailed discussion of potential use cases, refer to ETSI TR 103 692.</li> </ul>"},{"location":"kb-articles/quantum-threat/#european-telecommunications-standards-institute-etsi","title":"European Telecommunications Standards Institute (ETSI)","text":"<p>ETSI largely aligns with NIST standards and has published several reports detailing these cryptographic schemes. Additionally, ETSI has released technical reports on hybrid key exchange algorithms and stateful hash-based signature schemes, among other topics. A collection of drafts from the CYBER QSG working group is available here.</p> <ul> <li>ETSI TS 103 744: This technical specification outlines methods for combining the quantum-vulnerable ECDH key exchange with post-quantum key encapsulation mechanisms, resulting in \u201chybrid algorithms.\u201d The goal is to maintain the security guarantees of current cryptography while adding security against quantum computer threats. </li> <li>ETSI TR 103 692: This technical report focuses on state management for stateful authentication mechanisms, as used for LMS and XMSS (see NIST Special Publication 800-208).</li> </ul>"},{"location":"kb-articles/quantum-threat/#internet-engineering-task-force-ietf","title":"Internet Engineering Task Force (IETF)","text":"<p>IETF has several working groups dedicated to developing post-quantum standards. An overview of ongoing activities can be found here.</p> <ol> <li>IETF RFC 8391: This RFC describes the stateful hash-based signature scheme XMSS (eXtended Merkle Signature Scheme) and serves as a reference for NIST Special Publication 800-208.</li> <li>IETF RFC 8554: This RFC describes the stateful hash-based signature scheme LMS (Leighton-Micali Signature) and serves as a reference to NIST Special Publication 800-208.</li> <li>IETF RFC 8784: This RFC proposes an extension to the Internet Key Exchange Protocol Version 2 (IKEv2) to enhance resistance to quantum attacks by incorporating pre-shared keys. This approach is intended as a transition solution until IKEv2 is extended to include post-quantum key agreement algorithms. </li> </ol>"},{"location":"kb-articles/quantum-threat/#institute-of-electrical-and-electronics-engineers-ieee","title":"Institute of Electrical and Electronics Engineers (IEEE)","text":"<p>IEEE has published several documents related to post-quantum cryptography. These documents can be found here.  Notably, the list includes IEEE P3172 Recommended Practice for Post-Quantum Cryptography Migration.</p>"},{"location":"kb-articles/rsa-algorithm/","title":"RSA algorithm","text":"<p>RSA is among the most widely used public key cryptographic primitives for encryption, digital signatures, as well as a basis for key encapsulation and key agreement protocols.   </p> <p>Using small RSA keys can lead to a complete security breach, compromising confidentiality, authentication, or both, depending on the application. This can also pose a compliance threat. Moreover, RSA is vulnerable to quantum attacks. As a result, RSA is quantum-vulnerable and must be replaced with quantum-resistant alternatives to maintain security against quantum adversaries.</p> <p>Despite being a well-studied cryptosystem, RSA requires careful implementation to ensure security. To minimize the risk of faulty implementation, use well-established cryptographic libraries, which are developed and maintained by qualified cryptographic engineers.        </p>"},{"location":"kb-articles/rsa-algorithm/#detailed-description","title":"Detailed description","text":""},{"location":"kb-articles/rsa-algorithm/#general-introduction","title":"General introduction","text":"<p>RSA is a public-key cryptosystem that enables secure data encryption and digital signatures, relying on the mathematical difficulty of factoring large composite numbers.  It works by generating a pair of keys: a public key for encryption or signature verification and a private key for decryption or signature generation, ensuring that only the intended recipient or signer can perform the corresponding cryptographic operation. During key generation, two large primes are chosen independently and with enough randomness. Their product is taken as the RSA modulus.  The cryptographic primitives are designed such that the participating entities perform modular exponentiation, raising a number to an exponent modulo the chosen exponent.  This modular exponentiation can be computed efficiently. An eavesdropper attempting to decrypt an encryption or forge a signature is faced with a hard computational problem, which is inverse to modular exponentiation. Next, we provide an example focussing on encryption to provide intuition to the reader. It is followed by a description of the signature modes.  </p> <p>Key Generation example: RSA key generation starts with two distinct odd prime numbers, p and q, generated at random.  The RSA modulus is their product, n=pq. The RSA encryption exponent e\\&lt;(p-1)(q-1) with certain mathematical properties is then chosen.  A popular choice in practice is  65537=2<sup>16</sup>+1, for efficiency.  The decryption exponent d is then computed as an integer such that ed equals 1 modulo the least common multiple of p-1 and q-1.  The secret private key is (p,q,d) and the public key is (n,e).</p> <p>Encryption scheme example: Consider Bob wanting to encrypt and send a message m to Alice.  Alice generates a key pair as above, sends Bob the public key (n,e) and keeps the private key (p,q,d) secret.  Bob encodes the message into a positive integer encode(m)&lt;n and sends Alice the encrypted message c= encode(m)<sup>e</sup> modulo n.  Here, encode(m) is a possibly randomized cryptographically secure encoding function.  The arithmetic of RSA is such that Alice can recover encode(m) as c<sup>d</sup> modulo n.</p> <p>In the original RSA proposal, the encoding function is deterministic.  The resulting encryption and decryption primitives are referred to as RSAEP and RSADP, respectively, by IETF PKCS1.  It is a deterministic encryption scheme, meaning that, given a chosen public key, the encryption is a deterministic function of the message.  Therefore, it is not semantically secure and vulnerable to chosen-plaintext attacks. Furthermore, it is malleable and insecure under chosen cipher-text attacks.</p> <p>To mitigate these issues, the message has to be randomly \u201cpadded\u201d appropriately while being encoded as an integer. Two encryption modes resulting from the choice of padding are RSAES-PKCS1-v1_5 and RSAES-OAEP.  The RSAES-PKCS1-v1_5 padding scheme predates RSAES-OAEP and is deprecated by IETF due to vulnerability to the Bleichenbacher attack.</p> <p>RSAES-OAEP. The Optimal Asymmetric Encryption Padding (OAEP) is a padding scheme to mitigate aforementioned security issues arising from plain RSA being a deterministic encryption scheme (see, for instance NIST SP 800-56B Rev. 2, section 7.2.2).  It consists of encryption/decryption primitives RSAEP/RSADP, a cryptographic hash function H, a possibly different cryptographic hash function to serve as a mask generating function (MGF) and a random number bit generator (RBF).  RSAES-OAEP is IND-CCA2 secure, meaning it ensures security even against chosen-ciphertext attacks such as the Bleichenbacher attack.</p> <p>RSA Signature: There are two signature modes, namely  RSASSA-PSS and RSASSA-PKCS1-v1_5 (see for instance, IETS #1: RSA Cryptography Specifications Version 2.2.)  Unlike the encryption case, no attacks are known against the signature scheme RSASSA-PKCS1-v1_5 using the PKCS1-v1_5 padding.  Yet, as a precautionary measure, IETF requires RSASSA-PSS in new applications. RSASSA-PKCS1-v1_5 schemes may still persist, for compatibility with existing applications.  </p> <p>RSA Key Agreement: NIST SP 800-56B Rev. 2 describes two families (KAS1 and KAS2) of RSA based key agreement schemes and a family KTS-OAEP of key transfer mechanisms.  Both the agreement schemes employ RSA secret value encapsulation RSASVE as a primitive.  The main distinction between KAS1 and KAS2 is that in the former, only one of the party\u2019s key-establishment keys is used.  In the latter, both the party\u2019s key-establishment keys are used. As a consequence, KAS1 only comes with two confirmation modes KAS1-basic (with no confirmation) and KAS1-party_V-confirmation.  Whereas KAS2 accommodates KAS2-basic, KAS2-party_V-confirmation, KAS2-party_U-confirmation and KAS2- bilateral-confirmation.  See NIST SP 800-56B Rev. 2 section 10 for contexts in which KAS1, KAS2 or KTS-OAEP are recommended.</p>"},{"location":"kb-articles/rsa-algorithm/#security-recommendations","title":"Security recommendations","text":"<p>The best known classical cryptanalytic algorithm to break RSA is the Number Field Sieve integer factorization algorithm. Recent examples illustrate the vulnerability of small keys: </p> <ul> <li>In 2024, an 829-bit key was broken (factored) using 2700 core years of computation, costing an estimated $120,000\u2014affordable to organized threat actors.   </li> <li>By comparison, factoring RSA-512 in 2015 cost just $75 in 4 hours, making it accessible to individual attackers.</li> </ul> <p>As a consequence, to guarantee bit security on the order of hundreds, RSA key lengths need to be in the thousands of bits.  For instance, NIST SP 800-57 PART 1 REV. 5 recommends RSA key lengths of 3072 bits for 128-bit security and 7682 bits for 256-bit security.  A shorter key length risks an attacker recovering the secret key, with consequences including forged signatures, disclosure of plaintext from encrypted data, etc.  For instance, a conservative stance is to consider big organizations and nation-states as possessing the resources to break RSA with key length 1024.</p> <p>Shor\u2019s algorithm can factor integers efficiently with access to fault tolerant quantum computers, thereby breaking RSA in the advent of fault tolerant quantum computers. Therefore, RSA is not considered to be post-quantum secure.  </p> <p>RSA cryptographic primitives should be implemented  using well-established cryptographic libraries, adhering to the list of validations for RSA keys is in NIST SP 800-89 5.3.3 and FIPS 186-3.  For guidance on proper RSA implementation, refer to established standards and guidelines, including:</p> <ul> <li>NIST SP 800-56B REV. 2  </li> <li>NIST SP 800-89  </li> <li>NIST SP 800-57 PART 1 REV. 5  </li> <li>IETF PKCS #1: RSA Cryptography Specifications Version 2.2</li> </ul>"},{"location":"kb-articles/rsa-algorithm/#sources","title":"Sources","text":"<ul> <li>Rivest, R.; Shamir, A.; Adleman, L. (February 1978). A Method for Obtaining Digital Signatures and Public-Key Cryptosystems. Communications of the ACM. 21 (2): 120\u2013126..</li> <li>Knowledge base: Quantum threat</li> <li>Knowledge base: Hash functions</li> <li>Shor, P.W. (1994). \u201cAlgorithms for quantum computation: Discrete logarithms and factoring\u201d. Proceedings 35th Annual Symposium on Foundations of Computer Science. pp. 124\u2013134.</li> <li>Arjen K. Lenstra, Hendrik W. Lenstra. The Development of the Number Field Sieve. Lecture Notes in Mathematics.</li> <li>NIST SP 800-56B Rev. 2. Recommendation for Pair-Wise Key-Establishment Using Integer Factorization Cryptography.  </li> <li>NIST Special Publication 800-57 Part 1 Revision 5. Recommendation for Key Management: Part 1 \u2013 General.  </li> <li>NIST Special Publication 800-89. Recommendation for Obtaining Assurances for Digital Signature Applications.  </li> <li>Internet Engineering Task Force (IETF) PKCS #1: RSA Cryptography Specifications Version 2.2.  </li> <li>Eiichiro Fujisaki, Tatsuaki Okamoto, David Pointcheval, and Jacques Stern. RSA-OAEP is secure under the RSA assumption. Advances in Cryptology-CRYPTO 2001, vol. 2139 of Lecture Notes in Computer Science, SpringerVerlag, 2001.</li> </ul>"},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/","title":"Timing attacks and broader side-channel attacks","text":"<p>Side-channel attacks are threats that exploit unintended information leaks from cryptographic implementations, such as execution time, power consumption, or electromagnetic emissions. Such attacks target the physical or observable behaviors of systems rather than the underlying mathematical structure of cryptographic algorithms. By analyzing such leaks, attackers can infer sensitive information, including private keys, which can lead to unauthorized access, data breaches, and compromised authentication mechanisms.</p> <p>Among side-channel attacks, timing attacks are particularly common, leveraging variations in execution time caused by cryptographic operations. Real-world risks include vulnerabilities in RSA, AES, DSA, ECDSA, and post-quantum algorithms like ML-KEM. Preventing these attacks is important, as improper implementation can undermine otherwise robust cryptographic designs. This document details the mechanisms behind these attacks and their potential impact, and outlines crucial countermeasures and implementation strategies to mitigate these vulnerabilities. </p>"},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/#introduction-to-side-channel-attacks","title":"Introduction to side-channel attacks","text":"<p>Side-channel attacks target the unintended information leaks during cryptographic operations, which can compromise system security. While these attacks primarily focus on extracting secret keys, they can also target other sensitive information, such as plaintext messages. These vulnerabilities stem from implementation flaws, hardware design issues, or environmental factors, rather than weaknesses in the cryptographic algorithms themselves.</p> <p>To better understand the nature of timing and side-channel attacks, and the techniques used to mitigate them, we will introduce a few key definitions.</p> Timing side-channel attacks A timing side-channel is a vulnerability where an attacker can infer secret information by observing variations in how long a cryptographic operation takes to execute. For instance, if a program\u2019s execution time changes depending on a secret value\u2014as in the example <code>if(secret == x) then do_something()</code>\u2014an attacker can analyze these timing differences (in a conditional branch) to infer the secret value. Power or EM side-channel attacks A power or electromagnetic (EM) side-channel is a vulnerability where an attacker can infer secret data by observing physical emissions such as electromagnetic radiation or measuring power consumption. As a simple illustration, during the operation <code>c = a*b</code>, power consumption or EM emissions may differ depending on whether <code>b = 42</code> or <code>b = 0</code>, as setting a register to a non-zero value consumes slightly more power than leaving it at zero. These variations may correlate with the secret data, enabling attackers to extract sensitive information. Countermeasures to timing attacks Timing side-channels are mitigated with \u201cconstant-time programming\u201d techniques. This approach involves implementing cryptographic algorithms in a way that ensures their execution time is independent of the specific values of secret data. It involves eliminating execution time variations caused by: <ul><li>Conditional branches based on secret data</li><li>Memory accesses with indices derived from secret values</li><li>Instructions with data-dependent timing to process secret values, such as division, where execution time varies depending on the input values.</li></ul>The key is to ensure that all code paths execute in constant-time, independent of the values of secret data, ensuring that no timing differences leak information. While the term \u201cconstant-time\u201d suggests uniform execution time, in this context it means ensuring that the execution time depends only on non-secret factors, such as the algorithm itself, the length of the input, and public parameters. Countermeasures to power or EM side-channels Mitigating power or EM side-channels involves extending the principles of constant-time programming to ensure that physical emissions, such as power consumption or electromagnetic radiation, are independent of the specific values of secret data. For instance, during the operation <code>c = a*b</code>, the power usage or EM emissions should remain identical regardless of whether <code>b = 42</code> or <code>b = 0</code>. This is achieved through various techniques including masking, balancing operations, noise injection, and using specialized hardware protections such as electromagnetic shielding."},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/#implementations-with-known-side-channel-risks","title":"Implementations with known side-channel risks","text":"<p>Cryptographic implementations are inherently susceptible to side-channel attacks due to unintended information leakage during execution on commodity hardware. These vulnerabilities arise from implementation flaws, underlying hardware issues, or environmental factors, rather than weaknesses in the algorithms themselves. Side-channels, such as timing variations, power consumption patterns, or cache access patterns, can expose sensitive data even in implementations of otherwise secure algorithms.</p> <p>Among these, timing attacks are particularly concerning because they can (in principle) be conducted remotely, without requiring physical access to the target device. In contrast, other side-channel attacks, such as those exploiting power consumption or electromagnetic emissions, typically require direct access to the hardware, such as a smart card or IoT device. </p> <p>The following are examples of algorithms where side-channel vulnerabilities have been demonstrated:</p> <ol> <li> <p>RSA: Timing leaks during modular exponentiation in decryption and signing operations, such as when using the square-and-multiply algorithm, can expose private keys in unprotected implementations. A landmark example is the attack described by Brumley and Boneh in Remote Timing Attacks are Practical. This was one of the first known remote side-channel attacks demonstrating the feasibility of exploiting timing vulnerabilities over a network.  </p> </li> <li> <p>ECDSA: Timing variations during signature generation can be exploited to infer private keys. Specifically, the use of ephemeral keys (\u201ck\u201d) can introduce timing vulnerabilities. For example, Brumley and Tuveri demonstrated remote timing attacks on ECDSA in OpenSSL in their paper, Remote Timing Attacks are Still Practical, using network-based measurements to recover private keys involved in server authentication during TLS handshakes. Real-world vulnerabilities include:  </p> <ul> <li>CVE-2011-1945 highlighting timing differences in ECDSA implementations.  </li> <li>CVE-2018-0495 where timing leaks in cryptographic libraries exposed private keys. </li> </ul> <p>Research such as LadderLeak further emphasizes the risks of timing leaks during scalar multiplication.</p> </li> <li> <p>AES: Cache timing attacks target table lookups during encryption to recover secret keys. Bernstein\u2019s Cache-Timing Attacks on AES demonstrated how attackers with local access could observe cache behavior to recover AES keys. Building on this, Cache Based Remote Timing Attack on the AES showed that such attacks are also feasible remotely, where attackers infer cache behavior over a network to extract AES keys.</p> </li> <li> <p>Kyber (ML-KEM): Post-quantum cryptographic algorithms, such as Kyber, are designed to resist attacks from quantum computers. However, research, including KyberSlash: Exploiting secret-dependent division timings in Kyber implementations, shows how attackers can use small timing differences during decryption to extract sensitive information, such as private keys.</p> </li> <li> <p>DSA: The Digital Signature Algorithm (DSA) is vulnerable to timing attacks if its implementation does not ensure constant-time operations. Real-world vulnerabilities include CVE-2016-1000341 and CVE-2018-0734, where improper handling of DSA signing exponentiation led to timing leaks that could expose private keys. Research such as Lattice Attacks on Digital Signature Schemes further underscores the risks of nonce leakage.</p> </li> </ol>"},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/#severity-of-timing-and-side-channel-attacks","title":"Severity of timing and side-channel attacks","text":"<p>The severity of timing and side-channel attacks depends on several factors, including the cryptographic algorithm in use, the attacker\u2019s access level, and the type of information leaked. These attacks exploit weaknesses in implementation or hardware, making even theoretically secure systems vulnerable if proper safeguards against side-channel risks are not in place. </p> <p>Often, these weaknesses are not known to experts at the time the implementation was developed, and in some cases, even hardware manufacturers may be unaware of vulnerabilities in their designs. This uncertainty amplifies the risk, as side-channel vulnerabilities can remain undetected for years before being exploited.</p>"},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/#key-factors-contributing-to-severity","title":"Key factors contributing to severity","text":"<p>The severity of timing and side-channel attacks is influenced by several key factors which collectively determine the potential risk and consequences of such attacks.</p> <ul> <li>Accessibility of the Attack: Timing and other side-channel attacks differ in their accessibility.   </li> <li>Remote attacks, such as those targeting RSA or ECDSA timing leaks, require minimal access and may be launched over a network.   </li> <li>Local attacks, like cache timing on AES or power analysis, usually require physical or even privileged access to the target device. While more challenging to execute, local attacks remain significant in specific contexts, such as cloud environments or smart card implementations.</li> </ul> <ul> <li>Complexity of Execution: The complexity of a timing or other side-channel attack varies based on the algorithm and the required level of precision. While some attacks demand extensive measurements or high-resolution timing tools, advancements in attack techniques, statistical methods, and automation have made many attacks\u2014such as remote timing attacks on ECDSA\u2014practical and relatively straightforward to execute.</li> </ul> <ul> <li>Impact of the Vulnerability: Attackers typically target private keys or secret keys, as these provide the greatest level of access to sensitive data and systems. For example, private key exposure allows attackers to decrypt sensitive information or impersonate users. In some cases, plaintext messages can also be targeted. Even attacks that learn partial knowledge about keys or plaintexts can have devastating consequences, depending on the context.</li> </ul> <ul> <li>Algorithm and Implementation Weaknesses: Algorithms relying on non-constant-time operations, such as modular exponentiation, scalar multiplication, or table lookups, are particularly prone to timing and side-channel attacks. While RSA, DSA, and ECDSA are known to exhibit such vulnerabilities, post-quantum algorithms like Kyber are also susceptible if implemented without constant-time safeguards. Poor implementation practices and hardware-specific issues, such as cache behavior or branch prediction, further amplify the risks.</li> </ul>"},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/#countermeasures","title":"Countermeasures","text":"<p>Mitigating timing and side-channel vulnerabilities requires a combination of secure implementation practices, robust cryptographic tools, and careful risk assessment. The following measures help ensure systems remain resilient to these attacks:</p> <ol> <li> <p>Constant-time programming. The cryptography implementation designs should follow constant-time programming principles to eliminate timing leaks. This ensures their execution time is independent of secret values to prevent timing leaks that enable attackers to infer sensitive data.</p> </li> <li> <p>Using hardened libraries. Leverage well-maintained cryptographic libraries, such as OpenSSL or libsodium, which incorporate hardened implementations to minimize the risk of timing vulnerabilities.</p> </li> <li> <p>Regular vulnerability testing. Conduct regular testing for timing and side-channel vulnerabilities using tools like <code>ctgrind</code>, <code>Valgrind</code>, or side-channel analysis frameworks to identify weaknesses early in the development lifecycle.</p> </li> <li> <p>Timely updates. Regularly update cryptographic libraries and frameworks to address newly discovered vulnerabilities and maintain compliance with emerging standards.</p> </li> </ol> <p>Before implementing countermeasures against power and electromagnetic (EM) side-channels, organizations should first assess whether such risks are relevant to their specific use cases. Implementing power or EM countermeasures is generally necessary only in environments where attackers have physical access or the ability to observe emissions, including potential remote observation with highly sensitive equipment. If these risks are deemed relevant, organizations can mitigate them by adopting hardened cryptographic hardware solutions, such as hardware security modules (HSMs) or secure enclaves. This risk can also be mitigated by using certified hardware, such as devices evaluated under Common Criteria (EAL5+ or higher), or by commissioning external evaluations through accredited security labs to verify resistance against side-channel attacks.  </p> <p>However, it\u2019s crucial to acknowledge that side-channel mitigations aim to make attacks as infeasible as practically possible, rather than providing absolute immunity. Furthermore, due to the constant evolution of attack methodologies, existing countermeasures may require updates or modifications to remain effective against newly discovered vulnerabilities.</p> <p>Organizations must balance security requirements with practicality, ensuring countermeasures are tailored to their specific threat models and operational needs. In most cases, adhering to best practices for cryptographic implementation and selecting hardened libraries will sufficiently mitigate most timing and side-channel risks.</p>"},{"location":"kb-articles/timing-attacks-and-broader-side-channel-attacks/#sources","title":"Sources","text":"<ul> <li>Brumley, D., &amp; Boneh, D. Remote Timing Attacks are Practical. </li> <li>Brumley, B., &amp; Tuveri, N. Remote Timing Attacks are Still Practical. </li> <li>Bernstein, D. J. Cache-Timing Attacks on AES.</li> <li>Cetinkaya Koc, C. Cache Based Remote Timing Attack on the AES. </li> <li>KyberSlash: Practical Decryption Failure Timing Attacks on Lattice-Based Schemes.</li> <li>LadderLeak: Breaking ECDSA with Less Than One Bit of Nonce Leakage.</li> <li>CWE-385: Covert Timing Channel. Overview of timing attack risks and mitigation practices.</li> <li>CWE-208: Observable Timing Discrepancy. Analysis of timing discrepancies that leak sensitive information.</li> <li>Project Wycheproof by Google: test crypto libraries against known attacks.</li> <li>Valgrind - Tool for analyzing constant-time properties of cryptographic implementations.</li> <li>ctgrind - Tool for analyzing constant-time properties of cryptographic implementations.</li> <li>CVE-2018-0734. Timing vulnerability in OpenSSL\u2019s DSA, allowing private key recovery.</li> <li>CVE-2018-3615 (Foreshadow). Exploits L1 cache to infer data from secure Intel SGX enclaves.</li> <li>CVE-2016-7056. Vulnerability in OpenSSL related to cache access patterns during modular exponentiation.</li> <li>CVE-2024-23342. Remote timing vulnerability in a widely used cryptographic library, allowing attackers to recover sensitive keys.</li> <li>CVE-2024-2408. Cryptographic timing attack vulnerability exposing sensitive keys via observable timing differences.</li> <li>CVE-2020-25658. Timing discrepancies in cryptographic computations reveal sensitive information about secret keys.</li> <li>CVE-2020-29506. Timing leaks in cryptographic algorithms allow attackers to infer private keys remotely.</li> <li>CVE-2024-36405. Side-channel vulnerability in cryptographic hardware leaking electromagnetic emissions of sensitive keys.</li> <li>Howgrave-Graham, N. A., &amp; Smart, N.P. Lattice Attacks on Digital Signature Schemes.</li> <li>Ac\u0131i\u00e7mez, O., Schindler, W., &amp; Ko\u00e7, \u00c7. K. Cache based remote timing attack on the AES.</li> </ul>"},{"location":"kb-articles/transport-layer-security-tls/","title":"Transport Layer Security (TLS)","text":"<p>Transport Layer Security (TLS) is a widely used cryptographic protocol responsible for securing communication over the Internet. It ensures confidentiality, integrity, and authentication between clients and services, preventing unauthorized access and data interception. TLS is the successor to the now-obsolete Secure Sockets Layer (SSL) protocol and has evolved significantly to mitigate security weaknesses in earlier versions.</p> <p>TLS is essential for secure browsing (HTTPS) and is widely used in online transactions, VPN security, and enterprise encryption. However, misconfigurations, outdated versions, and weak cipher suites can expose organizations to vulnerabilities. To ensure security, use TLS 1.2 with secure configurations or, preferably, TLS 1.3 (the latest version), which eliminates legacy weaknesses and enhances security and performance.</p>"},{"location":"kb-articles/transport-layer-security-tls/#introduction-to-tls","title":"Introduction to TLS","text":"<p>TLS encrypts data in transit to ensure secure communication between clients and servers. It\u2019s also widely used to protect web traffic, email (SMTP, IMAP, POP3), VoIP, and APIs. In general, TLS aims to provide:</p> <ul> <li>Confidentiality: Encrypts transmitted data, ensuring that even if an attacker intercepts it, they cannot read the plaintext.  </li> <li>Authentication: Authenticates the server using certificates, and optionally the client identity.   </li> <li>Integrity: Detects tampering of data in transit using cryptographic integrity checks, ensuring that altered data is rejected.</li> </ul> <p>TLS 1.3, the latest version, was introduced to improve security and performance by removing outdated cryptographic primitives and simplifying cipher suite selection. It also supports post-quantum cryptography, ensuring resilience against future cryptographic threats. Ongoing research, like the IETF\u2019s \u201cHybrid Key Exchange in TLS 1.3\u201d, aims to integrate post-quantum security into TLS 1.3 through hybrid key exchange mechanisms that combine classical and post-quantum algorithms. TLS 1.2 will not officially support post-quantum cryptography.</p>"},{"location":"kb-articles/transport-layer-security-tls/#a-brief-history-from-ssl-to-tls","title":"A brief history: from SSL to TLS","text":"<p>TLS replaced Secure Sockets Layer (SSL) due to security flaws. SSL 2.0 and 3.0 were deprecated after attacks like POODLE (CVE-2014-3566) revealed them as unsafe.</p>"},{"location":"kb-articles/transport-layer-security-tls/#tls-version-history","title":"TLS version history","text":"<ul> <li>TLS 1.0 (1999) \u2013 Described in RFC 2246. Based on SSL 3.0 but vulnerable to BEAST (CVE-2011-3389).  </li> <li>TLS 1.1 (2006) \u2013 Described in RFC 4346. Introduced improvements but saw limited adoption.  </li> <li>TLS 1.2 (2008) \u2013 Described in RFC 5246. Strengthened security, replacing MD5 and SHA-1 with stronger cryptographic hash functions.  </li> <li>TLS 1.3 (2018) \u2013 Described in RFC 8446. Removed outdated ciphers (e.g., RSA key exchange, CBC mode), enforced forward secrecy, and improved handshake speed.</li> </ul>"},{"location":"kb-articles/transport-layer-security-tls/#current-status","title":"Current status","text":"<ul> <li>TLS 1.0 &amp; 1.1 - Deprecated (RFC 8996), should no longer be used.  </li> <li>TLS 1.2 - Still supported, but should be securely configured.  </li> <li>TLS 1.3  - Preferred for security and efficiency.</li> </ul> <p>Organizations should disable older TLS versions and ensure compliance with modern security standards.</p>"},{"location":"kb-articles/transport-layer-security-tls/#tls-adoption-and-usage-statistics","title":"TLS adoption and usage statistics","text":"<p>Table 1 shows data from different sources that provide insights into SSL/TLS version adoption.</p> TLS Version Chosen Protocol (F5 report, 2021) Supported Protocol (SSL Pulse, May 2024) TLS 1.3 63% of connections actively use TLS 1.3 70.1% of websites support TLS 1.3 TLS 1.2 36% of connections actively use TLS 1.2 99.9% of websites support TLS 1.2 TLS 1.1 0.4% of connections still use TLS 1.0 or 1.1 30% of websites still support TLS 1.1 TLS 1.0 0.4% of connections still use TLS 1.0 or 1.1 27.9% of websites still support TLS 1.0 SSL 3.0 0.002% of connections still use SSL 3.0 1.4% of websites still support SSL 3.0 SSL 2.0 Practically extinct 0.1% of websites still support SSL 2.0 <p>The key differences between the two reports in Table 1 are:</p> <ul> <li>F5 TLS Telemetry Report (2021) \u2192 Measures which TLS version is actually chosen and used in active connections across the Tranco top one million global sites.  </li> <li>SSL Pulse Report (May 2024) \u2192 Measures which TLS versions are offered as supported by the server, even if they aren\u2019t actually used, across the top 150,000 websites from Alexa rankings.</li> </ul>"},{"location":"kb-articles/transport-layer-security-tls/#offered-vs-negotiated-cipher-suites","title":"Offered vs. negotiated cipher suites","text":"<p>A cipher suite specifies the cryptographic algorithms for a secure communications channel. Think of the cipher suite as a recipe for security. Using poor or outdated ingredients like deprecated algorithms, incorrect configurations, or improper parameters leads to undesirable\u2014and potentially harmful\u2014results. </p> <p>Table 1 highlights the widespread use of outdated SSL/TLS versions, which poses a significant security risk by allowing the negotiation and use of insecure cryptographic options, including weak cipher suites.</p> <p>It\u2019s important to distinguish between cipher suites that a server offers and cipher suites that are actually used in connections:</p> <ul> <li>Offered Cipher Suites: The list of cipher suites that a server advertises as supported. These may include outdated or insecure options if the server hasn\u2019t been properly configured.  </li> <li>Negotiated (Used) Cipher Suites: The cipher suites actually selected and used in a TLS handshake between the client and server. This depends on both the server\u2019s and the client\u2019s configurations. In AQtive Guard, some rules may also refer to \u2018use\u2019 instead of negotiated, e.g. Use of ciphersuite with 3DES.</li> </ul> <p>Many websites continue to offer older TLS versions and weaker cipher suites, often for compatibility reasons. In reality, this is usually due to legacy infrastructure, lack of awareness, failure to upgrade systems, reliance on outdated internal policies, or simple inertia\u2014where organizations leave weak cryptographic configurations in place because of resource constraints or fear of breaking existing integrations.</p> <p>Even if a server defaults to a secure protocol, the presence of outdated cipher suites in the offered list introduces risks, as it may allow older, insecure clients to negotiate weak encryption settings.</p>"},{"location":"kb-articles/transport-layer-security-tls/#cipher-suites-in-tls","title":"Cipher suites in TLS","text":"<p>A cipher suite specifies the set of cryptographic algorithms used in a TLS connection to establish a secure communication channel. It defines the combination of encryption, authentication, and key exchange mechanisms that ensure data confidentiality, integrity, and authenticity during transmission. By determining the security parameters for data in transit, cipher suites play a critical role in maintaining secure communications.</p> <p>While TLS 1.2 has over 300 registered cipher suites, many have been officially deprecated due to security weaknesses, leaving only around 20 still recommended for secure use. Older cipher suites that rely on outdated cryptographic primitives, such as RSA key exchange, CBC-mode encryption, and SHA-1 hashing, have been formally deprecated in various security guidelines, including IETF RFCs and NIST recommendations, due to their vulnerabilities to downgrade attacks, padding oracle attacks, and cryptographic breakage. </p> <p>However, deprecated doesn\u2019t mean disabled\u2014many systems continue to support and even negotiate these insecure cipher suites if they haven\u2019t been explicitly removed. Since weak cipher suites can still be offered even if they\u2019re rarely used, security teams should audit their TLS configurations regularly and make sure only strong cipher suites are enabled. In contrast, TLS 1.3 officially supports only five cipher suites, as weak and redundant algorithms were eliminated to enforce stronger security.</p>"},{"location":"kb-articles/transport-layer-security-tls/#tls-cipher-suite-naming-conventions","title":"TLS cipher suite naming conventions","text":"<p>TLS cipher suite names follow a structured format to indicate which cryptographic algorithms are used. Table 2 shows an example of how TLS 1.2 cipher suites are named using the example <code>TLS_ECDHE_ECDSA_WITH_AES_128_GCM_SHA256</code>.</p> <p>Table 2 shows the naming convention of TLS 1.2 cipher suites.</p> Component Meaning TLS Protocol name (TLS) ECDHE Key Exchange Algorithm (Elliptic Curve Diffie-Hellman Ephemeral) ECDSA Authentication Algorithm (Elliptic Curve Digital Signature Algorithm) AES_128_GCM Encryption Algorithm (AES with 128-bit key in GCM mode) SHA256 Message Authentication Code (MAC) (SHA-256) <p>Table 3 shows an example of how TLS 1.3 cipher suites are named using the example TLS_AES_256_GCM_SHA384.</p> <p>Table 3 shows the naming convention of TLS 1.3 cipher suites.</p> Component Meaning TLS Protocol name (TLS) AES_256_GCM Encryption Algorithm (AES with 256-bit key in GCM mode) SHA384 Hash Algorithm (SHA-384) <p>TLS 1.3 eliminates key exchange and authentication from the cipher suite name, as these are negotiated separately during the handshake. Unlike TLS 1.2, which required  manual selection of cipher suites that included specific key exchange and authentication methods, TLS 1.3 ensures only secure algorithms are available by default. </p> <p>This provides several security and performance improvements:</p> <ul> <li>Prevents downgrade attacks \u2192 Attackers can no longer force the connection to fall back to a weaker key exchange or authentication method.  </li> <li>Simplified configuration \u2192 Reduces misconfigurations by eliminating manual selection of key exchange and authentication options.  </li> <li>Forward secrecy enforcement \u2192 TLS 1.3 requires ephemeral key exchange methods by default, making it impossible to decrypt past sessions even if the private key is compromised.  </li> <li>Reduced handshake latency \u2192 Simplified cipher suite negotiation enables faster and more efficient TLS handshakes.</li> </ul>"},{"location":"kb-articles/transport-layer-security-tls/#deprecated-and-insecure-cipher-suites","title":"Deprecated and insecure cipher suites","text":"<p>Over the years, numerous cipher suites have been found to be insecure due to weaknesses in their underlying cryptographic primitives. Despite being deprecated, many systems still support or offer these cipher suites, leaving them vulnerable to downgrade attacks, cryptanalysis, and padding oracle attacks.</p> <p>Of those over 300 TLS 1.2 cipher suites, the majority rely on outdated and compromised cryptographic methods \u2013 such as DES (16), 3DES (20), RC4 (18), RC2 (3), SHA-1 (110), and MD5 (14), all of which have been broken or proven weak. These ciphers must be fully disabled to prevent potential exploits.</p> <p>The following legacy cipher suites and algorithms are not considered secure and shouldn\u2019t be used:</p> <ul> <li>RC4-based cipher suites \u2192 Broken due to statistical biases that allow attackers to recover plaintext from encrypted traffic. RFC 7465 explicitly prohibits its use in TLS.  </li> <li>CBC-mode cipher suites \u2192 Vulnerable to padding oracle attacks, such as BEAST (CVE-2011-3389) and POODLE (CVE-2014-3566), which allow attackers to decrypt data by exploiting flaws in how block cipher padding is processed.  </li> <li>3DES (TLS_RSA_WITH_3DES_EDE_CBC_SHA) \u2192 Weak due to Sweet32 (CVE-2016-2183), which exploits the small 64-bit block size of 3DES, making it possible for attackers to recover plaintext after analyzing large amounts of encrypted traffic.  </li> <li>Export-grade RSA cipher suites \u2192 Originally designed to comply with 1990s-era cryptographic export restrictions, these weak ciphers can be forced onto modern connections via downgrade attacks, such as FREAK (CVE-2015-0204) and Logjam (CVE-2015-4000), compromising encryption strength.  </li> <li>MD5-based cipher suites \u2192 MD5 has been cryptographically broken for over a decade due to collision attacks, making it unsafe for message authentication or certificate signing. Despite this, 13 cipher suites in the IANA registry still use MD5.  </li> <li>SHA-1-based cipher suites \u2192 Once widely used for message authentication, SHA-1 has proven collision vulnerabilities that allow attackers to forge signatures and tamper with encrypted data. Despite its known weaknesses, 110 cipher suites still reference SHA-1.</li> </ul>"},{"location":"kb-articles/transport-layer-security-tls/#why-these-cipher-suites-are-still-a-risk","title":"Why these cipher suites are still a risk","text":"<p>Even though modern clients rarely choose these ciphers, offering them creates a major security risk. Attackers can exploit downgrade attacks to force weaker encryption, allowing them to:</p> <ul> <li>Compromise session confidentiality by recovering plaintext from encrypted traffic.  </li> <li>Break authentication mechanisms by exploiting weak or broken hash functions like SHA-1 and MD5.  </li> <li>Modify encrypted data in transit, which can lead to man-in-the-middle (MITM) attacks.</li> </ul> <p>Notable Downgrade Attack CVEs:</p> <ul> <li>FREAK Attack (CVE-2015-0204) \u2192 Allowed attackers to force a weak export-grade RSA cipher, making TLS connections vulnerable to decryption.   </li> <li>Logjam Attack (CVE-2015-4000) \u2192 Exploited weak Diffie-Hellman key exchanges to downgrade TLS connections and decrypt traffic.   </li> <li>POODLE Attack (CVE-2014-3566) \u2192 Forced TLS connections to downgrade to SSL 3.0, which had weak CBC padding, allowing data decryption.   </li> <li>CVE-2021-38502 \u2192 Demonstrated weaknesses in TLS implementations where legacy cipher suites could be forced despite server preferences.   </li> <li>CVE-2019-14887 \u2192 Exploited misconfigurations in TLS servers to allow protocol downgrades, forcing clients to use weaker encryption. </li> </ul> <p>Legacy support does not mean security. Many organizations still offer outdated ciphers simply because they haven\u2019t been explicitly disabled, exposing them to preventable risks.</p> <p>Mitigation Administrators should take the following critical steps to eliminate legacy cipher risks:</p> <ul> <li>Explicitly disable all cipher suites using RC4, CBC-mode, 3DES, export-grade RSA, MD5, and SHA-1 in server configurations.  </li> <li>Use only modern AEAD-based ciphers, such as AES-GCM or ChaCha20-Poly1305, which provide both encryption and integrity without relying on weak hash functions.  </li> <li>Regularly audit TLS configurations to ensure deprecated ciphers are fully removed, not just avoided.  </li> <li>Upgrade to TLS 1.3 where possible, as it removes support for all insecure ciphers by default, eliminating these risks entirely.</li> </ul> <p>By enforcing strict TLS policies and proactively disabling legacy cryptographic mechanisms, organizations can eliminate weaknesses, strengthen encryption security, and prevent attackers from exploiting outdated configurations.</p>"},{"location":"kb-articles/transport-layer-security-tls/#security-requirements-and-compliance","title":"Security requirements and compliance","text":"<p>As cyber threats evolve, ensuring the security of encrypted communications has become a regulatory requirement, rather than just a best practice. Organizations, governments, and industry groups have established mandatory guidelines requiring TLS 1.2 or higher, aiming to phase out outdated cryptographic protocols that risk sensitive data exposure.</p>"},{"location":"kb-articles/transport-layer-security-tls/#mandatory-tls-12-or-higher","title":"Mandatory TLS 1.2 or higher","text":"<p>With the growing number of vulnerabilities in older TLS versions, security organizations and regulatory bodies require the use of TLS 1.2 or later to maintain secure communication channels. TLS 1.0 and TLS 1.1, once the standard for encrypted internet traffic, are now considered obsolete and pose significant security risks. As a result, multiple organizations and standards bodies have issued mandatory deprecation policies, requiring systems to migrate to secure TLS versions, such as TLS 1.3.</p> <p>Some of the most notable mandates include:</p> <ul> <li>RFC 8996 (IETF) \u2013 The Internet Engineering Task Force (IETF) officially deprecated TLS 1.0 and TLS 1.1 in RFC 8996, recommending that all systems migrate to TLS 1.2 or TLS 1.3. The RFC explicitly states that these older versions shouldn\u2019 not be used due to known cryptographic weaknesses.  </li> <li>NIST SP 800-52 Rev. 2 \u2013 The National Institute of Standards and Technology (NIST) requires U.S. federal systems to use TLS 1.2 or higher as part of their guidelines for secure communications. This mandate ensures government agencies and contractors phase out insecure encryption methods and adopt stronger cryptographic protocols.  </li> <li>NSA Guidance on TLS \u2013 The National Security Agency (NSA) issued official guidance (Eliminating Obsolete TLS) explicitly mandating the immediate discontinuation of TLS 1.0 and the phased removal of TLS 1.1. The NSA states that \u201cTLS 1.1 and older provide inadequate protection for sensitive information\u201d and requires that government and high-security networks enforce TLS 1.2 or higher, with a strong recommendation to migrate to TLS 1.3 wherever possible. This aligns with broader federal and industry-wide efforts to eliminate obsolete encryption protocols and enhance cryptographic security.  </li> <li>GSA SSL/TLS Implementation Guide \u2013 The General Services Administration (GSA) prohibited the use of TLS 1.0 as early as 2018, requiring federal agencies and government contractors to adopt strong encryption configurations. This policy ensures that systems handling government-related data meet modern security standards.</li> </ul> <p>While TLS 1.2 remains widely supported, organizations should audit their configurations to ensure they aren\u2019t offering deprecated cipher suites like 3DES and RC4, still present in some TLS 1.2 implementations. Simply supporting TLS 1.2 isn\u2019t enough\u2014it must be configured securely to prevent attackers from exploiting legacy weaknesses.</p>"},{"location":"kb-articles/transport-layer-security-tls/#key-vulnerabilities-and-related-cves","title":"Key vulnerabilities and related CVEs","text":"<p>Older versions of TLS, as well as weak cipher suites, have been exploited in numerous real-world attacks, demonstrating the risks of failing to upgrade. The following vulnerabilities highlight why it is critical to fully disable and remove legacy TLS versions:</p> <ul> <li>BEAST (CVE-2011-3389) \u2013 The Browser Exploit Against SSL/TLS (BEAST) attack exploits weaknesses in TLS 1.0\u2019s CBC-mode encryption, allowing an attacker to perform a man-in-the-middle (MITM) attack and recover sensitive data from encrypted communications. This attack demonstrated why CBC-mode ciphers should no longer be used in modern TLS configurations.  </li> <li>POODLE (CVE-2014-3566) \u2013 The Padding Oracle On Downgraded Legacy Encryption (POODLE) attack forced TLS connections to downgrade to SSL 3.0, which had weak CBC padding mechanisms. Attackers could exploit these weaknesses to decrypt encrypted communications. While SSL 3.0 has been widely disabled, misconfigured servers that still offer it as a fallback remain vulnerable.  </li> <li>Sweet32 (CVE-2016-2183) \u2013 This attack targets 64-bit block ciphers, such as 3DES (Triple DES), which are still available in some TLS 1.2 configurations. Sweet32 exploits the small block size of 3DES, allowing attackers to recover plaintext data after collecting large amounts of encrypted traffic. This vulnerability reinforced the importance of disabling 3DES in TLS configurations.  </li> <li>RC4 Bias Attack (CVE-2013-2566, CVE-2015-2808) \u2013 RC4, once a widely used stream cipher in TLS, has been found to have statistical biases that allow attackers to decrypt encrypted traffic. The IETF officially prohibited RC4 in TLS (RFC 7465) due to these weaknesses. Despite this, some legacy systems still offer it as a fallback, leaving them vulnerable to downgrade attacks.</li> </ul> <p>These vulnerabilities make it clear that simply \u201cnot using\u201d a deprecated cipher suite isn\u2019t enough\u2014it must be removed from system configurations. Even if an outdated cipher suite or TLS version is rarely negotiated, its mere presence in the list of supported options exposes the server to downgrade attacks. Attackers can force  weaker cipher suite negotiations through techniques such as protocol version rollback attacks, where they intercept and manipulate the TLS handshake to downgrade the connection to an older, vulnerable encryption method. This was exploited in attacks such as POODLE (CVE-2014-3566) and FREAK (CVE-2015-0204).</p>"},{"location":"kb-articles/transport-layer-security-tls/#why-compliance-matters","title":"Why compliance matters","text":"<p>Failing to enforce modern TLS standards exposes sensitive data and leads to non-compliance with security regulations that require strong encryption. Many industries mandate TLS 1.2 or higher to protect financial transactions, healthcare records, cloud services, and user privacy.</p> <p>To reduce risk, organizations should enforce at least TLS 1.2 and adopt TLS 1.3 where possible. Regular security audits and configuration reviews are essential to identify and remove outdated protocols and weak cipher suites, ensuring a secure and compliant encryption posture.</p>"},{"location":"kb-articles/transport-layer-security-tls/#sources","title":"Sources","text":"<ul> <li>RFC 2246 \u2013 TLS 1.0 Specification</li> <li>RFC 4346 \u2013 TLS 1.1 Specification</li> <li>RFC 8996 \u2013 Deprecation of TLS 1.0 and 1.1</li> <li>RFC 7465 \u2013 Prohibition of RC4 in TLS</li> <li>NIST SP 800-52 Rev. 2 \u2013 TLS Guidelines for Federal Systems</li> <li>NSA Guidance on Eliminating Obsolete TLS \u2013 Mandates immediate discontinuation of TLS 1.0 and 1.1</li> <li>GSA SSL/TLS Implementation Guide \u2013 Prohibits TLS 1.0 since 2018</li> <li>PCI DSS Requirements for Secure TLS \u2013 Mandates TLS 1.2+ for payment transactions</li> <li>BEAST Attack (CVE-2011-3389) \u2013 CBC Mode Weakness in TLS 1.0</li> <li>POODLE Attack (CVE-2014-3566) \u2013 Forced Downgrade to SSL 3.0</li> <li>Sweet32 Attack (CVE-2016-2183) \u2013 3DES Block Size Weakness</li> <li>FREAK Attack (CVE-2015-0204) \u2013 Forced Export-Grade RSA Ciphers</li> <li>Logjam Attack (CVE-2015-4000) \u2013 Weak Diffie-Hellman Key Exchange</li> <li>RC4 Bias Attack (CVE-2013-2566) \u2013 RC4 Cryptanalysis</li> <li>RC4 Bias Attack (CVE-2015-2808) \u2013 RC4 Cryptanalysis</li> <li>CVE-2021-38502 \u2013 Legacy Cipher Suite Exploitation</li> <li>CVE-2019-14887 \u2013 Protocol Downgrade Attack</li> <li>F5 Labs 2021 TLS Telemetry Report \u2013 Analysis of TLS version adoption trends</li> <li>SSL Pulse Report \u2013 TLS Version &amp; Cipher Suite Adoption</li> <li>TLS 1.3 Specification (RFC 8446) \u2013 Defines TLS 1.3 improvements</li> <li>RFC 8446 \u2013 TLS 1.3 Specification</li> <li>F5 Labs 2021 TLS Telemetry Report \u2013 Analysis of TLS version adoption trends</li> <li>SSL Pulse Report \u2013 TLS Version &amp; Cipher Suite Adoption</li> <li>RFC 7525 \u2013 Recommendations for Secure TLS Configuration</li> <li>RFC 5288 \u2013 AES-GCM Cipher Suites for TLS 1.2</li> <li>RFC 5246 \u2013 TLS 1.2 Specification</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/","title":"Weak cryptography keys","text":"<p>Cryptographic keys are fundamental to securing digital communications, ensuring data confidentiality, integrity, and authentication. However, weaknesses in key generation, key management, or their implementations can lead to vulnerabilities that compromise the security of entire systems. In this discussion, we examine critical risks posed by weak cryptographic keys, including:</p> <ul> <li>Weak RSA keys</li> <li>ECDSA vulnerabilities</li> <li>Small subgroup attacks</li> <li>Precomputed attacks on common Diffie-Hellman primes</li> <li>Predictable pseudorandom number generator (PRNG) issues in key generation</li> <li>Side-channel attacks leading to key leakage</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#business-risk","title":"Business risk","text":"<p>Weak cryptographic keys pose significant business risks, including:</p> <ul> <li>Data breaches - Attackers can decrypt sensitive information, leading to intellectual property theft or exposure of personal data.   </li> <li>Unauthorized system access - Compromised keys enable adversaries to impersonate legitimate users or systems, facilitating unauthorized transactions or data manipulation.  </li> <li>Compliance violations - The use of weak keys can result in non-compliance with regulatory standards such as NIST SP 800-57, ISO/IEC 19790, and FIPS 140-3.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#what-breaks","title":"What breaks?","text":"<p>Data confidentiality is at risk when weak keys are used in encryption algorithms, making it easier for attackers to decrypt ciphertext and access plaintext data.  Data integrity can also be compromised, as weak keys used in digital signatures or message authentication codes (MACs) may allow attackers to forge valid signatures or MACs, leading to undetected data alterations.  Furthermore, authentication mechanisms that rely on weak keys (e.g., in digital certificates or key exchange protocols) may fail, allowing unauthorized entities to impersonate legitimate users or systems, or to forge digital signatures on transactions.</p>"},{"location":"kb-articles/weak-cryptography-keys/#consequences","title":"Consequences","text":"<p>The consequences of weak cryptographic keys can be severe and far-reaching, including: </p> <ul> <li>Intellectual property theft, exposing proprietary information and diminishing competitive advantage.   </li> <li>Financial losses, in  direct monetary costs, legal liabilities, and incident response expenses.   </li> <li>Reputational damage, which can have long-term effects on organizations, such as reducing customer trust and harming brand value.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#cryptographic-context","title":"Cryptographic context","text":"<p>This section examines various vulnerabilities and risks posed by weak cryptographic keys and provides guidance on mitigation strategies to help strengthen your cryptographic systems to protect against these threats.</p>"},{"location":"kb-articles/weak-cryptography-keys/#weak-rsa-keys","title":"Weak RSA keys","text":""},{"location":"kb-articles/weak-cryptography-keys/#small-rsa-keys","title":"Small RSA keys","text":"<p>RSA security relies on the difficulty of factoring large numbers. 512-bit RSA keys are trivially breakable, and 1024-bit keys are considered weak as they may be within the reach of state-level attackers.  The FREAK attack (CVE-2015-0204) demonstrated how some RSA implementations could be downgraded to 512-bit keys, allowing attackers to break encryption.  Advances in factoring algorithms and increasing computational power continue to weaken smaller RSA key sizes, making them unsuitable for secure applications.</p> <p>To mitigate these risks, RSA keys must be at least 2048 bits, with 3072 bits or higher recommended for long-term security.  Refer to Cryptographic keylengths and RSA algorithm for more information.</p>"},{"location":"kb-articles/weak-cryptography-keys/#roca-vulnerability","title":"ROCA vulnerability","text":"<p>The ROCA vulnerability (CVE-2017-15361) exposed a critical flaw in Infineon\u2019s cryptographic library, affecting RSA key generation in smart cards, TPMs, and hardware security modules.  The issue stemmed from a deterministic key generation process that produced RSA keys susceptible to efficient factoring attacks.  This affected keys up to 4096 bits, demonstrating that even large key sizes provide no security if the key generation process is flawed.  The vulnerability impacted widely used security tokens, enterprise hardware, and governmental authentication systems.</p> <p>To mitigate ROCA, affected devices required firmware updates or complete key replacement, as all generated RSA keys were permanently compromised.  This attack highlights the necessity of cryptographically strong PRNGs and proper key generation techniques, even in hardware implementations.</p>"},{"location":"kb-articles/weak-cryptography-keys/#small-public-exponent","title":"Small public exponent","text":"<p>A small public exponent, such as e = 3, does not inherently weaken an RSA key. However, e is a key generation parameter that can introduce vulnerabilities in certain implementations.  For example, when PKCS#1 v1.5 padding checks are incomplete, an attacker can exploit e = 3 to forge RSA signatures without knowing the private key.</p> <p>To minimize the risk of signature forgery attacks, it is recommended to use a public exponent of e = 65537, which strikes a balance between efficiency and security. This helps to ensure the integrity of digital signatures, even if the implementation has incomplete padding validation.</p>"},{"location":"kb-articles/weak-cryptography-keys/#recommendations","title":"Recommendations","text":"<ul> <li>Key size - Use at least a 2048-bit modulus for general security; 3072 bits or higher is recommended for long-term protection.  </li> <li>Public exponent - Avoid small public exponents such as e = 3; instead, use e = 65537 to reduce the risk of signature forgery attacks.  </li> <li>Key management - Don\u2019t use static or hardcoded RSA keys in software or firmware, as this increases the risk of key compromise.  </li> <li>Padding validation - Ensure proper padding validation for RSA signatures, especially when using PKCS#1 v1.5; consider using RSA-PSS instead.  </li> <li>Key generation - Avoid key generation implementations with deterministic prime selection flaws, such as Infineon\u2019s ROCA vulnerability (CVE-2017-15361), which made RSA keys factorable despite large key sizes.  </li> <li>Library usage - Use well-established libraries for generating and handling RSA keys, as these should address the issues and pitfalls mentioned above.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#weak-ecc-keys","title":"Weak ECC keys","text":""},{"location":"kb-articles/weak-cryptography-keys/#small-ecc-keys","title":"Small ECC keys","text":"<p>The security of Elliptic Curve Cryptography (ECC) depends on the choice of the curve, as different curves offer varying levels of security.  The key size in ECC is inherently determined by the curve parameters, where smaller curves correspond to shorter keys and provide lower security margins. </p> <p>160-bit curves, such as secp160r1, have private keys of 160 bits and public keys that range from 161 to 320 bits depending on encoding (compressed or uncompressed). Such small curves are not considered secure, as they provide around 80 bits of security, falling below modern cryptographic standards.  224-bit curves<sup>1</sup> are the minimum recommended by NIST, which correspond to 112 bits of security.  However, for long-term security, 256-bit curves or larger are preferred, as they provide at least 128-bit security. </p> <p>Refer to Cryptographic keylengths and Elliptic-Curve Cryptography for more information.</p>"},{"location":"kb-articles/weak-cryptography-keys/#deprecated-ecc-curves","title":"Deprecated ECC curves","text":"<p>The curves K-233, B-233, K-283, B-283, K-409, B-409, K-571, and B-571 were deprecated in NIST SP 800-186 (February 2023, Table 2).  Although these curves remain mathematically sound, they have seen limited adoption due to implementation challenges and performance limitations.  Their arithmetic differs from prime field curves (such as P-256), necessitating specialized optimizations that increase implementation complexity and introduce potential security risks. Additionally, modern hardware and cryptographic libraries predominantly optimize for prime field curves, further diminishing the adoption of binary field alternatives. </p>"},{"location":"kb-articles/weak-cryptography-keys/#ecdsa-signature-generation-and-k","title":"ECDSA signature generation and \u2018k\u2018","text":"<p>While not directly related to weak keys, ECDSA relies on a per-signature random value k; if k is reused, predictable, or generated with a biased RNG, it can compromise the private key, as demonstrated by the PlayStation 3 (PS3) compromise (CVE-2008-5161).</p>"},{"location":"kb-articles/weak-cryptography-keys/#recommendations_1","title":"Recommendations","text":"<ul> <li>Preferred curves - Use ECC curves with at least a 256-bit field size such as P-256 to achieve 128-bit security.   </li> <li>Avoid deprecated curves - Don\u2019t use deprecated curves, such as K-233, B-233, K-283, B-283, K-409, B-409, K-571, and B-571, as they have been deprecated in NIST SP 800-186.   </li> <li>Legacy use - If the use of deprecated curves is unavoidable\u2014such as for signature verification\u2014ensure that cryptographic libraries are up-to-date and adequately maintained to mitigate potential security risks.</li> </ul> <p>Note that while 224-bit curves such as P-224 provide 112-bit security, they offer a reduced security margin and are generally not recommended for long-term use.</p>"},{"location":"kb-articles/weak-cryptography-keys/#weak-diffie-hellman-dh-keys","title":"Weak Diffie-Hellman (DH) keys","text":""},{"location":"kb-articles/weak-cryptography-keys/#small-dh-keys","title":"Small DH keys","text":"<p>The security of Finite-Field Diffie-Hellman (DH) key exchange protocol relies heavily on the size of the prime modulus p used in the key agreement process.  To ensure strong security, it\u2019s essential to use sufficiently large moduli, which means having large enough key sizes.</p> <p>Finite-field DH keys smaller than 2048 bits are no longer considered secure due to advances in discrete logarithm attacks. 512-bit DH keys were demonstrated to be breakable in real-time with the Logjam attack (CVE-2015-4000), and 1024-bit DH keys are now within reach of well-resourced adversaries.  To ensure strong security, 2048-bit DH keys should be the absolute minimum, with 3072-bit keys recommended for long-term security.  Refer to Cryptographic keylengths for more information.</p>"},{"location":"kb-articles/weak-cryptography-keys/#secure-group-parameters","title":"Secure group parameters","text":"<p>In addition to using sufficiently large key sizes, it is crucial to carefully choose and validate the group parameters used in the DH key exchange.  Weak or improperly validated group parameters can allow attackers to recover partial private key information through repeated key exchanges.  For example, CVE-2016-0701 exposed how improper DH parameter validation in OpenSSL enabled small subgroup attacks, weakening the security of established session keys.</p>"},{"location":"kb-articles/weak-cryptography-keys/#use-of-common-dh-groups","title":"Use of common DH groups","text":"<p>Diffie-Hellman key exchange often relies on predefined prime groups, such as those from RFC 3526, to simplify implementation.  However, if widely used DH primes are reused across multiple systems, it can compromise security.</p> <ul> <li>Precomputed discrete logarithm attacks - Attackers can precompute solutions to the discrete logarithm problem for a known DH prime. This significantly reduces the security of key exchanges relying on static DH parameters with that prime.</li> <li>Logjam attack - The Logjam attack demonstrated that precomputing discrete logarithms for common DH groups (particularly 512-bit and 1024-bit) enables passive decryption of TLS connections.  </li> <li>Weak standardized groups - Certain standardized groups, such as Oakley Group 1 (768-bit) and Group 2 (1024-bit) from RFC 2409, are unsafe and should not be used, as any well-known DH group smaller than 2048 bits is vulnerable to precomputed attacks.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#recommendations_2","title":"Recommendations","text":"<ul> <li>Minimum key size - Use at least 2048-bit DH keys, as 512-bit and 1024-bit keys are breakable due to precomputed discrete logarithm attacks such as Logjam CVE-2015-4000.</li> <li>Avoid common DH groups - Avoid using common, predefined DH groups smaller than 2048 bits, including Oakley Group 1 (768-bit) and Group 2 (1024-bit), as they are vulnerable to precomputed attacks.  </li> <li>Long-term security - For long-term security, use 3072-bit DH keys or higher, following NIST SP 800-57 recommendations.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#weak-dsa-keys","title":"Weak DSA keys","text":"<p>The Digital Signature Algorithm (DSA) is no longer approved by NIST for signature generation.</p> <p>Note</p> <p>DSA is no longer specified in FIPS 186-5 (February 2023) and is no longer approved for use. Its previous specification, FIPS 186-4, was withdrawn in February 2024. NIST SP 800-131A Rev 2 does currently allow for DSA signature generation using DSA-2048 or DSA-3072, but in NIST SP 800-131A Rev 3 it is expected that this will be disallowed entirely. </p>"},{"location":"kb-articles/weak-cryptography-keys/#small-dsa-keys","title":"Small DSA Keys","text":"<p>Like RSA, the security of the DSA depends on key size. 1024-bit DSA keys, once considered secure, are becoming susceptible to practical discrete logarithm attacks, making them unsafe.</p>"},{"location":"kb-articles/weak-cryptography-keys/#structural-weaknesses","title":"Structural weaknesses","text":"<p>The security of a DSA signature depends not only on the modulus size but also on the size of the subgroup q and the randomness of the nonce k.</p> <ul> <li>Small subgroups - DSA operates in a group of prime order p, but signatures are computed in a smaller subgroup of order q. If q is too small, the security of DSA is significantly reduced. Previous, withdrawn versions of FIPS 186 recommended a 224-bit q for 2048-bit DSA keys.  </li> <li>Weak or reused nonces - Each DSA signature requires a unique, unpredictable nonce k. If the same k is used twice, the private key can be fully recovered. Such attacks have been demonstrated in practical settings where poor random number generation compromised DSA signatures.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#recommendations_3","title":"Recommendations","text":"<p>Whenever possible, use EdDSA (e.g., Ed25519) or ECDSA instead of DSA, as they provide better security and efficiency. </p> <p>If DSA must be used, choose a minimum of 2048-bit keys with a 224-bit or 256-bit q, though 3072-bit keys are strongly preferred. For more details on key length recommendations for EdDSA or ECDSA, refer to Cryptographic keylengths and Elliptic-Curve Cryptography.</p>"},{"location":"kb-articles/weak-cryptography-keys/#predictable-prng-issues-in-key-generation","title":"Predictable PRNG issues in key generation","text":"<p>Weak cryptographic keys can result from predictable or low-entropy pseudo-random number generators (PRNGs) used during key generation. If a PRNG lacks sufficient entropy or follows a deterministic pattern, attackers may predict or reconstruct private keys, compromising the entire cryptographic system.  This issue is classified under CWE-338 (Use of Cryptographically Weak PRNG) and CWE-332 (Insufficient Entropy in PRNG).</p>"},{"location":"kb-articles/weak-cryptography-keys/#real-world-examples-of-predictable-prng-failures","title":"Real-world examples of predictable PRNG failures","text":"<p>Several notable vulnerabilities have been caused by predictable PRNG failures, including:</p> <ul> <li>CVE-2008-0166 (Debian OpenSSL PRNG Vulnerability) \u2013 A critical flaw in Debian\u2019s OpenSSL package caused the PRNG to generate weak cryptographic keys due to an accidental removal of entropy-gathering code. This resulted in millions of weak and easily guessable SSH and TLS keys.  </li> <li>CVE-2017-15361 (ROCA Vulnerability) \u2013 A flaw in Infineon\u2019s RSA key generation algorithm produced factorable RSA keys, allowing attackers to efficiently compute private keys. Although not a direct PRNG failure, this vulnerability demonstrated the risks of deterministic key generation.  </li> <li>Dual_EC_DRBG (CVE-2007-6755, CVE-2013-4670) \u2013 A compromised elliptic curve PRNG, suspected of containing a backdoor, allowed attackers with special knowledge to predict its output.  </li> <li>CVE-2023-32549 \u2013 A cryptographic system used a weak PRNG for key generation, leading to insecure key material.  </li> <li>CVE-2022-45782 \u2013 A cryptographically insecure PRNG in dotCMS led to predictable password-reset tokens, increasing the risk of account takeover.  </li> <li>CVE-2021-0131 \u2013 Intel Security Library before version 3.3 used a weak PRNG in an API, exposing cryptographic material to potential attackers.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#mitigation-and-secure-recommendations","title":"Mitigation and secure recommendations","text":"<ul> <li>Use a cryptographically secure PRNG (CSPRNG) -  Recommendations by NIST SP 800-90A include Hash_DRBG and HMAC_DRBG (using SHA-2 or SHA-3 using 128-bit parameters or above), or CTR_DRBG (only using AES).  </li> <li>Avoid weak PRNGs - Steer clear of PRNGs with known weaknesses, such as Dual_EC_DRBG (CVE-2007-6755, CVE-2013-4670).  </li> <li>Ensure proper entropy collection - During key generation, following best practices from NIST SP 800-90B, to prevent issues like CVE-2008-0166.  </li> <li>Regularly update cryptographic libraries - Keep your libraries up to date to patch PRNG vulnerabilities and prevent weak key generation, as seen in CVE-2023-32549 and CVE-2021-0131.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#key-validation","title":"Key validation","text":"<p>Key validation ensures that cryptographic keys are correctly structured, adhere to security standards, and resist known attacks.  Proper validation reduces risks such as private key exposure, signature forgery, weak key exchanges, and authentication bypass vulnerabilities.  Validation requirements follow NIST, FIPS, ISO/IEC, and other cryptographic standards to ensure compliance and security.</p>"},{"location":"kb-articles/weak-cryptography-keys/#rsa-key-validation-issues","title":"RSA key validation issues","text":"<p>RSA key validation ensures that key parameters conform to cryptographic standards such as NIST SP 800-57, FIPS 186-5, and ISO/IEC 18033-2.  This includes verifying that key components are correctly structured, of sufficient size, and resistant to known cryptographic attacks.  Proper validation prevents the use of weak or improperly generated RSA keys, reducing risks such as factorization vulnerabilities, weak exponents, and private key compromise.</p> <ul> <li>CVE-2018-15836 and CVE-2016-1494 \u2013 Signature forgery due to weak public exponent selection (e \\= 3).  </li> <li>CVE-2022-26320 \u2013 Small private exponent allowing private key recovery via Boneh-Durfee attack.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#ecc-key-validation-issues","title":"ECC key validation issues","text":"<p>ECC key validation ensures that public keys and parameters conform to modern security standards such as NIST SP 800-186, FIPS 186-5, and ISO/IEC 15946-5.  This includes verifying that keys are valid points on the expected curve, of appropriate strength, and resistant to attacks that exploit invalid curve choices or subgroup vulnerabilities.  Proper validation protects against private key leakage and unauthorized access.</p> <ul> <li>CVE-2015-7511 \u2013 Invalid curve attack against Libgcrypt, allowing private key extraction in ECDH.  </li> <li>CWE-754 \u2013 Improper curve parameter validation, leading to cryptographic attacks.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#diffie-hellman-dh-key-validation-issues","title":"Diffie-Hellman (DH) key validation issues","text":"<p>DH key validation ensures that key exchange parameters follow NIST SP 800-56A, FIPS 186-5, and ISO/IEC 18033-2 standards.  This includes verifying that public key inputs are correctly formed, belong to the correct group, and are resistant to known attacks such as small subgroup attacks and precomputed discrete logarithm attacks.  Proper validation helps maintain key secrecy and secure key agreement.</p> <ul> <li>CVE-2016-1000346 \u2013 Bouncy Castle DH public key validation failure, allowing attackers to force key reuse and partial key exposure.  </li> <li>CVE-2015-4000 \u2013 Logjam attack exploiting weak DH groups to break encryption.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#dsa-key-validation-issues","title":"DSA key validation issues","text":"<p>DSA key validation ensures that parameters such as p, q, and the signing nonce follow NIST SP 800-57 and FIPS 186-5 guidelines.  This includes verifying that keys are correctly structured, nonces are randomly generated, and q is appropriately sized to maintain security. Proper validation helps prevent private key leakage and signature forgery.</p> <ul> <li>CVE-2016-1000343 \u2013 Weak q parameter selection in Bouncy Castle DSA, leading to weaker cryptographic strength.  </li> <li>CVE-2016-0695 \u2013 Oracle Java DSA vulnerability due to biased or reused nonces, enabling private key recovery.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#recent-example-of-key-validation-failure","title":"Recent example of key validation failure","text":"<p>CVE-2025-26465 (OpenSSH Key Validation Flaw) is a vulnerability in OpenSSH which causes improper key validation during session establishment, allowing an attacker to bypass authentication mechanisms.  This highlights how weak or absent key validation can lead to man-in-the-middle (MITM) attacks and unauthorized access.</p>"},{"location":"kb-articles/weak-cryptography-keys/#mitigation-and-recommendations","title":"Mitigation and recommendations","text":"<p>Follow these best practices to mitigate attacks that exploit weak or improperly structured cryptographic keys. </p> <ul> <li>Ensure key parameters conform to NIST, FIPS, and ISO/IEC standards to prevent weak or improperly structured keys.  </li> <li>Enforce validation of RSA, ECC, DH, and DSA key components to prevent attacks that exploit misconfigured or unchecked values.  </li> <li>Use only well-reviewed cryptographic libraries that enforce strict key validation rules, such as  OpenSSL, BoringSSL, or libsodium.  </li> <li>Regularly patch cryptographic implementations to prevent validation-related vulnerabilities like CVE-2025-26465, which demonstrated how improper key validation in OpenSSH enabled authentication bypass.</li> </ul>"},{"location":"kb-articles/weak-cryptography-keys/#sources","title":"Sources","text":"<ul> <li>Knowledge base: Cryptographic keylengths</li> <li>CVE-2007-6755 \u2013 Dual_EC_DRBG Backdoor Vulnerability</li> <li>CVE-2008-0166 \u2013 Debian OpenSSL PRNG Weakness</li> <li>CVE-2013-4670 \u2013 Dual_EC_DRBG Predictable Output</li> <li>CVE-2015-0204 \u2013 FREAK Attack (RSA Downgrade to 512-bit Keys)</li> <li>CVE-2015-4000 \u2013 Logjam Attack on DH Weak Groups</li> <li>CVE-2015-7511 \u2013 OpenSSL ECDH Invalid Curve Attack</li> <li>CVE-2016-0695 \u2013 Oracle Java DSA Nonce Bias</li> <li>CVE-2016-0701 \u2013 OpenSSL DH Parameter Generation Flaw </li> <li>CVE-2016-1000343 \u2013 Bouncy Castle DSA Weak q Parameter</li> <li>CVE-2016-1000346 \u2013 Bouncy Castle DH Public Key Validation Flaw</li> <li>CVE-2016-1494 \u2013 Python-RSA Signature Forgery via Weak Exponent</li> <li>CVE-2017-15361 \u2013 ROCA RSA Key Generation Flaw</li> <li>CVE-2008-5161 - PlayStation 3 ECDSA Key Compromise</li> <li>CVE-2018-15836 \u2013 Weak RSA Public Exponent Signature Forgery</li> <li>CVE-2021-0131 \u2013 Intel Security Library PRNG Issue</li> <li>CVE-2022-26320 \u2013 RSA Small Private Exponent Weakness</li> <li>CVE-2022-45782 \u2013 Weak PRNG in dotCMS Password Tokens (May not be key-related)</li> <li>CVE-2023-32549 \u2013 Weak PRNG for Cryptographic Key Generation</li> <li>CVE-2025-26465 \u2013 OpenSSH Key Validation Flaw</li> <li>CWE-332 \u2013 Insufficient Entropy in PRNG</li> <li>CWE-338 \u2013 Use of Cryptographically Weak PRNG</li> <li>CWE-754 \u2013 Improper Curve Parameter Validation in ECC</li> <li>NIST SP 800-18 \u2013 Guide for Developing Security Plans for Federal Information Systems</li> <li>NIST SP 800-57 \u2013 Key Management Guidelines</li> <li>NIST SP 800-90 \u2013 Recommendation for Random Number Generation</li> <li>NIST SP 800-186 \u2013 Recommendations for Discrete Logarithm-based Cryptography: Elliptic Curve Domain Parameters</li> <li>FIPS 186-4 \u2013 Digital Signature Standard (DSS) (Withdrawn, replaced by 186-5)</li> <li>FIPS 186-5 \u2013 Current Digital Signature Standard (DSS)</li> <li>RFC 2409 \u2013 The Internet Key Exchange (IKEv1), Includes Oakley Groups</li> <li>RFC 3526 \u2013 More Modular Exponential (MODP) Diffie-Hellman Groups</li> <li>NIST SP 800-131A Rev 3 IPD - Transitioning the Use of Cryptographic Algorithms and Key Lengths</li> <li>NIST SP 800-131A Rev 2 - Transitioning the Use of Cryptographic Algorithms and Key Lengths</li> </ul> <ol> <li> <p>It should be noted that when NIST SP 800-131A Revision 3 is published (as of writing it is in the Initial Public Draft stage) that the use of 112-bit security strength for the classical digital signature and key-establishment mechanisms will be deprecated after December 31, 2030.\u00a0\u21a9</p> </li> </ol>"},{"location":"login-and-user-roles/","title":"Login &amp; user roles","text":"<p>This section provides instructions for logging into AQtive Guard and managing user roles. </p>"},{"location":"login-and-user-roles/#aqtive-guard-login","title":"AQtive Guard login","text":"<p>Access the AQtive Guard web application at: <code>your-domain.aqtiveguard.sandboxaq.com</code></p> <p>Note</p> <p>If you need help with AQG access, contact your AQG system administrator or visit our support portal.</p>"},{"location":"login-and-user-roles/#assigning-user-roles","title":"Assigning user roles","text":"<p>AQtive Guard integrates with your organization\u2019s Identity Provider (IdP) to manage user configuration, including roles and access permissions. This means that user accounts and their associated roles are defined and controlled externally, through your existing IdP, rather than within AQtive Guard itself.</p> <p>The following sections outline the different user roles available in AQtive Guard, the permissions associated with each role, and the specific access granted to each.</p>"},{"location":"login-and-user-roles/#user-roles-and-permissions","title":"User roles and permissions","text":"<p>An AQtive Guard user can be assigned one of the following roles and permissions:</p> <ul> <li>Admin - has full access and capabilities in AQtive Guard.</li> <li>Uploader - the ability to upload data generated using an AQtive Guard sensor or supported external file format. </li> <li>Read only - has read-only access to limited pages in AQtive Guard.</li> </ul> <p>The following sections provide details for the access granted for each role.</p>"},{"location":"login-and-user-roles/#admin","title":"Admin","text":"<p>Users assigned the Admin role have full access to AQtive Guard. Administrators can:</p> <ul> <li>Perform system configuration in Settings. </li> <li>Configure integrations. </li> </ul>"},{"location":"login-and-user-roles/#read-only","title":"Read-only","text":"<p>Users assigned the Read-only user role can view content on the following pages:</p> <ul> <li>Dashboard</li> <li>Inventory</li> <li>IT Assets</li> <li>Rules (only list view; can\u2019t create/modify/delete rules)</li> <li>Issues</li> </ul> <p>Note</p> <p>To limit access to sensitive user and company data, users assigned the Read-only role can\u2019t access Settings.</p>"},{"location":"login-and-user-roles/#uploader","title":"Uploader","text":"<p>Uploader users can upload a supported file format, such as CBOM or PCAP, or a trace generated using an AQtive Guard sensor.  </p>"},{"location":"navigation/","title":"Navigation","text":"<p>The navigation panel, located on the left side of the screen, is your central hub for accessing AQtive Guard (AQG) features and resources.</p>"},{"location":"navigation/#navigation-panel","title":"Navigation panel","text":"<p>The navigation panel offers the following options:</p> <ol> <li> <p>Main menu - Access AQG features, explore your cryptographic inventory, and configure system settings.</p> </li> <li> <p>Data analysis status - Monitor analysis progress and completion.</p> </li> <li> <p>Help center -  Get answers to questions and learn more about your cryptography and NHIs.</p> </li> </ol> <p></p>"},{"location":"navigation/#main-menu","title":"Main menu","text":"<p>The main menu makes it easy to navigate to the following options from anywhere in AQG:</p> <ul> <li>Dashboard - Monitor the overall health of your IT assets, Crypto inventory, Scan status, and Highest impact issues.</li> <li>Inventory - Monitor and manage your cryptographic inventory by object type (Keys, Certificates, Operations, Keystores, TLS configs, Handshakes, and Secrets).</li> <li>IT Assets - Monitor and manage data related to your Hosts, Applications, and Container images.</li> <li>Profiles - Create and manage the profiles used in analysis to flag out-of-policy issues. </li> <li>Issues - Understand and address the high-impact issues discovered in analysis.</li> <li>Exports - Save and export custom Inventory or IT Asset views.</li> <li>Data sources - Configure integrations and run scan requests.</li> <li>Settings - System admins can create API tokens for administrator or uploader-level permissions. </li> </ul> <p>Tip</p> <p>For more information on each menu option, refer to the corresponding section in this user guide.</p>"},{"location":"navigation/#data-analysis-status","title":"Data analysis status","text":"<p>The data analysis status provides an update of progress as AQG analyzes incoming data. The following states are possible:</p> <ul> <li>All data processed - Indicates that AQtive Guard has completed analysis and inventory of all ingested data.</li> <li>Analyzing events -  Signifies that data analysis is currently in progress. </li> </ul> <p>During large-scale data ingestions, the status may also indicate the number of items processed.</p>"},{"location":"navigation/#help-center","title":"Help center","text":"<p>The help center at the bottom of the AQG navigation panel offers access to AQG documentation and resources for mastering cryptographic best practices. </p> <ul> <li>User guide - Browse this comprehensive guide to learn how to use AQG effectively and maximize its value. Addtional resources within the guide include:<ul> <li>Rules reference - Refer to this detailed guide for information on the cryptographic rules used by AQtive Guard to identify priority issues. The Rules reference is helpful when working with profiles.</li> <li>Knowledge base - Visit the Knowledge base for in-depth articles, expert guidance, and links to authoritative resources on cryptography best practices.</li> <li>Cryptography glossary - Consult this reference for clear definitions of key terms and concepts related to the cryptography discovered by AQG.</li> </ul> </li> <li>Contact support - If you have further questions or need additional help, select this menu item to visit our support portal. </li> <li>Logout - Ends your current session and returns you to the login page.</li> </ul> <p>Note</p> <p>If your organization hasn\u2019t set up a support portal account, reach out to your AQtive Guard account team for assistance. </p>"},{"location":"profiles/","title":"Profiles","text":"<p>In AQtive Guard (AQG), a profile is a collection of rules that define the criteria used for evaluating the health of your cryptographic and Non-human Identity (NHI) inventory. When you apply a profile, AQtive Guard uses its rules to:</p> <ul> <li>Identify the specific data you want to analyze.</li> <li>Determine which issues to flag.</li> <li>Assign a severity level to any detected issues.</li> </ul> <p>By applying different built-in and custom profiles to your data, you can tailor analysis to your unique context and ensure that AQtive Guard checks for the issues that are most relevant to your evolving security needs.</p> <p>Tip</p> <p>If you\u2019re using AQtive Guard to prepare for FIPS validation, refer to Knowledge base: Preparing for FIPS validation with AQtive Guard.</p>"},{"location":"profiles/#using-profiles","title":"Using profiles","text":"<p>AQtive Guard continuously monitors ingested data and updates your Inventory, IT Assets, and  Issues accordingly. When you apply a profile, AQtive Guard evaluates your inventory against the profile\u2019s rules, logging any issues with a severity level. If no global profile filters are applied, all rules are actively evaluating data, and all associated rules with issues are displayed in the Issues table.</p> <p>Tip</p> <p>Although you can apply a profile from any screen, the greatest impact to your data is seen in Issues. </p> <p>To apply or remove a profile, follow these steps:</p> <ol> <li>Select Issues from the main menu. </li> <li>Select the Profile global filter.</li> <li>In the drop-down, choose one or more of the available built-in or custom profiles. </li> </ol> <p>Applying one or more profiles has the following affects in the Issues table:    - The table updates to display only issues for rules included in the selected profile(s).</p> <p>- The rule severity will indicate the highest level of severity of all occurrances, based on the rule criteria in the selected profile(s). </p> <p>Caution</p> <p>Critical issues may be hidden when you apply a profile. Select Reset in the Profile global filter to see all issues for all active rules.  </p>"},{"location":"profiles/#profile-types","title":"Profile types","text":"<p>AQtive Guard provides two types of profiles:</p> <ol> <li>Built-in AQtive Guard profiles: Identified by the gold AQtive Guard icon; designed to adhere to industry best practices.</li> <li>Custom profiles: Configured by your organization to meet your unique requirements. </li> </ol>"},{"location":"profiles/#_1","title":"Profiles","text":""},{"location":"profiles/#built-in-profiles","title":"Built-in profiles","text":"<p>AQG built-in profiles are designed to adhere to industry best practices and provide a solid foundation for cryptographic security. These profiles serve as a starting point for configuring your security settings and can be used as a foundation for creating custom profiles. The available built-in profiles include:</p> <ul> <li>SandboxAQ: The SandboxAQ best practices profile provides a comprehensive set of rules that align with widely accepted cybersecurity standards, covering common security requirements. This profile includes graded severity versions of rules, offering a nuanced assessment of compliance, and is regularly updated with new rules as they become available.</li> <li>NIST: The NIST profile helps organizations align with NIST guidelines and standards, providing a foundation for evaluating cryptography against key requirements, while acknowledging that full NIST compliance involves additional considerations and evaluations. </li> <li>PQC: The PQC profile focuses on rules relevant to Post-Quantum Cryptography migration readiness, while also identifying classical risks.</li> <li>PQC-only: The PQC-only profile focuses your view solely on data specific to the Post-Quantum Cryptography rules.</li> </ul> <p>Important</p> <p>Built-in profiles are read-only and cannot be modified directly. To make changes, create a copy of a built-in profile and use it as a starting point to create a custom profile that aligns with your organization\u2019s specific policies and needs.</p>"},{"location":"profiles/#custom-profiles","title":"Custom profiles","text":"<p>You can create a custom profile from scratch, or use an existing profile as a template so you can adapt and refine your analysis to suit your specific security needs and organizational standards.</p> <p>Tip</p> <p>Any time you\u2019re working with rules or profiles, it\u2019s helpful to consult the Rules reference in the AQtive Guard user guide for detailed rule information and specifications. </p>"},{"location":"profiles/#create-a-custom-profile-from-scratch","title":"Create a custom profile from scratch","text":"<p>Follow these steps to create a custom profile by selecting from your AQG library of rules. </p> <ol> <li>Select Profiles from the main menu.</li> <li>Select Create profile.</li> <li>Enter a unique Profile name and Description. These will appear in your profile list.</li> <li>Select the rule(s) to include in your profile. Keep in mind you may need to select &gt; to see them all </li> <li>Select Save.</li> </ol>"},{"location":"profiles/#configure-a-custom-profile","title":"Configure a custom profile","text":"<p>You can\u2019t edit an existing profile. Instead, follow these steps to duplicate an existing profile as a template and use it to configure a new custom profile.</p> <ol> <li>Select Profiles from the main menu.</li> <li>Locate the profile you would like to use as a template. To review its rules, select Details.  </li> <li>Select Copy.</li> <li>Enter a unique Profile name and Description. These will appear in your profile list.</li> <li>Select the rule(s) to include in your profile. Keep in mind you may need to select &gt; to see them all </li> <li>Select Save.</li> </ol>"},{"location":"profiles/#delete-a-custom-profile","title":"Delete a custom profile","text":"<p>When you no longer need a profile, follow these steps to delete it. </p> <p>Note</p> <p>You can\u2019t delete the AQG built-in profiles. </p> <ol> <li>Select Profiles from the main menu and locate the profile you\u2019d like to delete.</li> <li>Select Details and carefully review the profile name, description, and rules to make sure you want to move forward with the deletion. </li> <li>Select Delete profile. If you are sure, select Yes, delete profile. This can not be undone. </li> </ol>"},{"location":"rules/","title":"Rules","text":"<p>In AQtive Guard, rules define the criteria for cryptographic analysis. When data is ingested, the system checks the detected cryptography against the active rules.  When a rule violation is detected, the corresponding issue is assigned a status and logged in the Issues table for further review and tracking.</p> <p>Tip</p> <p>To ensure effective risk management and compliance, collaborate with your organization\u2019s security and compliance teams to verify that the rule logic is aligned with your organization\u2019s security policies and relevant industry regulations. This alignment helps guarantee that AQtive Guard issue identification and severity categorization accurately reflects your organization\u2019s risk tolerance and compliance requirements.</p>"},{"location":"rules/#view-active-rules","title":"View active rules","text":"<p>Navigate to Rules from the main menu to view the available rules for your organization.</p> <p>Refer to the Rules reference in this user guide for detailed information about AQG rules.</p> <p></p>"},{"location":"rules/#rule-details","title":"Rule details","text":"<p>Select Details at the end of a rule row to view the rule reference.</p> <p>Each rule reference includes:</p> <ul> <li>A brief description of the issue and its potential impact.</li> <li>Guidance on how to resolve the issue.</li> <li>Links to in-depth articles in our Knowledge Base for further learning and context.</li> <li>Information on relevant compliance and regulatory requirements.</li> <li>Explanations of the security risks and potential consequences of not addressing the issue.</li> <li>References to industry-recognized standards and best practices.</li> <li>Specifications for the rule pass/fail policy and severity criteria.</li> </ul>"},{"location":"rules/rule-severity/","title":"Rule severity","text":"<p>AQtive Guard (AQG) assigns a severity level to each identified cryptographic vulnerability and compliance issue, as specified in its associated rule. This severity helps you prioritize remediation efforts by indicating the potential risk and impact of a detected issue. </p> <p>AQG severity ratings are calibrated to provide a realistic assessment of threats, balancing immediate exploitability with long-term security posture.</p>"},{"location":"rules/rule-severity/#how-aqtive-guard-determines-severity","title":"How AQtive Guard determines severity","text":"<p>AQG bases severity assignment on guiding principles and a well-defined classification system to ensure consistent and actionable insights. These principles include:</p> <ul> <li>Minimizing false positives. For issues classified as Critical, AQG prioritizes minimizing false positives. This ensures that high-priority alerts genuinely indicate the most urgent potential threats.</li> <li>Actionability vs. severity.  Your ability to act on an issue doesn\u2019t directly influence whether AQG categorizes it as Critical. Instead, actionability primarily guides prioritization within the High, Medium, and Low tiers.</li> <li>CVEs are not inherently critical.  A Common Vulnerabilities and Exposures (CVE) identifier doesn\u2019t automatically classify an issue as Critical. Severity is assessed based on the reasonable risk an issue poses. </li> <li>Contextual assessment.  When issue severity depends on the specific context (for example, whether SHA-1 is used for a signature in a certificate), AQG assumes the context is acceptable unless definitive data proves otherwise.</li> </ul>"},{"location":"rules/rule-severity/#severity-level-criteria","title":"Severity level criteria","text":"<p>This section explains the security criteria AQtive Guard uses to determine severity levels.</p>"},{"location":"rules/rule-severity/#critical","title":"Critical","text":"<p>A Critical severity indicates an immediate and severe security risk. These vulnerabilities are typically:</p> <ul> <li>Exploitable by a network attacker. Attackers can exploit them remotely, potentially through Machine-in-the-Middle (MITM) attacks or precise timing attacks.</li> <li>Likely to cause an outage. Issues that could lead to service disruption.</li> <li>Data leakage. Cryptographic protections that are easily breakable (for example, weakly encrypted data with known feasible attacks).</li> <li>Certificates (on the fly):<ul> <li>Certificates with 10-20% remaining valid period and less than one month until expiry.</li> <li>Certificates that are no longer valid.</li> </ul> </li> <li>Certificates (on disk). Certificates with a short period before or after their validity end date (either days after or weeks before).</li> </ul> <p>Note</p> <p>AQtive Guard doesn\u2019t classify unencrypted data as Critical.</p>"},{"location":"rules/rule-severity/#high","title":"High","text":"<p>High severity issues represent known vulnerabilities that are harder to exploit than Critical issues but still pose a significant risk. These include:</p> <ul> <li>Vulnerabilities difficult to exploit:<ul> <li>Attacks requiring specific environmental conditions, such as VM2VM attacks (for example, CPU hammering attacks).</li> <li>Timing attacks proven only in laboratory settings.</li> <li>Attacks requiring root access, such as unencrypted credentials on a compromised system.</li> </ul> </li> </ul> <ul> <li>NIST disallowed primitives. Use of cryptographic primitives that the National Institute of Standards and Technology (NIST) no longer permits for cryptographic protection.</li> <li>Certificates with a wider window. Certificates that are invalid within a wider time window than those considered Critical.</li> <li>Unknown certificates with long validity. Untrusted certificates that have an extended validity period.</li> </ul>"},{"location":"rules/rule-severity/#medium","title":"Medium","text":"<p>Medium severity issues indicate configurations that reflect bad practices or use cryptographic elements approaching deprecation or currently deprecated. These are:</p> <ul> <li>NIST upcoming deprecation: Use of cryptographic primitives that NIST has designated for deprecation in a future timeframe.</li> <li>NIST deprecated: Use of cryptographic primitives that NIST has already deprecated.</li> <li>Bad practices on unknown cryptography: Poor cryptographic practices applied to implementations not specifically recognized or categorized by AQG.</li> </ul>"},{"location":"rules/rule-severity/#low","title":"Low","text":"<p>Low severity issues are generally minor concerns or configurations that could become problematic in the distant future. These include:</p> <ul> <li>NIST upcoming deprecation: Use of cryptographic primitives that NIST will deprecate in a more distant future timeframe.</li> <li>Bad practices on known cryptography: Minor poor practices applied to known cryptographic elements.</li> </ul>"},{"location":"rules/rule-severity/#informational","title":"Informational","text":"<p>Informational level findings indicate issues that AQG has identified but deems to pose minimal to no practical risk. AQtive Guard retains these findings for full transparency and to ensure no issues are overlooked. Regardless of its original severity (Critical, High, Medium, or Low), a finding can be re-categorized as Informational if additional context suggests it is not a practical threat. This determination is often made by enriching rule logic with external data using the Enrichment database. </p> <p>By default, AQtive Guard hides Informational level findings in the user interface. </p> <p>Tip</p> <p>Issue severity can be downgraded to Informational level using Data enrichment.</p>"},{"location":"settings/","title":"Settings","text":"<p>Admins can create tokens for working with the AQtive Guard API. There are two token types available, with each corresponding to a specific user role and associated privileges: </p> <ul> <li>Uploader token - Authenticates uploads of data generated using an AQtive Guard sensor or supported external file format.</li> <li>Admin token - Grants full administrator privileges for configuring data sources and automation using the AQG API. </li> </ul> <p>Important</p> <p>API tokens are valid for 90 days, after which they expire and must be replaced with a new token.</p> <p>Refer to Assigning user roles for more information about user roles and privileges. </p>"},{"location":"settings/#important-security-considerations","title":"Important Security Considerations","text":"<p>Sharing Admin tokens poses a significant security risk to your AQtive Guard configuration and data. Admin tokens provide unrestricted access to your account, allowing the holder to:</p> <ul> <li>Create and modify configurations.</li> <li>Set up automation.</li> <li>Access sensitive data.</li> </ul> <p>To maintain the security and integrity of your AQtive Guard account: </p> <ul> <li>Protect all tokens as confidential. </li> <li>Only share Admin tokens with trusted administrators who have a legitimate need for full access. </li> <li>Ensure that all tokens are properly managed and revoked when no longer needed.</li> </ul>"},{"location":"settings/#create-an-api-token","title":"Create an API token","text":"<p>To create an AQG API token: </p> <ol> <li>Log in to AQtive Guard as an administrator. </li> <li>In Settings, select Create Uploader token or Create Admin token. </li> <li>Copy and securely save the token before closing the page. </li> </ol> <p>Note</p> <p>The token will not be displayed again for security reasons. </p>"}]}